\documentclass[%
	11pt,
	a4paper,
	utf8,
	%twocolumn
		]{article}	

\usepackage{style_packages/podvoyskiy_article_extended}


\begin{document}
\title{Заметки по машинному обучению и анализу данных}

\author{\itshape Подвойский А.О.}

\date{}
\maketitle

\thispagestyle{fancy}

Здесь приводятся заметки по некоторым вопросам, касающимся машинного обучения, анализа данных, программирования на языках \texttt{Python}, \texttt{R} и прочим сопряженным вопросам так или иначе, затрагивающим работу с данными.


\shorttableofcontents{Краткое содержание}{1}


\tableofcontents

\section{Основные термины}

\noindent\emph{Квантиль} -- значение, которое заданная случайная величина не превышает с фиксированной вероятностью. Если вероятность задана в процентах, то квантиль называют процентилем. Пример: фраза <<90-й процентиль массы тела у новорожденных мальчиков составляет 4 кг>>, что означает 90\% мальчиков рождаются с массой тела, меньшей или в частном случае равной 4 кг, а 10\% соответственно -- с массой большей 4 кг. Если распределение непрерывно, то $ \alpha $-квантиль однозначно задается уравнением
\begin{align*}
	F_X(x_\alpha) = \alpha.
\end{align*}

Для непрерывных распределений справедливо следующее широко использующееся при построении доверительных интервалов равенство
\begin{align*}
	\mathbb{P}\big(x_{\frac{1 - \alpha}{2}} \leqslant X \leqslant x_{\frac{1 + \alpha}{2}}\big) = \alpha.
\end{align*}

\noindent\emph{Интерквартильный размах} -- разность между третьим и первым квартилями, то есть $ x_{0,75} - x_{0,25} $. Интерквартильный размах является характеристикой разброса и является робастным аналогом дисперсии. Вместе, медиана и интерквартильный размах могут быть использованы вместо математического ожидания и дисперсии в случае распределений с большими выбросами.

\noindent\emph{Web-сокет} -- это технология, позволяющая создавать интерактивное соединение для обмена сообщениями в режиме реального времени. Web-сокет в отличие от HTTP не нуждается в повторяющихся запросах к серверу. Сокет работает таким образом, что достаточно лишь один раз выполнить запрос, а потом ждать отклика. То есть можно спокойно слушать сервер, который отправит сообщения по мере готовности. Сокеты применяют приложениях, обрабатывающих информацию в <<реальном времени>> (IoT-приложения, чаты и пр.)

\section{Теория алгоритмов и структуры данных}

В \emph{теории сложности вычислений} широкое распространение получило обозначение <<\emph{О}-большое>>. Типичный результат выглядит следующим образом: <<данный алгоритм работает за время $ O(n^2 \log n) $>>, и его следует понимать как <<существует такая константа $ C > 0 $, что \emph{время работы} алгоритма в \emph{наихудшем} случае не превышает $ C\,n^2 \log n $, начиная с некоторого $ n $>>.

Практическая ценность асимптотических результатов такого рода зависит от того, насколько мала неявно подразумеваемая константа $ c $. Как мы уже отмечали выше, для подавляющего большинства известных алгоритмов она находится в разумных пределах, поэтому, как правило, имеет место следующий тезис: алгоритмы, более эффективные с точки зрения их асимптотического поведения, оказываются также более эффективными и при тех сравнительно небольших размерах входных данных, для которых они реально используются на практике. Другими словами, \emph{асимптотические оценки эффективности} достаточно полно отражают реальное положение вещей.

Теория сложности вычислений по определению считает, что алгоритм, работающий за время $ O(n^2 \log n) $ лучше алгоритма с временем работы $ O(n^3) $, и в подавляющем большинстве случаев это отражает реально существующую на практике ситуацию.

\section{Настройка непрерывной интеграции (CI) на GitHub Actions}

\subsection{Порядок работы с pull-request}

Кратко о pull-request. Pull-request (запрос на изменения) -- запрос к управляющему каким-либо репозиторием (человеку, группе или вообще роботу) на выполнение изменений из вашего репозитория (и указанной ветки).

Порядок выполнения pull-request на свой проект:
\begin{itemize}
	\item Делаем форк репозитория,
	
	\item Через свой собственный GitHub-профиль клонируем репозиторий на локальную машину,
	
	\item Создаем новую ветку под изменения,
	
	\item Вносим изменения из-под новой ветки,
	
	\item Делаем \texttt{git push origin my\_branch},
	
	\item На странице GitHub репозитория выбираем новую ветку и нажимаем кнопку <<Compare \& pull request>>; появится форма, в которой можно описать коммит и и добавить комментарий; затем нажимаем <<Create pull request>>,
	
	\item На вкладке <<Pull requests>> появится диалоговая форма с возможностью сделать <<Merge pull request>>, просмотреть коммит и пр.
\end{itemize}

Можно выполнить <<Merge pull request>> через командную строку
\begin{lstlisting}[
style = bash,
numbers = none
]
# Step 1: From your project repository, bring in the changes and test.
git fetch origin
git checkout -b correct-description-readme origin/correct-description-readme
git merge main

# Step 2: Merge the changes and update on GitHub.
git checkout main
git merge --no-ff correct-description-readme
git push origin main
\end{lstlisting}

После удачного <<Merge pull request>> лучше удалить соответствующие локальную и удаленную (remote) ветки
\begin{lstlisting}[
style = bash,
numbers = none	
]
# удаление локальной ветки
git branch -d <branch_name>
# удаление удаленной ветки
git push origin --delete <branch_name>
\end{lstlisting}



\subsection{Конфигурационные файлы для непрерывной интеграции}

Существуют различные инструменты непрерывной интеграции (continuous integration, CI). Вот некоторые из них
\begin{itemize}
	\item GitHub CI,
	
	\item CircleCI,
	
	\item Travis CI,
	
	\item Buildkite и т.д.
\end{itemize}

В качестве пример рассмотрим настройку непрерывной интеграции с помощью GitHub Actions.

Простой пример \texttt{ci.yaml} 
\begin{lstlisting}[
title = {\sffamily .github/workflows/ci.yaml},
style = bash,
numbers = none	
]
# This workflow will install Python dependencies, run tests and lint with a variety of Python versions
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions

name: Python package

on:
push:
branches: [ master ]
pull_request:
branches: [ master ]
jobs:
build:
runs-on: ubuntu-latest
strategy:
matrix:
python-version: [3.6, 3.7, 3.8]
steps:
- uses: actions/checkout@v2
- name: Set up Python
uses: actions/setup-python@v2
with:
python-version: ${{ matrix.python-version }}
- name: Cache pip
uses: actions/cache@v1
with:
path: ~/.cache/pip # This path is specific to Ubuntu
# Look to see if there is a cache hit for the corresponding requirements file
key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
restore-keys: |
${{ runner.os }}-pip-
${{ runner.os }}-
# You can test your matrix by printing the current Python version
- name: Display Python version
run: python -c "import sys; print(sys.version)"
- name: Install dependencies
run: |
python -m pip install --upgrade pip
pip install Cython
pip install -r requirements.txt
pip install pycocotools
pip install shapely
pip install black flake8 mypy pytest hypothesis isort pylint
- name: Run black
run:
black --check .
- name: Run flake8
run: flake8
- name: Run pylint
run: pylint iglovikov_helper_functions
- name: Run Mypy
run: mypy iglovikov_helper_functions
- name: Run isort
run: isort --profile black iglovikov_helper_functions
- name: tests
run: |
pip install .[tests]
pytest
\end{lstlisting}

Пример CI-файла посложнее
\begin{lstlisting}[
title = {\sffamily .github/workflows/ci.yaml},
style = bash,
numbers = none	
]
name: CI
on: [push, pull_request]  # триггеры

jobs:
  test_and_lint:
    name: Test and lint
    runs-on: ${{ matrix.operating-system }}
    strategy:
      matrix:
        operating-system: [ubuntu-latest, windows-latest, macos-latest]
        python-version: [3.6, 3.7, 3.8]
      fail-fast: false
    steps:
    - name: Checkout
      uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    - name: Update pip
      run: python -m pip install --upgrade pip
    - name: Install PyTorch on Linux and Windows
      if: >
        matrix.operating-system == 'ubuntu-latest' ||
        matrix.operating-system == 'windows-latest'
      run: >
        pip install torch==1.4.0+cpu torchvision==0.5.0+cpu
        -f https://download.pytorch.org/whl/torch_stable.html
    - name: Install PyTorch on MacOS
      if: matrix.operating-system == 'macos-latest'
      run: pip install torch==1.4.0 torchvision==0.5.0
    - name: Install dependencies
      run: pip install .[tests]
    - name: Install linters
      run: pip install "pydocstyle<4.0.0" flake8 flake8-docstrings mypy
    - name: Run PyTest
      run: pytest
    - name: Run Flake8
      run: flake8
    - name: Run mypy
      run: mypy .

  check_code_formatting:
    name: Check code formatting with Black
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8]
    steps:
    - name: Checkout
      uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    - name: Update pip
      run: python -m pip install --upgrade pip
    - name: Install Black
      run: pip install black==19.3b0
    - name: Run Black
      run: black --config=black.toml --check .

  check_sphinx_build:
    name: Check Sphinx build for docs
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8]
    steps:
    - name: Checkout
      uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    - name: Update pip
      run: python -m pip install --upgrade pip
    - name: Install dependencies
      run: pip install -r docs/requirements.txt
    - name: Run Sphinx
      run: sphinx-build -b html docs /tmp/_docs_build

  check_transforms_docs:
    name: Check that transforms docs are not outdated
    runs-on: ubuntu-latest
      strategy:
        matrix:
          python-version: [3.8]
      steps:
      - name: Checkout
        uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}
      - name: Update pip
        run: python -m pip install --upgrade pip
      - name: Install dependencies
        run: pip install .
      - name: Run checks
        run: python tools/make_transforms_docs.py check README.md
\end{lstlisting}


\section{Настройка непрерывной доставки (CD) с помощью Codefresh}

\section{Разработка собственных Python-пакетов для PyPI}

\subsection{Семантическое управление версиями}

Номер релиза обычно дается в формате \href{https://semver.org/}{MAJOR.MINOR.PATCH}:
\begin{itemize}
	\item MAJOR: изменяется, когда разработчики вносят обратно несовместимые изменения API,
	
	\item MINOR: изменяется, когда разработчики вносят обратно совместимые изменения; например, если какая-нибудь функция публичного API признается устаревшей или появляется обратно совместимая новая функциональность,
	
	\item PATCH: изменяется, когда разработчики исправляют обратно совместимые ошибки (x.y.Z | x > 0).
\end{itemize}

Предрелизная версия имеет более низкий приоритет, чем связанные с ней обычные версии. Предрелизная версия указывает, что версия нестабильна и может не удовлетворять предполагаемым требованиям совместимости. Примеры: \texttt{1.0.0-alpha}, \texttt{1.0.0-alpha.1}, \texttt{1.0.0-0.3.7}.

Можно указывать метаданные сборки (метаданные не входят в приоритет): \texttt{1.0.0-alpha+001}, \texttt{1.0.0+201303}, \texttt{1.0.0-beta+exp.sha.5114f85}.

Пример разрешения приоритетов: \texttt{1.0.0-alpha} < \texttt{1.0.0-alpha.1} < \texttt{1.0.0-alpha.beta} < \texttt{1.0.0-beta} < \texttt{1.0.0-beta.2} < \texttt{1.0.0-beta.11} < \texttt{1.0.0-rc.1} < \texttt{1.0.0}.

\subsection{Краткая дорожная карта разработки собственного пакета}

Дорожная карта по разработке пользовательского python-пакета для PyPI:

\begin{enumerate}
	\item Создать рабочую директорию пакета, например, с именем \texttt{advancedstatistic}: другими словами, это директория локального git-репозитория, в которой будут расположены поддиректории с python-модулями, реализующими основной функционал пакета, конфигурационные файлы, \texttt{README.md}, лицензия и пр.,
	
	\item Создать сконфигурированное виртуальное окружение,
	
	\item Инициализировать git-репозиторий с помощью \texttt{git init}
	
	\item Создать поддиректорию пакета с тем же именем что и корневая директория: то есть в локальном репозитории с именем \texttt{advancedstatistic} будет находиться\\ поддиректория \texttt{advancedstatistic} с модулями, реализующими функционал пакета; в поддиректории обязательно должен находиться файл \texttt{\_\_init\_\_.py} (без него будет неверно определена структура пакета)
	\begin{lstlisting}[
	style = bash,
	numbers = none
	]
advancedstatistic/ <-- git-репозиторий
    README.md
    LICENSE
    .git/
    .gitignore
    ...
    advancedstatistic/ <-- директория пакета с модулями
        __init__.py <-- специальный файл, который помогает определить структуру пакета
        ... <-- разные py-модули
	\end{lstlisting}
	
	\item Создать файл зависимостей для разработки пакета \texttt{requirements\_dev.txt}
\begin{lstlisting}[
title = {\sffamily requirements\_dev.txt},
style = bash,
numbers = none	
]
pip==20.2.4
pytest==6.2.1
pytest-cov==2.10.1
wheel==0.35.1
twine==3.2.0
\end{lstlisting}

    \item Установить зависимости в виртуальное окружение
\begin{lstlisting}[
style = bash,
numbers = none	
]
pip install -r requirements_dev.txt    
\end{lstlisting}

    \item Запустить процедуру сборки <<классического>> архива и whl-архива
\begin{lstlisting}[
style = bash,
numbers = none	
]
python setup.py sdist bdist_wheel
\end{lstlisting}

В локальном репозитории будут созданы следующие директории:
\begin{itemize}
	\item \texttt{dist},
	
	\item \texttt{build},
	
	\item \texttt{advancedstatistic.egg-info}.
\end{itemize}

    \item Теперь можно опубликовать пакет на TestPyPI
\begin{lstlisting}[
style = bash,
numbers = none	
]
twine upload --repository-url https://test.pypi.org/legacy/ dist/*
\end{lstlisting}

    \item Для проверки можно установить пакет с TestPyPI на локальную машину
\begin{lstlisting}[
style = bash,
numbers = none	
]
pip install --index-url https://test.pypi.org/simple/ advancedstatistic
# или если у пакета есть зависимости
pip install \
  --index-url https://test.pypi.org/simple/ \
  --extra-index-url https://pypi.org/simple advancedstatistic
\end{lstlisting}

В сеансе Python
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
>>> from advancedstatistic.Gauss import source_outliers
...
\end{lstlisting}

    \item Если все прошло без проблем, то теперь можно опубликовать пакет на PyPI
\begin{lstlisting}[
style = bash,
numbers = none	
]
twine upload dist/*
\end{lstlisting}
    \item Теперь можно отправить материалы на удаленный git-репозиторий с помощью \texttt{git push origin my\_branch}
\end{enumerate}

\subsection{Инструменты и приемы разработки пакетов}

Очень неплохое введение в процедуру оформления проекта от Игловикова \url{https://ternaus.blog/tutorial/2020/08/28/Trained-model-what-is-next.html}.

Полезные статьи по оформлению пакета
\begin{itemize}
	\item \url{https://towardsdatascience.com/10-steps-to-set-up-your-python-project-for-success-14ff88b5d13},
	
	\item \url{https://towardsdatascience.com/build-your-first-open-source-python-project-53471c9942a7}.
\end{itemize}

Для того чтобы подчеркнуть, что определенный набор пакетов устанавливается только разработчиками настоящего пакета, имя файла с зависимостями задают не как \texttt{requirements.txt}, а как \texttt{requirements\_dev.txt}.

Пример файла зависимостей
\begin{lstlisting}[
title = {\sffamily requirements\_dev.txt},
style = bash,
numbers = none	
]
pip==20.2.4
pytest==6.2.1
pytest-cov==2.10.1
wheel==0.35.1
black==20.8b1
\end{lstlisting}

ВАЖНО: здесь указываются точные версии пакетов в формате <<\texttt{major.minor.micro}>>.

Установить пакеты из файла зависимостей можно так
\begin{lstlisting}[
style = bash,
numbers = none	
]
pip install -r requirements_dev.txt
\end{lstlisting}

Файл \texttt{setup.py} -- это сценарий сборки пакета. Функция \texttt{setuptools.setup()} создаст иерархию пакета для загрузки на PyPI. Эта функция содержит информацию о пакете, номере версии и о том какие пакеты требуются пользователям

\begin{lstlisting}[
title = {\sffamily setup.py},
style = ironpython,
numbers = none	
]
from setuptools import setup, find_packages

with open("README.md", "r") as readme_file:
    readme = readme_file.read()

# этот список должен быть как можно менее ограничительным
requirements = ["ipython>=6", "nbformat>=4", "nbconvert>=5", "requests>=2"]

setup(
    name="advancedstatistic",
    version="0.0.1",
    author="Leor Finkelberg",
    author_email="leor.finkelberg@yandex.ru",
    description="A package to convert your Jupyter Notebook",
    long_description=readme,
    long_description_content_type="text/markdown",
    url="https://github.com/LeorFinkelberg/advancedstatistic.git",
    packages=find_packages(),
    install_requires=requirements,  # <-- NB
    classifiers=[
        "Programming Language :: Python :: 3.7",
        "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
    ],
)
\end{lstlisting}

\remark{
Иногда в \texttt{setup.py} можно встретить аргумент \texttt{package\_dir}. Этот параметр нужен только тогда, когда устанавливаемые пакеты находятся в папке с другим именем
}

В отличие от списка зависимостей для разработки \texttt{requirements\_dev.txt}, список \texttt{requirements} в \texttt{install\_requires=...} должен быть как можно менее ограничительным.

Подробнее об этом в \href{https://stackoverflow.com/questions/14399534/reference-requirements-txt-for-the-install-requires-kwarg-in-setuptools-setup-py/33685899#33685899}{Stack Overflow}.

В \texttt{install\_requires=...} следует включать только те пакеты, которые необходимы для работы приложения.

\remark{%
	В случае когда пакет уже установлен и требуется его обновить до последней доступной версии, следует использовать конструкцию \texttt{pip install -U package\_name}
}

Twine \url{} -- это набор утилит для безопасной публикации Python-пакетов на PyPI. \texttt{twine} нужно добавить в \texttt{requirements\_dev.txt}
\begin{lstlisting}[
title = {\sffamily requirements\_dev.txt},
style = bash,
numbers = none	
]
pip==20.2.4
pytest==6.2.1
pytest-cov==2.10.1
wheel==0.35.1
twine==3.2.0
\end{lstlisting}

Теперь в корневой директории проекта следует запустить сборку
\begin{lstlisting}[
style = bash,
numbers = none	
]
python setup.py sdist bdist_wheel
\end{lstlisting}

Будет создано несколько директорий -- \directory{dist}, \directory{build} и \directory{package\_name.egg-info}.

В директории \texttt{dist} будут лежать:
\begin{itemize}
	\item \texttt{package\_name-0.0.1-py3-none-any.whl}: whl-архив
	
	\item \texttt{advancedstatistic-0.0.1.tar.gz}: архив исходников:
	\begin{itemize}
		\item \texttt{package\_name-0.0.1/.gitignore},
		
		\item \texttt{package\_name-0.0.1/package\_name/outlier\_detection.py}
		
		\item и т.д.
	\end{itemize}
\end{itemize}

На машине пользователя \texttt{pip} будет устанавливать пакет из whl-архива (всякий раз, когда это возможно). Такие whl-архивы быстрее устанавливаются. В том случае, если \texttt{pip} не может установить пакет из whl-архива, то он будет пытаться установить пакет из архива исходников.

Теперь необходимо создать учетную запись на тестовом сервере TestPyPI \url{https://test.pypi.org/account/register/}.

ВАЖНО: пароли для тестового сервера TestPyPI и официального PyPI должны различаться.

Для безопасной публикации пакета на TestPyPi будем использовать \texttt{twine} (при публикации пакета будет предложено ввести логин и пароль)
\begin{lstlisting}[
style = bash,
numbers = none
]
twine upload --repository-url https://test.pypi.org/legacy/ dist/*
\end{lstlisting}

Если возникнут ошибки, то нужно обновить версию пакета и удалить артефакты старой сборки (директории \texttt{build}, \texttt{dist} и \texttt{egg}). Затем перестроить пакет с помощью \texttt{python setup.py sdist bdist\_wheel} и перезагрузить пакет с помощью Twine. Номера версий на платформе TestPyPI не имеет большого значения.

Чтобы каждый раз при публикации пакета не нужно было вводить логин и пароль, можно в домашней директории подготовить конфигурационный файл (\texttt{twine} будет просматривать этот файл)
\begin{lstlisting}[
title = {\sffamily \~{}/.pypirc},
style = bash,
numbers = none	
]
[distutils]
index-servers =
pypi
testpypi

[testpypi]
repository: https://test.pypi.org/legacy
username = your_username
password = your_pypitest_password

[pypi]
username = your_username
password = your_pypi_password
\end{lstlisting}

Теперь можно опубликовать свой пакет 
\begin{lstlisting}[
style = bash,
numbers = none	
]
twine upload -r testpypi dist/*  # на TestPyPI
twine upload dist/*  # на PyPI
\end{lstlisting}

Если бы пакет был установлен на PyPI, то его можно было бы установить на локальную машину как обычно \texttt{pip install package\_name}.

В случае с TestPyPI придется использовать модифицированную команду
\begin{lstlisting}[
style = bash,
numbers = none	
]
pip install --index-url https://test.pypi.org/simple my_package
\end{lstlisting}


Если нужно разрешить \texttt{pip} извлекать другие пакеты из PyPI, то нужно использовать флаг \verb|--extra-index-url|, чтобы указать на PyPI. Это полезно, когда тестируемый пакет имеет зависимости
\begin{lstlisting}[
style = bash,
numbers = none	
]
pip install \
  --index-url https://test.pypi.org/simple/ \
  --extra-index-url https://pypi.org/simple my_package
\end{lstlisting}

Если у пакета есть зависимости, то нужно использовать второй вариант.

Посмотреть информацию о пакете (размещение пакета и пр.) можно с помощью так
\begin{lstlisting}[
style = bash,
numbers = none	
]
pip show my_package
\end{lstlisting}

Например, на macOs пакеты в виртуальных окружениях устанавливаются в \directory{\~ > opt > anaconda3 > envs > env\_name > lib > python3.7 > site-packages }.

После того как пакет, например, \texttt{my\_package}, будет установлен с помощью \texttt{pip install my\_package}, в директории \texttt{... > site-packages} будет создано две поддиректории
\begin{itemize}
	\item \texttt{my\_package},
	
	\item \texttt{my\_package-0.0.2.dist-info}.
\end{itemize}





\subsection{Примеры файлов \texttt{setup.py}}

Полезные ссылки на материалы, рассказывающие о тонкостях написания \texttt{setup.py}: 
\begin{itemize}
	\item \url{https://github.com/navdeep-G/setup.py},
	
	\item \url{https://packaging.python.org/guides/distributing-packages-using-setuptools/}
\end{itemize}

Пример простого файла \texttt{setup.py} из документации Python \url{https://packaging.python.org/tutorials/packaging-projects/}
\begin{lstlisting}[
title = {\sffamily setup.py},
style = ironpython,
numbers = none	
]
import setuptools

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

setuptools.setup(
    name="example-pkg-YOUR-USERNAME-HERE", # станет именем пакета
    version="0.0.1",
    author="Example Author",
    author_email="author@example.com",
    description="A small example package",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/pypa/sampleproject",
    packages=setuptools.find_packages(),
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires='>=3.6',
)
\end{lstlisting}

Директория проекта выглядит так
\begin{lstlisting}[
style = bash,
numbers = none	
]
packaging_tutorial
 |-- LICENSE
 |-- README.md
 |-- example_pkg  # будет пакетом!!!
 |   |-- __init__.py  # <-- NB
 |-- setup.py
 |-- tests
\end{lstlisting}

В этом примере:
\begin{itemize}
	\item \texttt{name} -- имя дистрибутива пакета,
	
	\item \texttt{version} -- версия пакета (см. \href{https://www.python.org/dev/peps/pep-0440/}{PEP 440}),
	
	\item \texttt{author}/\texttt{author\_email} -- автор пакета,
	
	\item \texttt{description} -- одностроковое описание пакета,
	
	\item \texttt{long\_description } -- подробное описание пакета (читается из \texttt{README.md}),
	
	\item \texttt{url} -- URL домашней страницы пакета (как правило это ссылка на GitHub, GitLab или еще какой-нибудь сервис хостинга),
	
	\item \texttt{packages} -- это список пакетов, которые должны быть включены в дистрибутив; вместо того, чтобы перечислять каждый пакет, можно для автоматического обнаружения пакетов и подпактов использовать функцию \texttt{find\_packages()} (в данном случае функция найдет только пакет \texttt{example\_pkg}),
	
	\item \texttt{classifier} -- дополнительные метаданные о пакете.
\end{itemize}

Для создания дистрибутива пакета следует запустить следующую команду в той же директории, где лежит файл \texttt{setup.py}
\begin{lstlisting}[
style = bash,
numbers = none	
]
python setup.py sdist bdist_wheel
\end{lstlisting}

Эта команда должна в директории \texttt{dist} сгенерировать два файла
\begin{lstlisting}[
style = bash,
numbers = none	
]
dist/
  example_pkg_YOUR_USERNAME_HERE-0.0.1-py3-none-any.whl  # whl-архив
  example_pkg_YOUR_USERNAME_HERE-0.0.1.tar.gz  # tar-архив исходников
\end{lstlisting}

Для распространения пакетов рекомендуется использовать \texttt{twine}.


Другой пример файла сборки позаимствован из репозитория \url{https://github.com/navdeep-G/setup.py}
\begin{lstlisting}[
title = {\sffamily setup.py},
style = ironpython,
numbers = none	
]
#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Note: To use the 'upload' functionality of this file, you must:
#   $ pipenv install twine --dev

import io
import os
import sys
from shutil import rmtree

from setuptools import find_packages, setup, Command

# Package meta-data.
NAME = 'mypackage'
DESCRIPTION = 'My short description for my project.'
URL = 'https://github.com/me/myproject'
EMAIL = 'me@example.com'
AUTHOR = 'Awesome Soul'
REQUIRES_PYTHON = '>=3.6.0'
VERSION = '0.1.0'

# What packages are required for this module to be executed?
# Этот список должен быть как можно менее ограничительным
REQUIRED = [
    # 'requests', 'maya', 'records',
]

# What packages are optional?
EXTRAS = {
	# 'fancy feature': ['django'],
}

# The rest you shouldn't have to touch too much :)
# ------------------------------------------------
# Except, perhaps the License and Trove Classifiers!
# If you do change the License, remember to change the Trove Classifier for that!

here = os.path.abspath(os.path.dirname(__file__))

# Import the README and use it as the long-description.
# Note: this will only work if 'README.md' is present in your MANIFEST.in file!
try:
    with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f:
        long_description = '\n' + f.read()
    except FileNotFoundError:
        long_description = DESCRIPTION

# Load the package's __version__.py module as a dictionary.
about = {}
if not VERSION:
    project_slug = NAME.lower().replace("-", "_").replace(" ", "_")
    with open(os.path.join(here, project_slug, '__version__.py')) as f:
        exec(f.read(), about)  # ???
else:
    about['__version__'] = VERSION


class UploadCommand(Command):
    """Support setup.py upload."""

    description = 'Build and publish the package.'
    user_options = []

    @staticmethod
    def status(s):
        """Prints things in bold."""
        print('\033[1m{0}\033[0m'.format(s))

    def initialize_options(self):
        pass

    def finalize_options(self):
        pass

    def run(self):
        try:
            self.status('Removing previous builds…')
            rmtree(os.path.join(here, 'dist'))
        except OSError:
            pass

        self.status('Building Source and Wheel (universal) distribution…')
        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))

        self.status('Uploading the package to PyPI via Twine…')
        os.system('twine upload dist/*')

        self.status('Pushing git tags…')
        os.system('git tag v{0}'.format(about['__version__']))
        os.system('git push --tags')

        sys.exit()


# Where the magic happens:
setup(
    name=NAME,
    version=about['__version__'],
    description=DESCRIPTION,
    long_description=long_description,
    long_description_content_type='text/markdown',
    author=AUTHOR,
    author_email=EMAIL,
    python_requires=REQUIRES_PYTHON,
    url=URL,
    packages=find_packages(exclude=["tests", "*.tests", "*.tests.*", "tests.*"]),
    # If your package is a single module, use this instead of 'packages':
    # py_modules=['mypackage'],

    # entry_points={
	#     'console_scripts': ['mycli=mymodule:cli'],
	# },
    install_requires=REQUIRED,
    extras_require=EXTRAS,
    include_package_data=True,
    license='MIT',
    classifiers=[
        # Trove classifiers
        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers
        'License :: OSI Approved :: MIT License',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.6',
        'Programming Language :: Python :: Implementation :: CPython',
        'Programming Language :: Python :: Implementation :: PyPy'
    ],
    # $ setup.py publish support.
    cmdclass={
        'upload': UploadCommand,
    },
)
\end{lstlisting}

Шикарный \texttt{setup.py} файл из репозитория \texttt{aiomisc}
\begin{lstlisting}[
title = {\sffamily setup.py (эталон)},
style = ironpython,
numbers = none	
]
import os

from setuptools import setup, find_packages
from importlib.machinery import SourceFileLoader


module_name = 'aiomisc'

try:
    version = SourceFileLoader(
        module_name,
        os.path.join(module_name, 'version.py') # читаем файл version.py (см. ниже)
    ).load_module()  # вернет что-то вроде  <module 'aiomisc' from 'aiomisc/version.py'>

    version_info = version.version_info  # (11, 1, 0, 'asdfasd')
except FileNotFoundError:
    version_info = (0, 0, 0)


__version__ = '{}.{}.{}'.format(*version_info) # задаем версию на базе version.py


def load_requirements(fname):
    """ load requirements from a pip requirements file """
    with open(fname) as f:
        line_iter = (line.strip() for line in f.readlines()) # генераторное выражение
        return [line for line in line_iter if line and line[0] != '#']


setup(
    name=module_name,
    version=__version__,
    author='Dmitry Orlov',
    author_email='me@mosquito.su',
    license='MIT',
    description='aiomisc - miscellaneous utils for asyncio',
    long_description=open("README.rst").read(),
    platforms="all",
    classifiers=[
        "Framework :: Pytest",
        'Intended Audience :: Developers',
        'Natural Language :: Russian',
        'Operating System :: MacOS',
        'Operating System :: POSIX',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.5',
        'Programming Language :: Python :: 3.6',
        'Programming Language :: Python :: 3.7',
        'Programming Language :: Python :: 3.8',
        'Programming Language :: Python :: 3.9',
        'Programming Language :: Python :: Implementation :: CPython',
    ],
    packages=find_packages(exclude=['tests']),
    package_data={"aiomisc": ["py.typed"]},
    install_requires=load_requirements('requirements.txt'),
    extras_require={
    	'aiohttp': ['aiohttp'],
	    'asgi': ['aiohttp-asgi'],
	    'carbon': ['aiocarbon~=0.15'],
	    'contextvars': ['contextvars~=2.4'],
	    'develop': load_requirements('requirements.dev.txt'),
	    'raven': ['raven-aiohttp'],
	    'uvloop': ['uvloop>=0.14,<1'],
	    'cron': ['croniter~=0.3.34'],
	    ':python_version < "3.7"': 'async-generator',
    },
    entry_points={ # точка входа
	    "pytest11": ["aiomisc = aiomisc_pytest.pytest_plugin"]
    },
    url='https://github.com/mosquito/aiomisc'
)
\end{lstlisting}

\begin{lstlisting}[
title = {\sffamily version.py},
style = ironpython,
numbers = none
]
""" This file is automatically generated by distutils. """

# Follow PEP-0396 rationale
version_info = (11, 1, 0, 'g4edc986')
__version__ = '11.1.0'
\end{lstlisting}

В \texttt{SourceFileLoader} передаем имя и полный путь до модуля, который нужно загрузить, например, так
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
from importlib.machinery import SourceFileLoader

version = SourceFileLoader(
    "version.py",
    os.path.join("fakedir", "version.py")
).load_module()
version.version_info
...
\end{lstlisting}

\subsection{Точки входа в \texttt{setup.py} файлах}

Точки входа -- это методы, с помощью которых другие программы на Python могут обнаруживать динамические свойства, обеспеченные пакетом.

Рассмотрим простой пример. Пусть рабочая директория выглядит так
\begin{lstlisting}[
style = bash,
numbers = none	
]
snake_package/ # рабочая директория
  |-- snake_package/ # пакет
  |     |-- snake.py
  |-- setup.py
\end{lstlisting}

Модуль \texttt{setup.py} выглядит следующим образом
\begin{lstlisting}[
title = {\sffamily setup.py},
style = ironpython,
numbers = none	
]
from setuptools import setup

setup(
    name = "snake_package",
    entry_points = {
        "console_scripts" : [ # группа точек
            "snake = snake_package.snake:main",
        ],
    }
)
\end{lstlisting}

Здесь \texttt{snake} -- это имя утилиты командной строки (и одновременно имя точки входа), \\ \texttt{snake\_package.snake} -- это путь до модуля \texttt{snake} от корня директории, а \texttt{main} -- функция модуля \texttt{snake}.

Теперь как выглядит модуль \texttt{snake.py} 
\begin{lstlisting}[
title = {\sffamily snake.py},
style = ironpython,
numbers = none
]
simple_snake = "simple snake"
	
def main():
	print(simple_snake)
	
	
if __name__ == "__main__":
	main()
\end{lstlisting}

Теперь, если запустить 
\begin{lstlisting}[
style = bash,
numbers = none	
]
python setup.py develop
\end{lstlisting}
то в виртуальном окружении (например, \directory{C > Users > ADM > Anaconda3 > envs > env\_for\_tests > Scripts}) будут созданы следующие файлы
\begin{itemize}
	\item \texttt{snake-script.py},
	
	\item \texttt{snake.exe}.
\end{itemize}

Упрощенно можно считать так: при вызове в командной строке \texttt{snake} будет вызвана одноименная точка входа из группы \texttt{console\_scripts}, которая вызовет ассоциированную с точкой входа \texttt{snake} функцию (в данном случае \texttt{main}).

Автоматически созданный сценарий \texttt{snake-script.py} будет иметь следующее содержание
\begin{lstlisting}[
title = {\sffamily snake-script.py},
style = ironpython,
]
#!C:\Users\ADM\Anaconda3\envs\env_for_tests\python.exe
# EASY-INSTALL-ENTRY-SCRIPT: 'snake-package','console_scripts','snake'
import re
import sys

# for compatibility with easy_install; see #2198
__requires__ = 'snake-package'

try:
    from importlib.metadata import distribution
except ImportError:
    try:
        from importlib_metadata import distribution
    except ImportError:
        from pkg_resources import load_entry_point


def importlib_load_entry_point(spec, group, name):
    dist_name, _, _ = spec.partition('==')
    matches = (
        entry_point
        for entry_point in distribution(dist_name).entry_points
        if entry_point.group == group and entry_point.name == name
    )
    return next(matches).load()


globals().setdefault('load_entry_point', importlib_load_entry_point)


if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
    sys.exit(load_entry_point('snake-package', 'console_scripts', 'snake')())
\end{lstlisting}

Здесь мы сначала пытаемся различными способами включить в пространство имен модуля функцию \texttt{distribution}, затем если что-то пошло не так -- функцию \texttt{load\_entry\_point} из библиотеки \texttt{pkg\_resources}.

Если удалось импортировать функцию \texttt{distribution}, то функции \texttt{load\_entry\_point} нет в словаре \texttt{globals()} и с помощью метода \texttt{setdefault} мы создаем в этом словаре пару из отсутствующего ключа \texttt{distribution} и ссылки на функцию \texttt{importlib\_load\_entry\_point} в качестве значения по умолчанию, а затем возвращаем это значение, так сказать, в никуда. Теперь, обращаясь к функции \texttt{load\_entry\_point} будет вызываться функция \texttt{importlib\_load\_entry\_point}.

Если же функция \texttt{load\_entry\_point} была импортирована, то она будет присутствовать в словаре \texttt{globals()} и мы просто вернем значение по ключу в никуда.

Модуль \texttt{snake-script.py} запускается на прямую, поэтому условие выполняется и мы переходим к строке 32. Здесь, используя регулярное выражение
\begin{lstlisting}[
style = bash,
numbers = none
]
(-script.\pyw?|\.exe)?$
\end{lstlisting}
которое ищет выражения вида \texttt{-script.py} или \texttt{-script.pyw} или \texttt{.exe} или ничего не ищет в конце строки, замещаем найденную группу пустой строкой в строке, которую возвращает \texttt{sys.argv[0]} (это имя запущенного сценария).

Переопределяем первый элемент списка аргументов командной строки и, наконец, вызываем функцию \texttt{load\_entry\_point}:
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
# эта функция возвращает ссылку на функцию, ассоциированную с ключом snake
load_entry_point(
    "snake-package",   # имя пакета
    "console_scripts", # имя группы точек входа 
    "snake"            # имя точки входа
)  # <function snake_package.snake.main()>
\end{lstlisting}

Эта функция вернет ссылку на функцию \texttt{main} из модуля \texttt{snake.py} пакета \texttt{snake\_package}, поэтому в строке 33 функция \texttt{load\_entry\_point} вызывается как
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
load_entry_point(...)() # main() -> "simple snake"
\end{lstlisting}

В том случае, если была загружена не непосредственно функция \texttt{pkg\_resources.load\_entry\_point}, а функция по умолчанию \texttt{importlib\_load\_entry\_point}, то произойдет следующее.

Сначала имя пакета будет разбито по строке \verb|"=="| на три части с помощью строкового метода \texttt{partition}. При распаковке в переменную \texttt{dist\_name} попадет первая часть строки (до подстроки \verb|"=="|). Затем с помощью функции \texttt{distribution} от имени пакета будут извлечены точки входа
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
distribution("snake_package").entry_points
# [EntryPoint(name='snake', value='snake_package.snake:main', group='console_scripts')]
\end{lstlisting}

Далее с помощью генераторного выражения отбираем только точки входа из группы \texttt{console\_scripts} с именем \texttt{snake}.

Конструкция \texttt{next(matches).load()} возвращает ссылку на функцию \texttt{main}. Дальше все как и раньше.

Если обобщить, то в данном случае скрипт \texttt{snake-script.py} просто вызывает функцию \texttt{main()}, ассоциированную с точкой входа \texttt{snake}.

\remark{
Для простоты можно считать, что, когда мы говорим в командной строке \texttt{snake}, Python ищет точку входа с этим именем и вызывает ассоциированную с этой точкой входа функцию
}

С помощью утилиты \texttt{epi} (Entry Point Inspector) можно визуализировать точки входа
\begin{lstlisting}[
style = bash,
numbers = none	
]
# выведет список имен групп точек входа
epi group list | grep console_scripts
\end{lstlisting}

Продолжим этот пример, но теперь рассмотрим вариант пакета с зарегистрированными точками входа. Пусть \texttt{setup.py} выглядит так
\begin{lstlisting}[
title = {\sffamily setup.py главного пакета},
style = ironpython,
numbers = none	
]
from setuptools import setup

setup(
    name = "snake_package",
    entry_points = {
	    "console_scripts" : [ # группа точек входа командных сценариев
	        "snake = snake_package.snake:main", # вызывает функцию main() из snake_package/snake.py
	    ],
	    "snake_types" : [ # группа точек входа
	        "normal = snake_package.snake:normal_snake", # точка входа
	        "simple = snake_package.snake:simple_snake", # точка входа
	    ],
    }
)
\end{lstlisting}
а основной модуль так
\begin{lstlisting}[
title = {\sffamily snake\_package/snake.py},
style = ironpython,
numbers = none	
]
import pkg_resources

simple_snake = "simple_snake"

normal_snake = "normal snake"

def main():
    for entry_point in pkg_resources.iter_entry_points("snake_types"):
        print(f"{entry_point.name} --> {entry_point.load().upper()}")


if __name__ == "__main__":
    main()
\end{lstlisting}

В случае, если точка входа связанна с функцией, \texttt{entry\_point.load()} вернет ссылку на эту функцию (а не вызовет функцию!!!), а в случае переменной \texttt{entry\_point.load()} вернет значение этой переменной.

Устанавливаем пакет
\begin{lstlisting}[
style = bash,
numbers = none	
]
python setup.py develop
\end{lstlisting}

Теперь при вызове в командной строке консольного сценария \texttt{snake} будет вызвана одноименная точка из группы \texttt{console\_scripts}, которая вызовет связанную с этой точкой функцию \texttt{main} из модуля \texttt{snake\_package/snake.py}.

Функция \texttt{main} с помощью \texttt{pkg\_resources.iter\_entry\_points} просканирует окружение на предмет наличия групп точек входа с именем \texttt{snake\_types}, и извлечет из ее точек имя и ссылку на связанную функцию.

Что особенно интересно \texttt{pkg\_resources.iter\_entry\_points("snake\_types")} будет сканировать \emph{все} окружение, то есть, если в каком-то другом пакете будет объявлена группа точек входа \texttt{snake\_types} (в \texttt{setup.py}), то функция \texttt{main} нашего пакета об этом узнает
\begin{lstlisting}[
title = {\sffamily setup.py из какого-то другого пакета},
style = ironpython,
numbers = none	
]
...
setup(
    name = "foobar",
    ...
    packages = ["foobar"],
    entry_points = {
        "console_scripts" : [
            "foobar-server = foobar.server:main",
            ...
        ]
        "snake_types" : [ # регистрация группы точек входа
            "pretty = add_module:pretty_snake", # вызовет переменную из модуля add_module.py в корне пакета foobar
        ],
    }
)
\end{lstlisting}



Другой пример
\begin{lstlisting}[
title = {\sffamily setup.py},
style = ironpython,
numbers = none
]
from setuptools import setup

setup(
    name="foobar",
    entry_points={ # точки входа
        "console_scripts"  : [ # группа точек входа
            "foobar-server = foobar.server:main", # вызовет функцию main() из foobar/server.py
            "foobar-client = foobar.client:main", # вызовет функию main() из foobar/client.py
        ],
    }
)
\end{lstlisting}

Здесь \texttt{foobar-server} и \texttt{foobar-client} -- это сценарии командной оболочки. Модуль \texttt{setuptools} читает \texttt{foobar-server = foobar.server:main} как 
\begin{lstlisting}[
style = bash,
numbers = none	
]
<консольный_скрипт> = <путь к питоновскому модулю:функция>
\end{lstlisting}
создавая для каждого элемента консольную утилиту при установке пакета.

Теперь если запустить
\begin{lstlisting}[
style = bash,
numbers = none	
]
python setup.py develop
\end{lstlisting}
то в виртуальном окружении (например, \directory{C > Users > ADM > Anaconda3 > envs > env\_for\_tests > Scripts}) будут созданы следующие файлы
\begin{itemize}
	\item \texttt{foobar-client.exe},
	
	\item \texttt{foobar-client-script.py},
	
	\item \texttt{foobar-server.exe},
	
	\item \texttt{foobar-server-script.py}.
\end{itemize}

Например, файл \texttt{foobar-client-script.py} имеет следующее содержание
\begin{lstlisting}[
title = {\sffamily foobar-client-script.py},
style = ironpython,
numbers = none	
]
#!C:\Users\ADM\Anaconda3\envs\env_for_tests\python.exe
# EASY-INSTALL-ENTRY-SCRIPT: 'foobar','console_scripts','foobar-client'
import re
import sys

# for compatibility with easy_install; see #2198
__requires__ = 'foobar'

try:
    from importlib.metadata import distribution
except ImportError:
    try:
        from importlib_metadata import distribution
    except ImportError:
        from pkg_resources import load_entry_point


def importlib_load_entry_point(spec, group, name):
    dist_name, _, _ = spec.partition('==')
    matches = (
        entry_point
        for entry_point in distribution(dist_name).entry_points
        if entry_point.group == group and entry_point.name == name
    )
    return next(matches).load()


globals().setdefault('load_entry_point', importlib_load_entry_point)


if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
    sys.exit(load_entry_point('foobar', 'console_scripts', 'foobar-client')())
\end{lstlisting}


Точки входа организованны в группы: каждая группа -- это список пар ключ/значение. Пары ключ/значение используют формат \texttt{path.to.module:variable\_name}. 

Простейший путь для визуализации точек входа, доступных в пакете, -- это использование пакета Entry Point Inspector. Его можно установить, запустив \texttt{pip install entry-point-inspector}. После установки доступна утилита командной строки \texttt{epi}
\begin{lstlisting}[
style = bash,
numbers = none	
]
$ epi group list
+------------------------------------------+
| Name                                     |
+------------------------------------------+
| apscheduler.executors                    |
| apscheduler.jobstores                    |
| apscheduler.triggers                     |
| asdf_extensions                          |
| babel.checkers                           |
\end{lstlisting}

Здесь каждый элемент таблицы -- это имя группы точек входа.

Точка входа может быть использована \texttt{setuptools} для установки маленькой программы в системное окружение, которая вызывает определенную функцию одного из модулей. Используя \texttt{setuptools}, можно указать вызов функции, с которой \emph{начнется программа}, установив пару ключ / значение в точку входа группы:
\begin{itemize}
	\item ключ -- это имя устанавливаемого скрипта,
	
	\item а значение -- это путь функции (что-то вроде \texttt{my\_module.main}).
\end{itemize}

Для того чтобы работала связка с группами точек входа, требуется, чтобы главная функция пакета (как в данном случае, функция \texttt{main}) умела сканировать группу точек входа, а сами группы точек входа могут объявляться как в файле \texttt{setup.py} главного пакета, так и в файлах \texttt{setup.py} других пакетов.

Создадим cron-подобного демона \texttt{pycrond}, который позволит любой программе Python регистрировать команду и выполнять ее каждые несколько секунд путем регистрации точки входа в группе \texttt{pytimed}.

Проект выглядит так
\begin{lstlisting}[
style = bash,
numbers = none	
]
base_directory/
  |-- pytimed.py
  |-- setup.py
  |-- hello
        |-- hello.py
\end{lstlisting}

Вот реализация \texttt{pycrond} с применением \texttt{pkg\_resources} для поиска точек входа в программе под названием \texttt{pytimed.py}
\begin{lstlisting}[
title = {\sffamily pytimed.py},
style = ironpython,
numbers = none	
]
import pkg_resources
import time


def main():
    seconds_passed = 0
    while True:
        for entry_point in pkg_resources.iter_entry_points("pytimed"): # сканирует окружение на предмет наличия группы pytimed
            try:
                seconds, callable = entry_point.load()() # потому что entry_point.load() возвращает лишь ссылку на функцию, а не вызывает ее
            except:
                pass
            else:
                if seconds_passed % seconds == 0:
                    callable()
        time.sleep(1)
        seconds_passed += 1
\end{lstlisting}

\begin{lstlisting}[
title = {\sffamily hello/hello.py},
style = ironpython,
numbers = none	
]
def print_hello():
    print("Hello, world")
    
def say_hello():
    return 2, print_hello
\end{lstlisting}

Наконец, файл \texttt{setup.py}
\begin{lstlisting}[
title = {\sffamily setup.py},
style = ironpython,
numbers = none	
]
from setuptools import setup

setup(
    name = "hello",
    version = "1",
    packages = ["hello"],
    entry_points = {
	    "pytimed" : [ # группа точек входа
	        "hello = hello.hello:say_hello", # вызывается функция say_hello() модуля hello.py пакета hello
	    ],
    }
)
\end{lstlisting}

Теперь 
\begin{lstlisting}[
style = bash,
numbers = none	
]
python setup.py develop
\end{lstlisting}

Скрипт \texttt{setup.py} регистрирует точку входа в группе \texttt{pytimed} с ключом \texttt{hello} и значением, обращенным к функции \texttt{hello.hello.say\_hello}.

Как только с помощью \texttt{setup.py} устанавливается пакет -- например, через \texttt{pip install}, -- скрипт \texttt{pytimed} обнаруживает вновь добавленную точку входа.

При импорте \texttt{pytimed} отсканирует группу \texttt{pytimed} и найдет ключ \texttt{hello}. Далее он вызовет функцию \texttt{hello.hello.say\_hello}, получив два значения: количество секунд для паузы между каждым вызовом и функцию для вызова
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
>>> import pytimed
>>> pytimed.main()
Hello, world
Hello, world
...
\end{lstlisting}

Возможности, предоставляемые этим механизмом, обширны: можно без проблем создавать драйверы, устанавливать хуки и расширения.

Точки входа делают процесс поиска и динамической загрузки кода легче для развертываемого пакета, но это не единственное их применение. Любое приложение может предлагать и регистрировать точки входа или их группы для использования по своему усмотрению.

Библиотека \texttt{stevedore} обеспечивает поддержку динамических плагинов на основе ранее продемонстрированного механизма. Рассмотренный пример уже очень прост, но его можно еще упростить в следующем скрипте
\begin{lstlisting}[
title = {\sffamily pytimed\_stevedore.py},
style = ironpython,
numbers = none
]
from stevedore.extension import ExtensionManager
import time

def main():
    seconds_passed = 0
    extensions = ExtensionManager("pytimed", invoke_on_load=True)
    while True:
        for extension in extensions:
            try:
                seconds, callable = extension.obj
            except:
                pass
            else:
                if seconds_passed % seconds == 0:
                    callable()
        time.sleep(1)
        seconds_passed += 1
\end{lstlisting}

Класс \texttt{ExtensionManager}, принадлежащий \texttt{stevedore}, обеспечивает простой путь для загрузки всех расширений группы точек входа. Имя передается в качестве аргумента. Аргумент \texttt{invoke\_on\_load=True} обеспечивает, при ее нахождении, вызов каждой функции группы. Это делает результаты доступными напрямую из атрибута \texttt{obj}, принадлежащего расширению.

Если нужно загрузить и запустить только одно расширение из группы точек входа, то это можно сделать с помощью класса \texttt{stevedore.driver.DriverManager}
\begin{lstlisting}[
title = {\sffamily Применение stevedore для запуска одного расширения из точки входа},
style = ironpython,
numbers = none	
]
from stevedore.driver import DriverManager
import time

def main(name):
    seconds_passed = 0
    seconds, callable = DriverManager("pytimed", name, invoke_on_load=True).driver
    while True:
        if seconds_passed % seconds == 0:
            callable()
        time.sleep(1)
        seconds_passed += 1
        
main("hello")
\end{lstlisting}

В этом случае только одно расширение загружается и выбирается по имени. Это позволяет быстро создать систему драйверов, в которой программа загружает и использует только одно расширение.

\section{Тестирование в Python}

Хранить тесты нужно в поддиректории \texttt{tests} пакета, приложения или библиотеки, к которым они относятся. Использование иерархии в дереве теста, которая повторяет иерархию модуля, сделает тесты более управляемыми. Это значит, что тест для кода \texttt{mylib/foobar.py} необходимо размещать в \texttt{mylib/tests/test\_foobar.py}. 

Для маленьких проектов с простым применением -- пакет \texttt{pytest}. Пакет \texttt{pytest} предоставляет команду \texttt{pytest}, которая загружает каждый файл, имя которого начинается на \texttt{test\_}, и затем выполняет все функции внутри каждого файла, если они тоже начинаются на \texttt{test\_}.

Запустить тест можно так
\begin{lstlisting}[
style = bash,
numbers = none	
]
pytest -v test_true.py
\end{lstlisting}

Флаг \verb|-v| выводит имя каждого теста в отдельной строке. Если тест не пройден, вывод меняется и отображается информация об ошибке.

Тест не проходит, как только возникает исключение \texttt{AssertationError}.

Чтобы пропустить тест удобно пользоваться декоратором \texttt{pytest.mark.skip()}. Этот декоратор безоговорочно пропускает декорированную функцию, поэтому его можно использовать всегда, когда нужно пропустить тест
\begin{lstlisting}[
title = {\sffamily Пропуск тестов},
style = ironpython,
numbers = none	
]
imoprt pytest

try:
    import mylib
except ImportError:
    mylib = None
    
@pytest.mark.skip("Do not run this")
def test_fail():
    assert False
    
@pytest.mark.skipif(mylib is None, reason="mylib is not available")
def test_mylib():
    assert mylib.foobar() == 42
    
def test_skip_at_runtime():
    if True:
        pytest.skip("Finally I don't want to run it")
\end{lstlisting}


\section{Инструменты автоматического форматирования, инспектирования и анализа кода}

Список обсуждаемых инструментов:
\begin{itemize}
	\item \texttt{Deepsource}, \texttt{Deepcode}, \texttt{Codacy}, 
	
	\item \texttt{flake8},
	
	\item \texttt{black},
	
	\item \texttt{pre-commit},
	
	
\end{itemize}

Выполнить автоматический анализ кода можно с помощью следующих инструментов на базе~AI:
\begin{itemize}
	\item Deepsource \url{https://deepsource.io/}: нужно просто зарегистрироваться, например, через git-репозиторий, а затем подключить свой репозиторий для анализа; после того, как будет сделан коммит, запуститься процедура анализа на платформе DeepSource (результаты ); в репозитории будет создан специальный конфигурационный файл \texttt{.deepsource.toml},
	
	\item Deepcode \url{https://www.deepcode.ai/},
	
	\item Codacy \url{https://www.codacy.com/}
\end{itemize}

Пример конфигурационного файла для автоматического анализа кода на базе Deepsource
\begin{lstlisting}[
title = {\sffamily .deepsource.toml},
style = bash,
numbers = none	
]
version = 1

test_patterns = [
  'tests/**'
]

exclude_patterns = [

]

[[analyzers]]
name = "python"
enabled = true
runtime_version = "3.x.x"

  [analyzers.meta]
  max_line_length = 79
\end{lstlisting}

Наиболее общий инструмент инспектирования кода (так называемые линтеры) -- \texttt{flake8}. Flake8 умеет работать не только с PEP8, но и с другими правилами (кроме того поддерживате пользовательские плагины).

Для автоматического форматирования кода из командной строки или как pre-commit hook удобно использовать \texttt{black} \url{https://black.readthedocs.io/en/stable/installation_and_usage.html}.

Инструмент \texttt{black} поддерживается \texttt{vim} \url{https://black.readthedocs.io/en/stable/editor_integration.html#vim}.

Проще всего установить \texttt{black} в Vim с помощью менеджера плагинов \texttt{Vundle}.

Сначала нужно прописать в конфигурационном файле \verb|~/.vimrc| следующую строку
\begin{lstlisting}[
title = {\sffamily ~/.vimrc},
style = bash,
numbers = none	
]
Plugin 'psf/black'
\end{lstlisting}

А затем в сеансе Vim нужно набрать \texttt{:PluginInstall}. 

Если возникнет ошибка конца строки <<E492: Not an editor command: \verb|^M|>>, то в директории плагина (например, \directory{Users>leor.finkelberg>.vim>bundle>black}) можно воспользоваться конструкцией 
\begin{lstlisting}[
style = bash,
numbers = none	
]
# переконвертировать все vim-файлы в unix-формат
find . -name '*.vim' | xargs dos2unix -f
\end{lstlisting}

Утилита командной строки (и pre-commit hook) \texttt{yesqa} \url{https://pypi.org/project/yesqa/} используется для автоматического удаления ненужных комментариев вида \verb|# noqa|.

Для того чтобы перед коммитом \texttt{yesqa} проверяла код требуется включить в конфигурационный файл \texttt{.pre-commit-config.yaml} следующие строки
\begin{lstlisting}[
title = {\sffamily .pre-commit-config.yaml},
style = bash,
numbers = none	
]
...
-   repo: https://github.com/asottile/yesqa
    rev: v1.2.2
    hooks:
-   id: yesqa
...
\end{lstlisting}

Вот сложный пример \texttt{.pre-commit-config.yaml}

\begin{lstlisting}[
title = {\sffamily .pre-commit-config.yaml},
style = bash,
numbers = none	
]
exclude: _pb2\.py$
repos:
- repo: https://github.com/pre-commit/mirrors-isort
rev: f0001b2  # Use the revision sha / tag you want to point at
hooks:
- id: isort
args: ["--profile", "black"]
- repo: https://github.com/psf/black
rev: 20.8b1
hooks:
- id: black
- repo: https://github.com/asottile/yesqa
rev: v1.1.0
hooks:
- id: yesqa
additional_dependencies:
- flake8-bugbear==20.1.4
- flake8-builtins==1.5.2
- flake8-comprehensions==3.2.2
- flake8-tidy-imports==4.1.0
- flake8==3.7.9
-   repo: https://github.com/pre-commit/pre-commit-hooks
rev: v2.3.0
hooks:
- id: check-docstring-first
- id: check-json
- id: check-merge-conflict
- id: check-yaml
- id: debug-statements
- id: end-of-file-fixer
- id: trailing-whitespace
- id: flake8
- id: requirements-txt-fixer
-   repo: https://github.com/pre-commit/mirrors-pylint
rev: d230ffd
hooks:
-   id: pylint
args:
- --max-line-length=119
- --ignore-imports=yes
- -d duplicate-code
- repo: https://github.com/asottile/pyupgrade
rev: v2.7.3
hooks:
- id: pyupgrade
args: ['--py37-plus']
- repo: https://github.com/pre-commit/pygrep-hooks
rev: v1.5.1
hooks:
- id: python-check-mock-methods
- id: python-use-type-annotations
- repo: https://github.com/pre-commit/mirrors-mypy
rev: 9feadeb
hooks:
- id: mypy
args: [--ignore-missing-imports, --warn-no-return, --warn-redundant-casts, --disallow-incomplete-defs]
\end{lstlisting}

Перед коммитом полезно выполнять команду
\begin{lstlisting}[
style = bash,
numbers = none	
]
pre-commit run --all-files
\end{lstlisting}

ВАЖНО: если виртуальное окружение конструируется с помощью \texttt{conda} может возникнуть ошибка <<FileNotFoundError: [Errno 2] No such file or directory...>> \\ при запуске \verb|pre-commit run --all-files|.

Поэтому следует использовать \texttt{virtualenv} версии 20.0.33 (!)
\begin{lstlisting}[
style = bash,
numbers = none	
]
$ pip install virtualenv==20.0.33
$ virtualenv env
$ source env/bin/actiavte # для Linux/MacOs etc
$ env\Scripts\activate.bat # для Windows
\end{lstlisting}


\section{Тонкости импортирования модулей и пакетов в Python}

ОЧЕНЬ ВАЖНО: при импорте \emph{модуля} Python выполняет \emph{весь} код в этом модуле
\begin{lstlisting}[
style = bash,
numbers = none	
]
import something_module  # весь код модуля выполнился
\end{lstlisting}

ОЧЕНЬ ВАЖНО: при импорте \emph{пакета} Python выполняет модуль \verb|__init__.py| этого пакета и импортированные имена становятся частью пространства имен пакета
\begin{lstlisting}[
style = bash,
numbers = none	
]
import something_package  # выполнился модуль __init__.py этого пакета
\end{lstlisting}

Пример. Пусть структура пакетов выглядит так
\begin{lstlisting}[
style = bash,
numbers = none	
]
root_dir/
|-- packageA/
    |-- __init__.py  # from packageA import moduleA
    |                # from packageA import packageB
    |                # from packageA import foo
    |-- moduleA.py   # здесь объявлена функция mainA()
    |-- packageB/
    |   |-- __init__.py  # from packageA.packageB import moduleB
    |   |-- moduleB.py   # здесь объявлена функция mainB()
    |-- foo.py
\end{lstlisting}

То есть в модуле \verb|__init__.py| пакета \texttt{packageA} лежит такой код
\begin{lstlisting}[
title = {\sffamily packageA/\_\_init\_\_.py},
style = bash,
numbers = none	
]
# используются абсолютные пути от корня директории, из-под которой запускается
# интерактивная Python-сессия или базовый Python-сценарий
from packageA import moduleA
from packageA import packageB
from packageA import foo
\end{lstlisting}

Другими словами, при импорте пакета \texttt{packageA} будет выполнен модуль \verb|__init__.py|, который добавит в пространство имен пакета имена \texttt{moduleA}, \texttt{packageB} и \texttt{foo}. Это означает, что эти модули и пакет будут доступны через обращение к пакету \texttt{packageA}
\begin{lstlisting}[
style = bash,
numbers = none	
]
import packageA

packageA.foo
packageA.moduleA
packageA.packageB
\end{lstlisting}

Убедится в том, что при запуске главного сценария из-под корневой директории будут доступны для импорта через имя пакета \texttt{packageA} имена \texttt{moduleA}, \texttt{packageB} и \texttt{foo} можно так
\begin{lstlisting}[
title = {\sffamily вызов из-под корневой директории},
style = bash,
numbers = none
]
import pkgutil

list(pkgutil.iter_modules(["./packageA"]))
# [ModuleInfo(module_finder=FileFinder('./packageA'), name='foo', ispkg=False),
# ModuleInfo(module_finder=FileFinder('./packageA'), name='moduleA', ispkg=False),
# ModuleInfo(module_finder=FileFinder('./packageA'), name='packageB', ispkg=True)]
\end{lstlisting}

Функцию \texttt{pkgutil.iter\_modules(path=search\_path)} можно использовать, чтобы получить список всех модулей/пакетов, которые можно импортировать по заданному пути.

При импорте пакета \texttt{packageB} аналогично будет выполнен модуль \texttt{\_\_init\_\_.py} этого пакета
\begin{lstlisting}[
title = {\sffamily packageA/packageB/\_\_init\_\_.py},
style = bash,
numbers = none	
]
# используются абсолютные пути от корня директории, из-под которой запускается
# интерактивная Python-сессия или базовый Python-сценарий
from packageA.packageB import moduleB
\end{lstlisting}

Таким образом, модуль \texttt{moduleB} становится частью пространства имен пакета \texttt{packageB}. То есть при импорте пакета \texttt{packageB} к модулю \texttt{moduleB} можно будет обраться так
\begin{lstlisting}[
style = bash,
numbers = none	
]
import pkgutil

import packageA.packageB as pkB

pkB.moduleB
list(pkgutil.iter_modules(["./packageA/packageB"]))
# [ModuleInfo(module_finder=FileFinder('./packageA/packageB'), name='moduleB', ispkg=False)]
\end{lstlisting}

Первым элементом списка путей \texttt{sys.path} будет директория, в которой лежит выполняемый Python-сценарий.

Python ищет модули в следующем порядке
\begin{itemize}
	\item Модули стандартной библиотеки (например, \texttt{math}, \texttt{os} etc.),
	
	\item Модули/пакеты, указанные в \texttt{sys.path}:
	\begin{itemize}
		\item Если интерпретатор запущен в интерактивном режиме:
		\begin{itemize}
			\item \texttt{sys.path[0]} -- пустая строка, \verb|""|. Это значит, что Python будет искать в текущей директории, из-под которой был запущен интерпретатор,
		\end{itemize}
	
	    \item Если сценарий был запущен командой \texttt{python <script.py>}:
	    \begin{itemize}
	    	\item \texttt{sys.path[0]} -- это путь к \texttt{<script.py>}.
	    \end{itemize}
	\end{itemize}

    \item Директории, указанные в переменной среды \texttt{PYTHONPATH},
    
    \item Директория по умолчанию, которая зависит от дистрибутива Python.
\end{itemize}

\remark{%
При запуске сценария для \texttt{sys.path} важна не директория, из-под которой запускается сценарий, а \emph{путь к самому сценарию}
}

У модуля \verb|__init__.py| есть две функции:
\begin{itemize}
	\item Превратить папку со скриптами в импортируемый пакет модулей (до версии Python 3.3, в версиях Python 3.3+ в этом нет необходимости, так как все директории по умолчанию считаются пакетами),
	
	\item Выполнить код инициализации пакета.
\end{itemize}

Чтобы импортировать модуль/пакет из директории, которая находится не в директории запущенного сценария, этот модуль/пакет должен быть в пакете.

В момент, когда пакет или один из его модулей импортируется в первый раз, Python выполняет \verb|__init__.py| в корне пакета, если такой файл существует. Все объекты, определенные в \verb|__init__.py|, считаются частью пространства имен пакета.

\remark{%
Если Python-сценарий запускается из-под пакета (директории, содержащей или не содержащей модуля \texttt{\_\_init\_\_.py}), то модуль \texttt{\_\_init\_\_.py} этого пакета не вызывается, так как директория, в которой находится сценарий \emph{пакетом не считается}
}

При \emph{абсолютном} импорте используется полный путь от начала корневой папки проекта к нужному модулю.

При \emph{относительном} импорте используется относительный путь, начиная с местоположения текущего модуля и заканчивая местоположением интересующего модуля.

\remark{%
Как правило, рекомендуется использовать абсолютный импорт
}


\section{Автоматический анализ кода с платформой DeepsSource}

Платформа DeepSource \url{https://deepsource.io/} позволяет проводить автоматический анализ кода на анти-паттерны, проблемы с производительностью и поиск уязвимостей при каждом коммите или pull-request. Кроме того платформа отслеживает количество зависимостей, покрытие документацией и пр.

С документацией можно ознакомиться здесь \url{https://deepsource.io/docs/}.

Управлять поведением DeepSource можно с помощью конфигурационного \\файла \texttt{.deepsource.toml}
\begin{lstlisting}[
title = {\sffamily .deepsource.toml},
style = bash,
numbers = none	
]
version = 1

test_patterns = [
    "tests/**",
    "test_*.py"
]

exclude_patterns = [
    "migrations/**",
    "**/examples/**"
]

[[analyzers]]
name = "python"  # анализатор для Python
enabled = true
dependency_file_paths = ["requirements/development.txt"] # список внешних зависимостей

    [analyzers.meta]  # метаданные для анализатора
    runtime_version = "3.x.x" # версия языка
    type_checker = "mypy" # анализатор проверки типов
    max_line_length = 79  # максимально допустимая длина строки
    skip_doc_coverage = ["module", "magic", "init"] # артифакты, которые следует пропустить при расчете покрытия документации
    additional_builtins = ["_", "pretty_output"] # дополнительные модули
    
[[analyzers]]
name = "shell"  # анализатор для сценариев командной оболочки
enabled = true

    [analyzers.meta]
    dialect = "zsh"

[[analyzers]]  # анализатор для SQL
name = "sql"
enabled = true

    [analyzers.meta]
    max_line_length = 100
    tab_space_size = 4
    indent_unit = "tab"
    comma_style = "trailing"
    capitalisation_policy = "consistent"
    allow_scalar = true
    single_table_references = "consistent"
\end{lstlisting}

По умолчанию, если в репозитории обнаруживаются ниже перечисленные файлы, то они проверяются на наличие зависимостей
\begin{itemize}
	\item \texttt{Pipfile},
	
	\item \texttt{Pipfile.lock},
	
	\item \texttt{poetry.lock},
	
	\item \texttt{pyproject.toml} (если содержит разделы \texttt{[tool.poetry]} или \texttt{[tool.fit]}),
	
	\item \texttt{requirements.txt},
	
	\item \texttt{setup.py}.
\end{itemize}

Файл \texttt{.deepsource.toml} располагается в корне проекта
\begin{lstlisting}[
style = bash,
numbers = none	
]
.
|-- .deepsource.toml
|-- README.md
|-- bar
|   |-- baz.py
|-- foo.py
\end{lstlisting}

Вот исчерпывающий список возможных вариантов конфигурации
\begin{itemize}
	\item \texttt{version}: обязательное свойство; на текущий момент поддерживается только значение <<1>>
\begin{lstlisting}[
style = bash,
numbers = none	
]
version = 1
\end{lstlisting}
	
	\item \texttt{exclude\_patterns}: список шаблонов файлов и директорий, которые не должны учитываться при выполнении анализа; шаблоны строятся относительно корня проекта
\begin{lstlisting}[
style = bash,
numbers = none
]
exclude_patterns = [
    "bin/**",
    "**/node_modules/",
    "js/*/*/min.js"
]
\end{lstlisting}
	
	\item \texttt{test\_patterns}: список шаблонов файлов и директорий, содержащих тестовые файлы; шаблоны строятся относительно корня проекта,
\begin{lstlisting}[
style = bash,
numbers = none	
]
test_patterns = [
    "tests/**",
    "test_*.py"
]
\end{lstlisting}
	
	\item \texttt{analyzers}: список анализаторов (в конфигурации должен быть хотя бы один анализатор),
\begin{lstlisting}[
style = bash,
numbers = none	
]
[[analyzers]]
name = "python"
enabled = true
dependency_file_paths = [
    "requirements.txt",
    "Pipfile"
]
\end{lstlisting}
	
	\item \texttt{transformers}: список трансформеров
\begin{lstlisting}[
style = bash,
numbers = none	
]
[[transformers]]
name = "black"
enabled = true
\end{lstlisting}
\end{itemize}

\subsection{Связка DeepSource и Travis CI}

Для того чтобы платформа DeepSource имела возможность извлекать метрики покрытия кода тестами из Travis CI \url{https://www.travis-ci.com/} следует в конфигурационный \\файл \texttt{.deepsource.toml} добавить анализатор <<Test Coverage>> (см. \url{https://deepsource.io/docs/how-to/add-python-cov-ci.html})
\begin{lstlisting}[
title = {\sffamily .deepsource.toml},
style = bash,
numbers = none	
]
...
[[analyzers]]
name = "test-coverage"
enabled = true
...
\end{lstlisting}
а в конфигурационный файл \texttt{.travis.yml} следующие строки
\begin{lstlisting}[
title = {\sffamily .travis.yml},
style = bash,
numbers = none	
]
# Travis CI config
...
after_success:  # this is for DeepSource.io
  # Generate coverage report in xml format
  - coverage xml

  # Install deepsource CLI
  - curl https://deepsource.io/cli | sh

  # From the root directory, run the report coverage command
  - ./bin/deepsource report --analyzer test-coverage --key python --value-file ./coverage.xml
\end{lstlisting}

Затем на странице Travis CI, на вкладке Settings, например, \url{https://www.travis-ci.com/github/LeorFinkelberg/termostablizator/settings}, необходимо создать переменную окружения \texttt{DEEPSOURCE\_DSN} со значением Data Source Name (DNS), которое можно узнать на странице DeepSource по пути \directory{Settings > Reporting}.

Получится что-то вроде
\begin{lstlisting}[
style = bash,
numbers = none
]
DEEPSOURCE\_DSN=https://0a69347b13674f13a8ca39b9464cb682@deepsource.io
\end{lstlisting}




\section{\texttt{Python} и \LaTeX}

Для компиляции \LaTeX-документов прямо из-под \texttt{Python} можно использовать библиотеку \texttt{pylatex}\footnote{Устанавливается как обычно \texttt{pip install pylatex}} \url{https://jeltef.github.io/PyLaTeX/current/}.


\section{Градиентный бустинг}

\subsection{Общие сведения}

\subsection{Особенности реализации в пакете \texttt{sklearn}}

\subsection{Особенности реализации в пакете \texttt{XGBoost}}

Алгоритм экстремального градиентного бустинга XGBoost добавляет больше масштабируемых методов, которые задействуют многопаточность на одиночной машине и параллельную обработку на кластерах из многочисленных серверов (используя сегментирование).

Самое важное усовершенствование, вносимое алгоритмом XGBoost, по сравнению с градиентным бустингом, состоит в возможности первого управлять разреженными данными. Алгоритм XGBoost принимает разреженные данные автоматически, не храня нулевых значений в памяти. Второе преимущество XGBoost заключается в том, каким образом вычисляются значения наилучшего расщепления узлов при ветвлении дерева, при этом используется метод, который называется квантильной схемой. Этот метод преобразует данные алгоритмов взвешивания, в результате которого потенциальные расщепления сортируются на основе определенного уровня точности.

Экстремальный градиентый бустинг XGBoost является по-настоящему масштабируемым решением с различных точек зрения. XGBoost -- это новое поколение алгоритмов градиентного бустинга с серьезной доводкой исходного алгоритма бустинга деревьев. Алгоритм XGBoost обеспечивает параллельную обработку. Предлагаемая алгоритмом масштабируемость реализуется благодаря доработанным авторами несколькими параметрическими настройками и добавлениями:
\begin{itemize}
	\item алгоритм принимает \emph{разреженные данные}, в которых могут задействоваться разреженные матрицы, экономя оперативную память (отсутствует потребность в плотных матрицах) и продолжительность вычисления (нулевые значения обрабатываются особым образом),
	
	\item обучение приближенному дереву (взвешенные метод квантильной схемы), которое показывает аналогичные результаты, но за гораздо меньшее время, чем классический исчерпывающий просмотр возможных точек ветвления,
	
	\item \emph{параллельные вычисления} на одиночной машине (используя многопоточность в фазе поиска лучшего расщепления) и аналогичным образом \emph{распределенные вычисления} на нескольких машинах,
	
	\item \emph{внеядерные вычисления на одиночной машине} с привлечением решения для хранения данных под названием <<постолбцовый блок>>, которое располагает данные на диске столбцами, тем самым экономя вермя -- данные с диска поступают в том виде, в котором их ожидает алгоритм оптимизации (который оперирует векторами-столбцами).
\end{itemize} 

XGBoost довольно неплохо обрабатывает \emph{пропущенные данные}. Другие древовидные ансамбли, основанные на стандартных деревьях решений, требуют сначала \emph{импутировать}\footnote{Импутация -- процесс замещения пропущенных, некорректных или несостоятельных значений другими значениями} пропущенные данные, используя \underline{внешкальные} значения (в частности, \emph{большое отрицательное число}, например, -999999), чтобы выработать надлежащее ветвление дерева в случае пропущенных значений.

В отличие от них, алгоритм XGBoost сначала выполняет подгонку всех непропущенных значений и после создания ветвления для переменной затем решает, какая ветвь лучше всего подходит для пропущенных значений с целью уменьшения ошибки прогнозирования. Такой подход приводит к более компактным деревьям, а эффективная стратегия импутации -- к большей прогнозирующей способности.

Самые важные параметры алгоритма XGBoost:
\begin{itemize}
	\item \texttt{learning\_rate}: скорость (темп) обучения, 
	
	\item \texttt{min\_child\_weight}: более высокие значения предотвращают переподгонку и вычислительную сложность,
	
	\item \texttt{max\_depth}: максимальная глубина дерева базовых учеников,
	
	\item \texttt{subsample}: доля подвыборок обучающих экземпляров, которые берутся на каждой итерации,
	
	\item \texttt{colsample\_bytree}: доля признаков, которые используются при построении каждого дерева,
	
	\item \texttt{reg\_lambda}: $ L_2 $-регуляризация.
\end{itemize}

Алгоритм экстремального градиентного бустинга XGBoost по умолчанию параллелизирует алгоритм по всем доступным ядрами. С помощью библиотеки \texttt{joblib} можно сохранять натренированные модели и затем использовать их для прогноза
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import joblib
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

...

params = {
    'max_depth' : [4, 6, 8],
    'n_estimators' : [100],
    'min_child_weight' : range(1, 3),
    'learning_rate' : [0.1, 0.01, 0.001],
    'colsample_bytree' : [0.8, 0.9, 1.0],
    'gamma' : [0, 1]
}

xgbr = xgb.XGBRegressor(gamma=0, objective='reg:squarederror', n_jobs=-1)
gscv = GridSearchCV(
            estimator=gscv,
            param_grid=params,
            n_jobjs=-1,
            scoring='neg_mean_absolute_error',
            verbose=True
)
gscv.fit(X_train, y_train)
y_pred = gscv.predict(X_test)
joblib.dump(gscv.best_estimator_, 'grid_search_cv_best.pkl')  # в текущей директории появится pkl-файл
\end{lstlisting}



\subsubsection{Установка пакета \texttt{xgboost} на \texttt{Windows}}

Устанавливать пакет \texttt{xgboost} рекомендуется с помощью следующей команды

\begin{lstlisting}[
numbers = none
]
conda install -c anaconda py-xgboost 
\end{lstlisting}

Существует альтернативный способ установки пакета \texttt{xgboost} (разумеется он работает и для других пакетов). Для начала требуется вывести список доступных каналов (см. \pic{fig:xgboost_conda_search}), по которым будет проводиться поиск интересующего пакета (в данном случае пакета \texttt{xgboost}), а затем можно воспользоваться конструкцией

\begin{lstlisting}[
numbers = none
]
anaconda search -t conda xgboost
\end{lstlisting}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.85]{figures/xgboost_conda_search.png}
	\caption{Окно командной оболочки \texttt{cmd.exe} со списком доступных каналов, по которым будет проводиться поиск пакета \texttt{xgboost} }\label{fig:xgboost_conda_search}
\end{figure}


После, выбрав канал, можно приступать к установке пакета

\begin{lstlisting}[
numbers = none
]
conda install -c free py-xgboost
\end{lstlisting}

\subsubsection{Простой пример работы с \texttt{xgboost} и \texttt{shap}}

Решается задача бинарной классификации. Требуется построить модель, предсказывающую годовой доход заявителя по порогу \$50'000 (то есть больше или меньше \$50'000 зарабатывает заявитель в год). Используется набор данных  UCI Adult income

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import xgboost
import shap  # для оценки важности признаков вычисляются значения Шепли (Shapley value)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

shap.initjs()

X, y = shap.datasets.adult()
X_display, y_display = shap.datasets.adult(display=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)
d_train = xgboost.DMatrix(X_train, label=y_train)
d_test = xgboost.DMatrix(X_test, label=y_test)

params = {
    'eta' : 0.01,
    'objective' : 'binary:logistic',
    'subsample' : 0.5,
    'base_score' : np.mean(y_train),
    'eval_metric' : 'logloss'        
}
model = xgboost.train(params, d_train,
                      num_boost_round = 5000,  # число итераций бустинга
                      evals = [(d_test, 'test')],
                      verbose_eval=100,  # выводит результат на каждой 100-ой итерации бустинга
                      early_stopping_rounds=20)

xgboost.plot_importance(model)
\end{lstlisting}

На \pic{fig:xgboost_plot}, \pic{fig:xgboost_plot_cover} и \pic{fig:xgboost_plot_gain} изображены графики важности признаков.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{figures/xgboost_plot.pdf}
	\caption{График важности признаков \lstinline{xgboost.plot_importance(model)},\\построенный с помощью пакета \texttt{xgboost} }\label{fig:xgboost_plot}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{figures/xgboost_plot_cover.pdf}
	\caption{График важности признаков \lstinline{xgboost.plot_importance(model, importance_type='cover')},\\построенный с помощью пакета \texttt{xgboost} }\label{fig:xgboost_plot_cover}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{figures/xgboost_plot_gain.pdf}
	\caption{График важности признаков \lstinline{xgboost.plot_importance(model, importance_type='gain')},\\построенный с помощью пакета \texttt{xgboost} }\label{fig:xgboost_plot_gain}
\end{figure}

Следует иметь в виду, что в библиотеке \texttt{xgboost} поддерживается три варианта вычисления важности признаков (см.~\href{https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27}{Interpretable Machine Learning with XGBoost}):

\begin{itemize}
	\item \texttt{weight}: общее число сценариев по всем деревьям, когда $i$-ый признак используется для расщепления обучающего набора данных,
	
	\item \texttt{cover}: общее число сценариев по всем деревьям, когда $i$-ый признак используется для расщепления набора данных, взвешенное по числу точек обучающего набора данных, которые проходят через эти расщепления,
	
	\item \texttt{gain}: среднее снижение потерь на обучающем наборе данных, полученное при использовании $i$-ого признака.
\end{itemize}

Еще один простой типовой пример использования библиотеки \texttt{xgboost}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import xgboost
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error

boston = load_boston()
X, y = boston.data, boston.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)

xgbr = xgboost.XGBRegressor(verbosity=0)
xgbr.fit(X_train, y_train)

score = xgbr.score(X_train, y_train)
scores = cross_val_score(xgbr, X_train, y_train, cv=5)
kfold = KFold(n_splits10, shuffle=True)
kf_cv_scores = cross_val_score(xgbr, X_train, y_train, cv=kfold)

y_pred = xgbr.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
\end{lstlisting}


\subsection{Особенности реализации в пакете \texttt{LightGBM}}

\subsection{Особенности реализации в пакете \texttt{CatBoost}}


\section{Потоки и процессы. Глобальная блокировка интерпретатора}

\emph{Процесс} -- просто программа в ходе ее выполнения. \emph{Потоки} подобны процессам, за исключением того, что все они выполняются в пределах одного и того же процесса, следовательно используют один и тот же контекст. 

Все потоки, организованные в одном процессе, используют общее пространство данных с основным процессом, поэтому могут обмениваться информацией или взаимодействовать друг с другом с меньшими сложностями по сравнению с отдельными процессами. Потоки, как правило, выполняются параллельно.

Если два или несколько потоков получают доступ к одному и тому же фрагменту данных, то в зависимости от того, в какой последовательности происходит доступ, могут возникать несогласованные результаты \cite[\strbook{184}]{chun:2015}.

Выполнением кода Python управляет виртуальная машина Python (называемая также \emph{главным циклом интерпретатора}). Язык Python разработан таким образом, чтобы в этом главном цикле мог выполняться \emph{только один поток управления}.

В интерпретаторе Python могут эксплуатироваться \emph{несколько потоков}, но в каждый отдельный момент времени интерпретатором выполняется \emph{строго один поток} \cite[\strbook{185}]{chun:2015}.

Для управления доступом к виртуальном машине Python применяется \emph{глобальная блокировка интерпретатора} (GIL). Именно эта блокировка обеспечивает то, что выполняется \emph{один и только один поток}.

Вообще говоря, применение нескольких потоков в программе может способствовать ее улучшению. Однако в интерпретаторе Python применяется глобальная блокировка, которая накладывает свои ограничения, поэтому \emph{многопоточная} организация является более подходящей для приложений, ограничиваемых пропускной способностью ввода-вывода\footnote{Так называемые IO-bound задачи} (при вводе-выводе происходит освобождение глобальной блокировки интерпретатора, что способствует повышению степени распараллеливания), а не приложений, ограничиваемых пропускной способностью процессора\footnote{CPU-bound задачи}. В последнем случае для достижения более высокой степени распараллеливания необходим иметь возможность \emph{параллельного выполнения процессов} несколькими ядрами или \emph{процессорами} \cite[\strbook{229}]{chun:2015}.

Глобальная блокировка GIL защищает состояние интерпретатора от вытесняющей (приоритетной многопоточности), когда один поток пытается взять на себя управление программой, прерывая другой поток. Если такое прерывание происходит в неподходящий момент времени, то это может разрушить состояние интерпретатора.

\textbf{Выводы}:
\begin{itemize}
	\item для приложений, ограниченных пропускной способностью ввода-вывода следует использовать
	\begin{itemize}
		\item многопоточность,
		
		\item приемы асинхронного программирования,
	\end{itemize}

    \item для приложений, ограниченных пропускной способностью процессора следует иметь возможность выполнения процессов несколькими процессорами.
\end{itemize}


\section{Форматирование строк в языке \texttt{Python}}

Пример форматирования строк в \texttt{Python} 

\begin{lstlisting}[
style = ironpython,
numbers = none
]
'{:*>+12.3f}, {:#^+17.5G}, {!r}'.format(
                                     math.pi,
                                     -math.exp(1)*10**(+6),
                                     type(list)  # для этого объекта будет
                                                 # использована функция repr()
                                 )
# "******+3.142, ###-2.7183E+06###, <class 'type'>"
\end{lstlisting}

Часть, стоящая после двоеточия, называется \emph{спецификатором формата} \cite[\strbook{283}]{ramalho:python-2016}. Полезные приемы форматирования можно найти в \cite{prohorenok:python-2016}.

В \texttt{Python} f-строки поддерживают вложенные элементы \verb|{...}|. Например, выведем числа $ n_i $ в едином формате, вычисляемые по формуле
$$
    n_i = (-1)^{i} \, \pi^{(-1)^i\,B}, \quad i = (1, \dots, m).
$$
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import math

B = 15
for i in range(1, 5+1):
    n = (-1)**i*math.pi**((-1)**i*B)
    print(f'This is pi with {i} decimal places: {n:#>+15.{i}e}.')
    # вложенный элемент {...} может находится только в части спецификатора формата, после ':'
# вывод
This is pi with 1 decimal places: #######-3.5e-08.
This is pi with 2 decimal places: ######+2.87e+07.
This is pi with 3 decimal places: #####-3.489e-08.
This is pi with 4 decimal places: ####+2.8658e+07.
This is pi with 5 decimal places: ###-3.48941e-08.
\end{lstlisting}

\section{SSH-клиент в браузере}

В работе клиенты используют протокол SSH (Secure Shell) -- сетевой протокол, позволяющий осуществлять удаленное управление различными операционными системами. Поддерживает туннелирование TCP-соединений для передачи файлов и различные алгоритмы шифрования, благодаря чему возможна безопасная передача других протоколов через SSH-туннели.

Приложение \href{https://chrome.google.com/webstore/detail/secure-shell-app/pnhechapfaindjhompbnflcldabbghjo?hl=ru}{\ttfamily Secure Shell App} представляет собой эмулятор терминала, совместимый с xterm и SSH-клиент для Chrome. Он работает путем соединения SSH-команд, портированных в Google Native Client с эмулятором терминала hterm, что позволяет приложению предоставить клиенту Secure Shell прямо в браузере, не полагаясь на внешние прокси.

Установить SSH-клиент еще можно с помощью приложения \href{https://www.termius.com/}{Termius}. Поддреживает Windows, MacOS, Linux.


\section{Большие данные в Hadoop}

Hadoop это платформа для распределенного хранения и распределенной обработки больших данных.

\texttt{Hadoop} лучше всего подходит для:
\begin{itemize}
	\item Для хранения и обработки \emph{неструктурированных данных} объемом от \underline{1 терабайта} -- такие массивы сложно и дорого хранить в локальном хранилище,
	
	\item Для компонуемых вычислений -- когда нужно собрать множество схожих разрозненных данных в одно целое. Также подходит для выделения полезной информации из массива лишней информации,
	
	\item Для пакетной обработки, обогащения данных и ETL -- извлечения информации из внешних источников, ее переработки и очистки под потребности компании, последующей загрузки в базу данных.
\end{itemize}

\section{Теорема Байеса}

Пусть $ X $ -- случайная величина, ее возможное значение (или реализацию) будем обозначать через $ x $. Если $ \vec{X} = (X_1, \dots, X_n) $ -- случайный вектор, то его реализация -- $ \vec{x} = (x_1, \dots, x_n) $.

Для того чтобы охарактеризовать случайную величину, необходимо задать распределение вероятностей по ее возможным значениям. Для осуществления этого используется понятие \emph{функции распределения} вероятностей, которое является универсальным инструментом, пригодным для изучения любой случайной величины, одномерной или многомерной, и непрерывного, дискретного или смешанного типа.

Если $ X $ -- одномерная случайная величина непрерывного типа с бесконечным числом возможных значений на действительной оси $ \mathbb{R}^1 = \{x: -\infty < x < +\infty\} $, то она характеризуется функцией распределения вероятностей, которая определяется в виде
\begin{align*}
	F_X(x) = \mathbf{P}(X \leqslant x), \, x \in \mathbb{R}^1.
\end{align*}

Другими словами, функция распределения непрерывной случайной величины это вероятность события, состоящая в том, что случайная величина $ X $ примет значение меньшее или в частном случае равное некоторому значению $ x $, т.е. вероятность события, что случайная величина окажется левее.

Иногда удобнее описывать случайную величину $ X $ одномерной \emph{плотностью распределения вероятностей} $ f_X(x) = F_X^{'}(x), \, x \in \mathbb{R}^1 $ .

Для описания случайного вектора $ \vec{X} = (X_1, \dots, X_n) $ используют функцию $ n $ переменных, которая в точке $ (x_1, \dots, x_n) \in \mathbb{R}^n $, где $ \mathbb{R}^n $ обозначает $ n $-мерное евклидово пространство, определяется с помощью вероятности совместного осуществления событий в квадратных скобках, то есть функция распределения случайного вектора $ \vec{X} = (X_1, \dots, X_n) $ определяется в виде
\begin{align*}
	F_{X_1, \dots, X_n}(x_1, \dots, x_n) = F_{\vec{X}}(x_1, \dots, x_n) = \mathbf{P}[\,X_1 \leqslant x_1; X_n \leqslant x_n\,], \, (x_1, \dots, x_n) \in \mathbb{R}^n.
\end{align*}

Итак, для того чтобы охарактеризовать случайную величину $ X $, необходимо задать ее функцию распределения вероятностей.

Как известно, различают дискретные и непрерывные случайные величины. Две случайные величины называются \emph{независимыми}, если
\begin{align*}
	p(x,y) = p(x)p(y),
\end{align*}
где $ p(x) $ и $ p(y) $ -- плотности распределения непрерывных случайных величин.

Чтобы получить обратно из совместной вероятности вероятность того или иного исхода одной из случайных величин, нужно просуммировать по другой (этот процесс часто называют маргинализацией)
$$
p(x) = \sum_{y} p(x, y).
$$

В случае непрерывных случайных величин получается, что мы фактически проецируем двумерное распределение -- поверхность в трехмерном пространстве -- на одну из осей, получая функцию от одной переменной
$$
p(x) = \int_{Y} p(x, y)dy.
$$

\emph{Условная вероятность} $ p(x | y) $ -- вероятность наступления одного события, если известно, что произошло другое. Формально ее обычно определяют так
$$
p(x | y) = \dfrac{ p(x, y) }{p(y)}.
$$

Аналогично можно определить \emph{условную независимость}: $ x $ и $ y $ условно независимы при условии $ z $, если
\begin{align*}
	p(x, y | z) = p(x | z)p(y | z).
\end{align*}

\subsection{Регистрация пользовательских функций выхода из приложения}

Удобно использовать встроенный модуль \texttt{atexit} для регистрации пользовательских функций, которые вызываются при выходе из приложения. Например
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
import atexit


def hello_fun():
    print('Hello message')

def exit_fun():
    print('New exit message')

atexit.register(exit_fun)
hello_fun()
\end{lstlisting}


\section{Глубокое обучение}

\subsection{Функции активации}

Без функции активации (например, такой как ReLU) полносвязный слой \texttt{keras.layers.Dense} сможет обучаться только на \emph{линейных} (аффинных) преобразованиях входнных данных: пространство гипотез было бы совокупностью всех возможных линейных преобразований входных данных.

Такое пространство гипотез слишком ограничено, и наложение нескольких слоев представлений друг на друга не приносило бы никакой выгоды, потому что \emph{глубокий стек линейных слоев} все равно реализует \emph{линейную операцию}: добавление новых слоев не расширяет пространство гипотез.

Чтобы получить доступ к более обширному пространству гипотез, дающему дополнительные выгоды от увеличения глубины представлений, необходимо применить \emph{нелинейную}, или функцию активации.

Наконец, нужно выбрать \emph{функцию потерь} и \emph{оптимизатор}. Пусть для определенности перед нами стоит задача бинарной классификации и результатом работы сети является вероятность (сеть заканчивается одномодульным слоем с сигмоидной функцией активации). В этом случае предпочтительнее использовать функцию потерь \texttt{binary\_crossentropy}. Перекрестная энтропия обычно дает более качественные результаты, когда результатами работы модели являются вероятности.

\emph{Перекрестная энтропия} -- мера расстояния между распределением вероятностей, или между фактическими данными и предсказаниями.

\begin{align*}
	H(p,q) \bydef H(p) + D_{KL}(p||q),
\end{align*}
где $ H(p) $ -- энтропия\footnote{Мера неопределенности некоторой системы} $ p $, $ D_{KL}(p||q) $ -- дивергенция Кульбака-Лейблера\footnote{Дивергенция Кульбака-Лейблера -- неотрицательнозначный фукнционал, являющийся несимметричной мерой удаленности друг от друга двух вероятностных распределений, определенных на общем пространстве элементарных событий} от $ p $ и $ q $ (она же относительная энтропия).

Для дискретных $ p $ и $ q $
\begin{align*}
	H(p,q) = - \sum_{x} p(x) \log q(x).
\end{align*}

Для непрерывного распределения
\begin{align*}
	H(p,q) = - \int_{X} p(x) \log q(x) dx.
\end{align*}

Нужно учесть, что, не смортя на формальную аналогию функционалов для непрерывного и дискретного случаев, они обладают разными свойствами и имеют разный смысл. Непрерывный случай имеет ту же специфику, что и понятие дифференциальной энтропии.

Настраиваем модель оптимизатором \texttt{rmsprop} и функцией потерь \texttt{binary\_crossentropy}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
model.compile(optimizator='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
\end{lstlisting}

Или так, если нужно передать дополнительные параметры настройки
\begin{lstlisting}[
style = ironpython,
numbers = none
]
from keras import optimizers
from keras import metrics
from keras import losses
# настройка оптимизатора
model.compile(
    optimizer=optimizers.RMSprop(lr=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# использование нестандартных функций потерь и метрик
model.compile(
    optimizer=optimizers.RMSprop(lr=0.001),
    loss=losses.binary_crossentropy,
    metrics=[metrics.binary_crossentropy]
)
\end{lstlisting}

Чтобы проконтролировать точность модели во время обучения на данных, которые она прежде не видела, создадим проверочный набор данных, выбрав 10000 образцов из оригинального набора обучающих данных.
\begin{lstlisting}[
style = ironpython,
numbers = none
]
x_val = x_train[:10000]
partial_x_train = x_train[10000:]

y_val = y_train[:10000]
partial_y_train = y_train[10000:]
\end{lstlisting}

Теперь  проведем обучение модели в течение 20 эпох (выполнив 20 итераций по всем образцам в тензорах \texttt{x\_train}, \texttt{y\_train}) пакета по 512 образцов. В тоже время будем следить за потерями и точностью на 10000 отложенных образцах. Для этого достаточно передать проверочные данные в аргументе \texttt{validation\_data}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
model.compile(
    optimizer='rmsprop',
    loss='binary_crossentropy',
    metrics=['acc']
)
history = model.fit(
    partial_x_train,
    partial_y_trian,
    epochs=20,  # 20 пробегов по обучающему набору данных
    batch_size=512,
    validation_data=(x_val, y_val) # вычисляем потерю и точность в конце каждой эпохи
)
\end{lstlisting}

\emph{В конце каждой эпохи} обучение приостанавливается, потому что модель вычисляет \emph{потерю} и \emph{точность} на 10000 образцах проверочных данных.

\subsection{Стохастический градиентный спуск}

Стохастический градиентный спуск это метод поиска \emph{локального} экстремума. Идея состоит в следующем:
\begin{itemize}
	\item Извлекается пакет обучающих экземпляров \texttt{x} и соответствующих целей \texttt{y},
	
	\item Сеть обрабатывает пакет \texttt{x} и получает пакет предсказаний \texttt{y\_pred},
	
	\item Вычисляются потери сети на пакете, дающие оценку несовпадания между \texttt{y\_pred} и \texttt{y},
	
	\item Вычисляется градиент потерь для параметров сети (обратных проход),
	
	\item Параметры корректируются на небольшую величину в направлении антиградиента, и тем самым снижают потери.
\end{itemize}

Сколько ни придумывай хитрых способов ускорить градиентный спуск, обойти небольшие локальные минимумы, выбраться из ущелий, мы все равно не сможем изменить тот факт, что градиентный спуск -- это метод местного значения, и ищет он только \emph{локальный} минимум/максимум.

Строго говоря, это реализация \emph{минипакетного стохастического градиентного спуска}. А истинный стохастический градиентный спуск на каждой итерации использует единственный образец и цель, а не весь пакет данных.

Здесь термин стохастический относится к тому, что \emph{каждый пакет} данных выбирается \emph{случайно}.

Обучим новую сеть с нуля 
\begin{lstlisting}[
style = ironpython,
numbers = none
]
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
              
model.fit(x_train, y_train, epochs=4, batch_size=512)
results = model.evaluate(x_test, y_test)

# сделать прогноз
model.predict(x_test)
\end{lstlisting}

В задачах бинарной классификации в конце нейросети должен находиться полносвязанный слой \texttt{Dense} с одним нейроном и функцией активации \texttt{sigmoid}: результатом работы сети должно быть скалярное значение в диапазоне между 0 и 1, представляющее собой вероятность.

С таким скалярным результатом, получаемым с помощью сигмоидной функции, в задачах бинарной классификации следует использовать функцию потерь \texttt{binary\_crossentropy}.

В общем случае оптимизатор \texttt{rmsprop} является наиболее подходящим выбором для любого типа задач.



\section{Хэшируемые пользовательские классы в языке \texttt{Python}}

Чтобы класс был хэшируемым\footnote{Обычно говорят, что объект называется хэшируемым если i) у него есть хэш-значение, которое не изменяется пока объект существует, и ii) объект поддерживает сравнение с другими объектами. Однако на мой взгляд лучше сказать, что объект является хэшируемым, если его структура не может изменяется и он поддерживает сравнение с другими объектами}, следует реализовать метод \texttt{\_\_hash\_\_}. Нужно также, чтобы векторы были \emph{неизменяемыми}. И этого можно добиться, сделав компоненты \texttt{x} и \texttt{y} свойствами, доступными только для чтения.

\begin{lstlisting}[
title = {\sffamily Пример неизменяемого, но нехэшируемого класса},
style = ironpython,
emph = {__init__, x, y, __iter__, __repr__, __str__, angle, __format__,
__bytes__, __eq__, __abs__, __bool__},
numbers = none
]
import array
import math

class Vector2d:
    '''
    Неизменяемый, но еще нехэшируемый класс
    '''
    typecode = 'd'

    def __init__(self, x, y):
        self.__x = x  # закрытый атрибут экземпляра класса
        self.__y = y  # закрытый атрибут экземпляра класса

    # открытое свойство; прочитать значение `x` можно, но нельзя передать новое значение
    @property  
    def x(self):
        return self.__x

    # открытое свойство; прочитать значение `y` можно, но нельзя передать новое значение
    @property  
    def y(self):
        return self.__y

    def __iter__(self):
        return (i for i in (self.x, self.y))

    def __repr__(self):
        class_name = type(self).__name__
        return '{}({!r}, {!r})'.format(class_name, *self)

    def __str__(self):
        return str(tuple(self))

    def angle(self):
        return math.atan2(self.y, self.x)

    def __format__(self, fmt_spec = ''):  # пользовательский формат
        if fmt_spec.endswith('p'):  # если спецификатор формата заканчивается на 'p',
                                    # то координаты выводятся в полярном формате
            fmt_spec = fmt_spec[:-1]
            coords = (abs(self), self.angle())
            outer_fmt = '<{}, {}>'
        else:
            coords = self
            outer_fmt = '({}, {})'
        components = (format(c, fmt_spec) for c in coords)
        return outer_fmt.format(*components)

    def __bytes__(self):
        return (bytes([ord(self.typecode)]) + bytes(array(self.typecode, self)))

    def __eq__(self, other):
        return tuple(self) == tuple(other)

    def __abs__(self):
        return math.hypot(self.x, self.y)

    def __bool__(self):
        return bool(abs(self))
\end{lstlisting}

То есть здесь декоратор \texttt{@property} помечает метод чтения свойств, который возвращает значение закрытого атрибута экземпляра класса \texttt{self.\_\_x} или \texttt{self.\_\_y}.

Так как в реализации класса есть метод \texttt{\_\_format\_\_}, можно печатать класс управляя форматом, например,

\begin{lstlisting}[
title = {\sffamily Пример использования класса с реализованным методом \texttt{\_\_format\_\_}},
style = ironpython,
numbers = none
]
>>> v1 = Vector2d(10, 5)
>>> '{:*^+12.3gp}'.format(v1)  # '<***+11.2****, ***+0.464***>'
>>> '{:.3f}'.format(v1)  # '(10.000, 5.000)'
\end{lstlisting}

Наконец, можно реализовать метод \texttt{\_\_hash\_\_}. Он должен возвращать \texttt{int} и в идеале учитывать хэши объектов-атрибутов, потому что у равных объектов хэши также должны быть одинаковыми.

В документации по специальному методу \texttt{\_\_hash\_\_} рекомендуется объединять хэши компонентов с помощью побитового оператора\footnote{Побитовые операторы рассматривают операнды как бинарные последовательности} \emph{исключающего ИЛИ} (\texttt{\^}) \cite[\strbook{287}]{ramalho:python-2016}

\begin{lstlisting}[
style = ironpython,
emph = {__hash__},
numbers = none
]
...
def __hash__(self):
    return hash(self.__x) ^ hash(self.__y)  # побитовое исключающее ИЛИ
\end{lstlisting}

Теперь класс \texttt{Vector2d} стал \emph{хэшируемым}.

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> v1 = Vector2d(3, 4)
>>> v2 = Vector2d(3.1, 4.2)
>>> hash(v1), hash(v2)  # (7, 384307168202284039)
>>> set([v1, v2])  # {Vector2d(3, 4), Vector2d(3.1, 4.2)}
\end{lstlisting}

\remark{
Строго говоря, для создания хэшируемого типа необязательно вводить свойства или как-то иначе защищать атрибуты экземпляра класса от изменения. Требуется только корректно реализовать методы \texttt{\_\_hash\_\_} и \texttt{\_\_eq\_\_}. Но хэш-значения экземпляра никогда не должно изменяться \cite[\strbook{288}]{ramalho:python-2016}
}

\section{Как интерпретировать связь между именем функции и объектом функции в \texttt{Python}}

Рассмотрим класс, который печатает выводимые в терминал строки в обратном порядке

\begin{lstlisting}[
style = ironpython,
emph = {LookingGlass, __enter__, reverse_write, __exit__},
]
class LookingGlass:
    def __enter__(self):
        import sys
        # атрибут экземпляра класса self.original_write -> объект функции sys.stdout.write
        self.original_write = sys.stdout.write  
        # переменная sys.stdout.write -> объект функции self.reverse_write
        sys.stdout.write = self.reverse_write  
        return 'jabberwocky'.upper()
        
    def reverse_write(self, text):
        self.original_write(text[::-1])
        
        
    def __exit__(self, exc_type, exc_value, traceback):
        import sys
        # переменная sys.stdout.write "через" атрибут экземпляра self.original_write 
        # ссылается на объект функции sys.stdout.write
        sys.stdout.write = self.original_write
\end{lstlisting}

В методе \texttt{\_\_enter\_\_} есть несколько неочевидных нюансов. В строке 4 атрибут экземпляра класса \texttt{self.original\_write} получает ссылку на метод \texttt{write} стандартного потока вывода, а в строке 5 <<как бы метод>> \texttt{sys.stdout.write} получает ссылку на метод экземпляра класса \texttt{self.reverse\_write} и кажется, что должен был бы образоваться рекурсивный вызов, но на самом деле это не так. Дело в том, что значение имеет с какой стороны от оператора \texttt{=} стоит имя функции: если слева, то это \emph{имя переменной}, а если справа, то это \emph{объект функции}.

Итак, по порядку: в строке 4 атрибут экземпляра класса \texttt{self.original\_write} получает ссылку на \emph{объект функции} \texttt{sys.stdout.write}, а в 5-ой строке \emph{переменная} \texttt{sys.stdout.write} получает ссылку на \emph{объект функции} (метод экземпляра класса) \texttt{self.reverse\_write}, который <<через>> атрибут экземпляра \texttt{self.original\_write} вызывает \emph{объект функции} \texttt{sys.stdout.write}.

А в строке 18, мы возвращаем все как было, т.е. \emph{переменная} \texttt{sys.stdout.write} получает ссылку на \emph{объект функции} \texttt{sys.stdout.write}.

Рассмотрим более простой пример (см. \pic{fig:variable_with_link_function})

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> def f(): pass  # переменная f -> объект функции f()
>>> def g(): pass  # переменная g -> объект функции g()
# модель: переменная -> объект
>>> a = f  # переменная a -> объект функции f()
>>> f = g  # переменная f -> объект фукнции g()
# НИКАКОЙ ТРАНЗИТИВНОСТИ!
>>> a  # <function __main__.f()>
>>> f  # <function __main__.g()>
\end{lstlisting}

То есть, когда объявляется функция, например, \texttt{def f(): pass}, то создается \emph{переменная} \texttt{f}, которая получает ссылку на \emph{объект функции} \texttt{f()}.

\remark{
Даже если используется одно и тоже имя \texttt{f}: слева от оператора присваивания \texttt{f} -- это \emph{переменная}, а справа от оператора \texttt{f} -- это \emph{объект} (например, объект функции), так как в \texttt{Python} переменные ссылаются только на объекты!
}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.85]{figures/variable_with_link_function.png}
	\caption{ Схема, описывающая связи между именами функций и их объектами }\label{fig:variable_with_link_function}
\end{figure}

\section{Использование \texttt{@contextmanager}}

Если \emph{генератор} снабжен декоратором \texttt{@contextmanager}, то \texttt{yield} разбивает тело функции на две части:

\begin{itemize}
	\item все, что находится до \texttt{yield}, исполняется в начале блока \texttt{with}, когда интерпретатор вызывает метод \texttt{\_\_enter\_\_} ,
	
	\item а все, что находится после \texttt{yield}, выполняется при вызове метода \texttt{\_\_exit\_\_} в конце блока.
\end{itemize}

Например,

\begin{lstlisting}[
style = ironpython,
title = {\sffamily неудачный пример},
emph = {looking_glass, reverse_write}
]
# mirror_gen.py
import contextlib

@contextlib.contextmanager  # декорируем генераторную функцию
def looking_glass():  # генераторная функция
    import sys
    original_write = sys.stdout.write  # (1)
    
    def reverse_write(text):  # замыкание
        original_write(text[::-1])  # здесь original_write -- свободная переменная
    
    sys.stdout.write = reverse_write  # (2)
    # все что выше `yield` выполняется в начале блока with
    yield 'jabberwocky'.upper()  # (3)
    # все что ниже `yield` выполняется в конце блока with
    sys.stdout.write = original_write  # (4)
\end{lstlisting}

Комментарии к коду:

\begin{itemize}
	\item (1) -- локальная \emph{переменная} \texttt{original\_write} получает ссылку на \emph{объект функции} (вернее на объект метода) стандартного потока вывода; теперь вызывая \texttt{original\_write} мы будет вызывать \texttt{sys.stdout.write},
	
	\item (2) -- переменная \texttt{write} из подмодуля \texttt{stdout} модуля \texttt{sys} получает ссылку на \emph{замыкание} \texttt{reverse\_write} (функцию с расширенной областью видимости, которая включает все неглобальные переменные); теперь, когда мы вызываем \texttt{sys.stdout.write} будет вызываться \\ \texttt{reverse\_write}, который в свою очередь будет вызывать \texttt{original\_write}, вызывающий метод \texttt{sys.stdout.write} и передавать ему обращенную строку,
	
	\item (3) -- здесь функция приостанавливается на время выполнения блока \texttt{with},
	
	\item (4) -- когда поток выполнения покидает блок \texttt{with} любым способом, выполнение функции возобновляется с места, следующего за \texttt{yield}; в данном случае восстанавливается исходный метод \texttt{sys.stdout.write}
\end{itemize}

Пример работы функции

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> from mirror_gen import looking_glass

>>> with looking_glass() as what:
        print('Alice, Kitty and Snowdrop')  # pordwonS dna yttiK ,ecilA
        print(what)                         # YKCOWREBBAJ
\end{lstlisting}

По существу декоратор \texttt{@contextlib.contextmanager} обертывает функцию классом, который реализует методы \texttt{\_\_enter\_\_} и \texttt{\_\_exit\_\_}\footnote{Этот класс называется \texttt{\_GeneratorContextManager}}.

Метод \texttt{\_\_enter\_\_} этого класса выполняет следующие действия \cite[\strbook{488}]{ramalho:python-2016}:

\begin{enumerate}
	\item Вызывает \emph{генераторную функцию} \texttt{looking\_glass()}\footnote{При вызове генераторной функции возвращается объект-генератор} и запоминает объект-генератор (пусть называется \texttt{gen}),
	
	\item Вызывает \texttt{next(gen)}, чтобы заставить генератор выполнить код до предложения \texttt{yield},
	
	\item Возвращает значение, отданное \texttt{next(gen)}, чтобы его можно было связать с переменной в части \texttt{as} блока \texttt{with}, т.е. строка, отданная инструкцией \texttt{yield} связывается с переменной \texttt{what}.
\end{enumerate}

По завершении блока \texttt{with} метод \texttt{\_\_next\_\_} выполняет следующие действия:

\begin{enumerate}
	\item Смотрит, было ли передано исключение в параметре \texttt{exc\_type}; если да, вызывает \\ \texttt{gen.throw(exception)}, в результате чего строка в теле генераторной функции, содержащая \texttt{yield}, возбуждает исключение,
	
	\item В противном случае вызывает \texttt{next(gen)}, что приводит к выполнению части генераторной функции после \texttt{yield}.
\end{enumerate}

В рассмотренном примере есть очень серьезный дефект: если в теле блока \texttt{with} возникает исключение, то интерпретатор перехватывает его и повторно возбуждает в выражении \texttt{yield} внутри \texttt{looking\_glass}. Но здесь нет никакой обработки исключений, поэтому функция аварийно завершается, оставив систему в некорректном состоянии.

Более аккуратный вариант генераторной функции приведен ниже

\begin{lstlisting}[
title = {\sffamily Правильный вариант},
emph = {looking_glass, reverse_write},
style = ironpython,
numbers = none
]
# mirror_gen_exc.py
import contextlib

@contextlib.contextmanager
def looking_glass():  # здесь генераторная функция работает скорее как сопрограмма
    import sys
    original_write = sys.stdout.write  # переменная получает -> на объект функции write

    def reverse_write(text):  # замыкание
        original_write(text[::-1])

    sys.stdout.write = reverse_write  # переменная write получает -> на замыкание reverse_write
    msg = ''
    try:
        yield 'jabberwocky'.upper()  # отдает строку и переключается на блок with
    except ZeroDivisionError:
        msg = 'Пожалуйста не делите на ноль!'
    finally:  # выполняется в любом случае
        sys.stdout.write = original_write  # переменная write получает -> на объект функции write
        if msg:  # if msg != ''
            print(msg)
\end{lstlisting}

Пример выполнения

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> from mirror_gen_exc import looking_glass
>>> with looking_glass() as what:
        print('aaaabb')  # bbaaaa
        print(5/0)       # Пожалуйста не делите на ноль!
\end{lstlisting}

\remark{
Отметим, что использование слова \texttt{yield} в генераторе, который используется совместно с декоратором \texttt{@contextmanager}, не имеет ничего общего с итерированием. В рассмотренных примерах генераторная функция работает скорее, как \emph{сопрограмма}: процедура, которая доходит до определенной точки, затем приостанавливается и дает возможность поработать клиентскому коду до тех пор, пока он не захочет возобновить выполнение процедуры с прерванного места
}

\section{Перегрузка операторов в языке \texttt{Python}}

Перегрузка операторов позволяет экземплярам классов участвовать в обычных операциях \cite{prohorenok:python-2016}.

Основы перегрузки операторов:

\begin{itemize}
	\item запрещается перегружать операторы для встроенных типов,
	
	\item запрещается создавать новые операторы, можно перегружать существующие,
	
	\item несколько операторов нельзя перегружать вовсе: \texttt{is}, \texttt{and}, \texttt{or}, \texttt{not} (на побитовые операторы это не распространяется)
\end{itemize}

Фундаментальное правило: инфиксный оператор всегда возвращает \emph{новый объект}, т.е. создает новый экземпляр (составные операторы изменяемых объектов возвращают \texttt{self}, т.е. изменяют левый операнд на месте).

Иначе говоря, в случае инфиксных операторов нельзя модифицировать \texttt{self}, а нужно создавать и возвращать новый экземпляр подходящего типа \cite[\strbook{405}]{ramalho:python-2016}.

\remark{
\emph{Инфиксные} операторы (\texttt{*}, \texttt{+} и т.д.) независимо от типа данных всегда возвращают \emph{новый объект}. \emph{Составные} операторы (\texttt{+=}, \texttt{*=} и пр.) для объектов \emph{неизменяемого} типа данных (кортежи, строки и пр.) возвращают новый объект, но в случае объектов \emph{изменяемого} типа данных (списки) -- изменяют объект на месте
}

\begin{lstlisting}[
style = ironpython,
title = {\sffamily Сравнение работы инфиксных и составных операторов},
numbers = none
]
# изменяемый объект
>>> lst = [100]  
>>> id(lst)  # 179426376
>>> lst = lst*2  # инфиксный оператор возвращает новый объект, поэтому id будет другим
>>> id(lst)  # 117159368 -- изменился
>>> lst  # [100, 100]
>>> lst *= 2  # но составной оператор для изменяемого объекта изменяет левый операнд на месте
>>> lst  # [100, 100, 100, 100]
>>> id(lst)  # 117159368 -- не изменился
# неизменяемый объект
>>> tpl = (100,)
>>> id(tpl)  # 114189896
>>> tpl = tpl*2  # инфиксный оператор вернет новый объект
>>> tpl  # (100, 100)
>>> id(tpl)  # 82350344 -- изменился
>>> tpl *= 2  # составной оператор создаст новый объект и перепривяжет его к tpl
>>> tpl # (100, 100, 100, 100)
>>> id(tpl)  # 93229768 -- изменился
\end{lstlisting}

При умножении \emph{последовательности} (списки, кортежи, строки) на \emph{целое число} создается копия последовательности заданное число раз, а затем копии склеиваются. 

Как читать выражения с математическими операторами:
\begin{itemize}
	\item Смотрим к какому классу относится оператор: \emph{инфиксному} или \emph{составному},
	
	\item Если оператор инфиксный, то независимо от того являются операнды изменяемыми или нет будет возвращен новый объект\footnote{При условии, что оператор в случае данных операндов имеет смысл},
	
	\item Если оператор составной, то нужно выяснить является левый операнд изменяемым или нет,
	\begin{itemize}
		\item левый операнд изменяемый: составной оператор изменит левый операнд на месте (идентификатор не изменится),
		
		\item левый операнд неизменяемый: составной оператор создаст новый объект и перепривяжет его к переменной (изменится идентификатор).
	\end{itemize}
\end{itemize}

\subsection{Перегрузка оператора сложения}

Для поддержки операций с объектами \emph{разных типов} в \texttt{Python} имеется особый механизм диспетчеризации для специальных методов, ассоциированных с инфиксными операторами.

Видя выражение \texttt{a + b}, интерпретатор выполняет следующие шаги:

\begin{itemize}
	\item Если у \texttt{a} есть метод \texttt{\_\_add\_\_}, вызвать \texttt{a.\_\_add\_\_(b)} и вернуть результат, если только он не равен \texttt{NotImplemented}\footnote{\texttt{NotImplemented} -- это значение-синглтон, которое должен возвращать специальный метод инфиксного оператора, чтобы сообщить интерпретатору, что не умеет обрабатывать данный операнд} (т.е. оператор не знает как обрабатывать данный операнд),
	
	\item Если у левого операнда \texttt{a} нет метода \texttt{\_\_add\_\_} или его вызов вернул \texttt{NotImplemented}, проверить, есть ли у правого операнда \texttt{b} <<правый>> метод \texttt{\_\_radd\_\_}\footnote{Иногда такие методы называют <<инверсными>> методами, но лучше их представлять как \emph{правые} методы, так как они вызываются от имени правого операнда}, и, если да, вызвать \texttt{b.\_\_radd\_\_(a)} и вернуть результат, если только он не равен \texttt{NotImplemented},
	
	\item Если у \texttt{b} нет метода \texttt{\_\_radd\_\_} или его вызов вернул \texttt{NotImplemented}, возбудить исключение \texttt{TypeError}.
\end{itemize}

Рассмотрим реализацию методов сложения для объектов

\begin{lstlisting}[
style = ironpython,
emph = {VectorUser, __add__, __radd__, __iter__, __init__, __repr__},
numbers = none
]
import itertools
import reprlib

class VectorUser:
    def __init__(self, seq):
        self._seq = array('d', seq)

    def __iter__(self):
        return iter(self._seq)

    def __repr__(self):
        components = reprlib.repr(self._seq)
        components = components[components.find('['):-1]
        return f'Vector({components})'

    def __add__(self, other):
        try:
            pairs = itertools.zip_longest(self, other, fillvalue=0.0)
            return VectorUser(a + b for a, b in pairs)  # возвращает новый экземпляр класса
        except TypeError:
            return NotImplemented

    def __radd__(self, other):
        return self + other
\end{lstlisting}

Как работает этот код. Рассмотрим случай, когда экземпляр класса \texttt{Vector} находится слева от оператора \texttt{+}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> v1 = VectorUser([3, 4, 5])
>>> v1 + (10, 20, 30)  # Vector([13.0, 24.0, 35.0])
# v1.__add__((10, 20, 30))
# удобно представлять VectorUser.__add__(v1, (10, 20, 30))
\end{lstlisting}

Первым делом интерпретатор пытается выяснить есть ли у левого операнда метод \texttt{\_\_add\_\_}. В данном случае у объекта \texttt{v1} есть такой метод, поэтому ничто не мешает вызвать его напрямую. Аргумент \texttt{self} метода \texttt{\_\_add\_\_} получает ссылку на \texttt{v1} (экземпляр класса \texttt{Vector}), а \texttt{other} -- ссылку на кортеж. Далее с помощью \texttt{zip\_longest} конструируется генератор кортежей, который в следующей строке используется в генераторном выражении при создании нового экземпляра класса \texttt{Vector} (оператор должен возвращать новый объект).

Теперь рассмотрим случай, когда экземпляр класса \texttt{VectorUser} находится справа от оператора \texttt{+}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> (10, 20, 30) + v1
\end{lstlisting}

И снова интерпретатор пытается выяснить есть ли у левого операнда метод \texttt{\_\_add\_\_}. У кортежа есть такой метод, но он не умеет работать с объектом \texttt{VectorUser} (возвращает \texttt{NotImplemented}).

Теперь интерпретатор проверяет есть ли у правого операнда <<правый>> метод \texttt{\_\_radd\_\_}. Правый операнд это экземпляр класса \texttt{VectorUser}, поэтому \texttt{v1.\_\_radd\_\_((10, 20, 30))} это то же самое что и \texttt{VectorUser.\_\_radd\_\_(v1, (10, 20, 30))}.

Другими словами, аргумент \texttt{self} метода \texttt{\_\_radd\_\_} получает ссылку на объект \texttt{v1}, а аргумент \texttt{other} -- ссылку на кортеж. И тогда в выражении \texttt{self + other}, которое возвращается методом \texttt{\_\_radd\_\_}, экземпляр класса \texttt{VectorUser} окажется слева от оператора \texttt{+}. Интерпретатор, встретив выражение \texttt{self + other}, начинает с поиска метода \texttt{\_\_add\_\_} у левого операнда и, найдя его, возвращает новый экземпляр класса \texttt{VectorUser(...)}.

\remark{
Еще раз: чтобы поддержать операции с \emph{разными типами}, мы возвращаем специальное значение \texttt{NotImplemented} -- не исключение, -- давая интерпретатору возможность попробовать еще раз: поменять операнды местами и вызывать специальный инверсный (правый) метод, соответствующий тому же оператору (например, \texttt{\_\_radd\_\_})
}

\subsection{Перегрузка оператора умножения на скаляр}

Рассмотрим в качестве примера умножение вектора \texttt{VectorUser} на скаляр

\begin{lstlisting}[
style = ironpython,
emph = {__mul__, __rmul__},
numbers = none
]
import numbers

# внутри класса VectorUser
def __mul__(self, scalar):
    if isinstance(scalar, numbers.Real):  # сравнение с абстрактным базовым классом
        return VectorUser(n*scalar for n in self)
    else:
        return NotImplemented

def __rmul__(self, scalar):
    return self*scalar
\end{lstlisting}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> v1 = VectorUser([3, 4, 5])
>>> v1*4  # Vector([12.0, 16.0, 20.0])
>>> 10*v1 # Vector([30.0, 40.0, 50.0])
\end{lstlisting}

В первом случае интерпретатор начинает с поиска метода \texttt{\_\_mul\_\_} у левого операнда. Метод найден, объект справа (число 4) действительно является экземпляром подкласса абстрактного базового класса \texttt{numbers.Real}. Значит теперь можно вернуть экземпляр \texttt{VectorUser}.

Во втором случае интерпретатор так же начинает с поиска метода \texttt{\_\_mul\_\_} у левого операнда и не находит его. Поэтому на следующем шаге ищется правый метод \texttt{\_\_rmul\_\_} у правого операнда. Теперь объект \texttt{v1} в выражении \texttt{self*scalar} стоит слева и потому в методе \texttt{\_\_rmul\_\_} аргумент \texttt{self} ссылается на \texttt{v1}, а \texttt{scalar} -- на \texttt{4}. Видя выражение \texttt{self*scalar} интерпретатор вызывает метод \texttt{\_\_mul\_\_}, который на этот раз выполняется без проблем.



\remark{
В общем случае, если прямой инфиксный метод (например, \texttt{\_\_mul\_\_}) предназначен для работы только с операндами того же типа, что и \texttt{self}, бесполезно реализовывать соответствующий инверсный метод (например, \texttt{\_\_rmul\_\_}), потому что он, по определению, вызывается, только когда второй операнд имеет другой тип \cite[\strbook{425}]{ramalho:python-2016}
}

\subsection{Операторы сравнения}

Обработка операторов сравнения (\texttt{==}, \texttt{!=}, \texttt{>}, \texttt{<=} и т.д.) интерпретатором \texttt{Python} похожа на обработку инфиксных операторов, но есть два важных отличия \cite[\strbook{417}]{ramalho:python-2016}:

\begin{itemize}
	\item для прямых и инверсных (правых) методов служит один и тот же набор методов; например, в случае оператора \texttt{==} как прямой, так и правый вызов обращаются к методу \texttt{\_\_eq\_\_}, но изменяется порядок аргументов.
	
	\item в случае \texttt{==} и \texttt{!=}, если инверсный (правый) вызов завершается ошибкой, то \texttt{Python} сравнивает идентификаторы объектов, а не возбуждает исключение (см.~\tblref{tab:operators_comp}).
\end{itemize}

\begin{table}[h]
	\centering
	\caption{\itshape Операторы сравнения. Инверсные (правые) методы вызываются, когда\\ прямой вызов вернул \texttt{NotImplemented}}\label{tab:operators_comp}
	%\renewcommand{\arraystretch}{1.05}
	\begin{tabular}{lllll}
		{Группа} & {Инфиксный} & Прямой вызов & Инверсный вызов & Запасной вариант \\
		{} & оператор & метода & метода & {} \\ \hline\hline
		Равенство & \texttt{a == b} & \texttt{a.\_\_eq\_\_(b)} & \texttt{b.\_\_eq\_\_(a)} & \texttt{return id(a) == id(b)} \\
		\rowcolor[gray]{0.96} {} & \texttt{a != b} & \texttt{a.\_\_ne\_\_(b)} & \texttt{b.\_\_ne\_\_(a)} & \texttt{return not (a == b)} \\
		\hline
		Порядок & \texttt{a > b} & \texttt{a.\_\_gt\_\_(b)} & \texttt{a.\_\_lt\_\_(b)} & \texttt{raise TypeError} \\
		\rowcolor[gray]{0.96} {} & \texttt{a < b} & \texttt{a.\_\_lt\_\_(b)} & \texttt{a.\_\_gt\_\_(b)} & \texttt{raise TypeError} \\
		{} & \texttt{a >= b} & \texttt{a.\_\_ge\_\_(b)} & \texttt{a.\_\_le\_\_(b)} & \texttt{raise TypeError} \\
		\rowcolor[gray]{0.96} {} & \texttt{a <= b} & \texttt{a.\_\_le\_\_(b)} & \texttt{a.\_\_ge\_\_(b)} & \texttt{raise TypeError}
	\end{tabular}
\end{table}

Однако поведение оператора \texttt{==} пользовательских классов зависит от реализации метода \texttt{\_\_eq\_\_}. Например, пусть есть класс \texttt{Vector}

\begin{lstlisting}[
style = ironpython,
emph = {__eq__},
numbers = none
]
# в классе Vector
def __eq__(self, other):
    if isinstance(other, Vector):
        return (len(self) == len(other) and all(a == b for a, b in zip(self, other)))
    else:
        return NotImpemented
\end{lstlisting}

и какой-то другой класс \texttt{Vector2d}

\begin{lstlisting}[
style = ironpython,
emph = {__eq__},
numbers = none
]
# в классе Vector2d
def __eq__(self, other):
    retrun tuple(self) == tuple(other)
\end{lstlisting}

Если теперь сравнить экземпляры этих классов

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> v1 = Vector([1, 2])
>>> v2 = Vector2d(1, 2)
>>> v1 == v2  # True
\end{lstlisting}
то порядок действий будет следующим:

\begin{itemize}
	\item для вычисления \texttt{v1 == v2} интерпретатор вызовет \texttt{Vector.\_\_eq\_\_(v1, v2)},
	
	\item метод \texttt{Vector.\_\_eq\_\_(v1, v2)} видет, что \texttt{v2} не является экземпляром класса \texttt{Vector} и возвращает \texttt{NotImplemented},
	
	\item получив значение \texttt{NotImplemented}, интерпретатор вызывает метод \texttt{\_\_eq\_\_} правого операнда, т.е. \texttt{v2}: \texttt{Vector2d.\_\_eq\_\_(v2, v1)},
	
	\item \texttt{Vector2d.\_\_eq\_\_(v2, v1)} преобразует оба операнда в кортежи и сравнивает их, результат оказывается равен \texttt{True}.
\end{itemize}

Теперь рассмотрим сравнение с кортежем

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> t = (1, 2)
>>> v1 == t  # False
\end{lstlisting}

В этом случае:

\begin{itemize}
	\item для вычисления \texttt{v1 == t} \texttt{Python} вызывает \texttt{Vector.\_\_eq\_\_(v1, t)},
	
	\item метод \texttt{Vector.\_\_eq\_\_(v1, t)} видит, что кортеж \texttt{t} не является экземпляром класса \texttt{Vector} и возвращает \texttt{NotImplemented},
	
	\item получив результат \texttt{NotImplemented}, интерпретатор вызывает метод \texttt{\_\_eq\_\_} правого объекта, т.е. \texttt{tuple.\_\_eq\_\_(t, v1)}
	
	\item но \texttt{tuple.\_\_eq\_\_(t, v1)} ничего не знает о классе \texttt{Vector}, и поэтому возвращает \texttt{NotImplemented},
	
	\item если правый вызов вернул \texttt{NotImplemented}, то \texttt{Python} в качестве последнего средства сравнивает идентификаторы объектов, что в данном случае возвращает \texttt{False}
\end{itemize}




 
\section{Области видимости в языке \texttt{Python}}

Когда мы говорим о поиске значения имени применительно к программному коду, под термином \emph{область видимости} подразумевается \emph{пространство имен} -- то есть место в программном коде, где имени было присвоено значение \cite{lutz:learningpython-2011}.

В любом случае область видимости переменной (где она может использоваться) всегда определяется местом, где ей было присвоено значение.

\remark{%
Термины <<\emph{область видимости}>> и <<\emph{пространство имен}>> можно использовать как синонимичные
}

При каждом вызове функции создается новое \emph{локальное пространство имен}. Это пространство имен представляет локальное окружение, содержащее имена параметров функции, а также имена переменных, которым были присвоены значения в теле функции.

По умолчанию операция присваивания создает локальные имена (это поведение можно изменить с помощью \texttt{global} или \texttt{local}).

Схема разрешения имен в языке \texttt{Python} иногда называется \emph{правилом LEGB}\footnote{Local, Enclosing, Global, Built-in} \cite[\strbook{477}]{lutz:learningpython-2011}:

\begin{itemize}
	\item Когда внутри функции выполняется обращение к неизвестному имени, интерпретатор пытается отыскать его в четырех областях видимости -- в \emph{локальной}, затем в \emph{локальной области любой объемлющей функции} или в выражении \texttt{lambda}, затем в \emph{глобальной} и, наконец, во \emph{встроенной}. Поиск завершается, как только будет найдено первое подходящее имя.
	
	\item Когда внутри функции выполняется операция присваивания \lstinline{a=10} (а не обращения к имени внутри выражения), интерпретатор всегда создает или изменяет имя в \emph{локальной области видимости}, если в этой функции оно не было объявлено глобальным или нелокальным.
\end{itemize}

Пример

\begin{lstlisting}[
style = ironpython,
numbers = none
]
# глобальная область видимости
X = 99  

def func(Y):  # Y и Z локальные переменные
    # локальная область видимости
    Z = X + Y # X - глобальная переменная
    return Z
    
func(1)  # Y = 1
\end{lstlisting}

Переменные \texttt{Y} и \texttt{Z} являются \emph{локальными} (и существуют только во время выполнения функции), потому что присваивание значений обоим именам осуществляется внутри определения функции: присваивание переменной \texttt{Z} производится с помощью инструкции \texttt{=}, а \texttt{Y} -- потому что аргументы всегда передаются через операцию присваивания.

Когда внутри функции выполняется операция присваивания значения переменной, она всегда выполняется в \emph{локальном пространстве имен функции}

\begin{lstlisting}[
style = ironpython,
emph = {f},
numbers = none
]
a = 10  # глобальная область видимости

def f():
    a = 100  # локальная область видимости
    return a
\end{lstlisting}

В результате переменная \texttt{a} в теле функции ссылается на совершенно другой объект, содержащий значение 100, а не тот, на который ссылается внешняя переменная.

Переменные во вложенных функциях привязаны к \emph{лексической области видимости}. То есть поиск имени переменной начинается в \emph{локальной области видимости} и затем последовательно продолжается во всех \emph{объемлющих областях видимости внешних функций}, в направлении от внутренних к внешним.

Если и в этих \emph{пространствах имен} искомое имя не будет найдено, поиск будет продолжен в \emph{глобальном пространстве имен}, а затем во \emph{встроенном пространстве имен}, как и прежде.

При обращении к локальной переменной до того, как ей будет присвоено значение, возбуждается исключение \texttt{UnboundLocalError}. Следующий пример демонстрирует один из возможных сценариев, когда такое исключение может возникнуть

\begin{lstlisting}[
style = ironpython,
emph = {foo},
numbers = none
]
i = 0
def foo():
    i = i + 1  # приведет к исключению UnboundLocalError
    print(i)
\end{lstlisting}

В этой функции переменная \texttt{i} определяется как \emph{локальная} (потому что внутри функции ей присваивается некоторое значение и отсутствует инструкция \texttt{global}).

При этом инструкция присваивания \lstinline{i = i + 1} пытается прочитать значение переменной \texttt{i} еще до того, как ей будет присвоено значение.

Хотя в этом примере существует глобальная переменная \texttt{i}, она не используется для получения значения. Переменные в функциях могут быть либо \emph{локальными}, либо \emph{глобальными} и не могут произвольно изменять \emph{область видимости} в середине функции.

\remark{
Оператор \texttt{global} делает локальную переменную в теле функции \emph{глобальной} и говорит интерпретатору чтобы тот не искал переменную в локальной области видимости текущей функции
}

Например, нельзя считать, что переменная \texttt{i} в выражении \lstinline{i + 1} в предыдущем фрагменте обращается к глобальной переменной \texttt{i}; при этом переменная \texttt{i} в вызове \texttt{print(i)} подразумевает локальную переменную \texttt{i}, созданную в предыдущей инструкции.

\quinta{%
	Когда интерпретатор, построчно сканируя тело функции \texttt{def}, натыкается на строку \texttt{i = i + 1}, он заключает что переменная \texttt{i} является \emph{локальной}, так как ей присваивается значение именно в теле функции. А когда функция вызывается на выполнение и интерпретатор снова доходит до строки \texttt{i = i + 1}, выясняется, что переменная \texttt{i}, стоящая в правой части, не имет ссылок на какой-либо объект и потому возникает ошибка \texttt{UnboundLocalError}
}

\section{Декораторы в \texttt{Python}}

\emph{Декораторы} выполняются \emph{сразу после} загрузки или импорта модуля, однако увидеть какие-либо изменения можно только в том случае, если декоратор явно взаимодействует с пользователем на <<верхнем уровне>>\footnote{Если декоратор простой одноуровневый, то под верхним уровнем понимается его локальная область видимости, а если декоратор содержит замыкание, то -- понимается область видимости объемлющей функции}, например, печатает строку в терминале.  \emph{Задекорированные} же { функции} выполняются строго в результате явного вызова \cite[\strbook{217}]{ramalho:python-2016}.

\subsection{Реализация простого декоратора}

Рассмотрим простой декоратор, который хронометритует каждый вызов задекорированной функции и печатает затраченное время

\begin{lstlisting}[
style = ironpython,
title = {\sffamily clockdeco.py, не очень удачный пример декоратора},
emph = {clock, clocked},
numbers = none
]
import time

def clock(func):
    print('test string from `clock`')  # <- строка будет выведена в терминал
                                       # сразу после загрузки модуля, который
                                       # импортирует данный декоратор
    def clocked(*args):  # замыкание
        t0 = time.perf_counter()  # запомнить начальный момент времени
        result = func(*args)  # вызвать функцию
        elapsed = time.perf_counter() - t0  # вычислить сколько прошло времени
        name = func.__name__
        arg_str = ', '.join(repr(arg) for arg in args)
        print(f'{elapsed}, {name}({arg_str}) -> {result}')
        return result  # вернуть результат
return clocked
\end{lstlisting}

Использование декоратора выглядит так

\begin{lstlisting}[
label = {lst:deco},
style = ironpython,
title = {\sffamily clockdeco\_demo.py},
emph = {simple_deco_1, simple_deco_2,
simple_func_1, simple_func_2, inner, snooze, factorial}
]
import time
from clockdeco import clock

def simple_deco_1(f): 
    '''
    Декоратор с замыканием
    '''
    def inner():
        print('test string from `simple_deco_1`')  # <- строка НЕ будет выведена
                                                   # после загрузке модуля
    return inner

def simple_deco_2(f):
    '''
    Простой одноуровневый декоратор
    '''
    print('test string from `simple_deco_2`')  # <- строка будет выведена в терминал
                                               # сразу после загрузки модуля
    return f

@simple_deco_1  # simple_func_1 = simple_deco_1(f=simple_func_1) -> inner
def simple_func_1():
    print('test string from `simple_func_1`')   

@simple_deco_2  # simple_func_2 = simple_deco_2(f=simple_func_2) -> simple_func_2
def simple_func_2():
    print('test string from `simple_func_2`')   

@clock  # snooze = clock(func=snooze) -> clocked
def snooze(seconds):
    time.sleep(seconds)

@clock
def factorial(n):
    return 1 if n < 2 else n*factorial(n-1)


if __name__ == '__main__':
    print('*'*10, 'Calling snooze(.123)')
    print('snooze_result = {}'.format(snooze(.123)))
    print('*'*10, 'Calling factorial(6)')
    print('6! = ', factorial(6))
    print(f'This is result from `simple_func_1`: {simple_func_1()}')
    print(f'This is result from `simple_func_2`: {simple_func_2()}')
\end{lstlisting}

\begin{lstlisting}[
title = {\sffamily Вывод clockdeco\_demo.py},
numbers = none
]
test string from `simple_deco_2`
test string from `clock`
test string from `clock`
********** Calling snooze(.123)
0.1261, snooze(0.123) -> None
snooze_result = None
********** Calling factorial(6)
1.866e-06, factorial(1) -> 1
7.589e-05, factorial(2) -> 2
0.0001266, factorial(3) -> 6
0.0001732, factorial(4) -> 24
0.0002224, factorial(5) -> 120
0.0002715, factorial(6) -> 720
6! =  720
test string from `simple_deco_1`
this is result from `simple_func_1`: None
test string from `simple_func_2`
this is result from `simple_func_2`: None
\end{lstlisting}

\remark{
Приведенный выше пример декоратора \texttt{clock} из модуля \texttt{clockdeco.py} не удачен в том смысле, что если нам, например, потребуется вывести значение атрибута \texttt{\_\_name\_\_} задекорированной функции \texttt{snooze}, т.е. \texttt{snooze.\_\_name\_\_}, то будет возвращена строка \texttt{'clocked'}, а не \texttt{'snooze'}.

Чтобы декоратор <<не портил>> значения атрибута \texttt{\_\_name\_\_}, следует задекорировать замыкание декоратора с помощью \texttt{@functools.wraps(func)}
}


При разгрузке модуля \texttt{clockdeco\_demo.py} будут выполнены все декораторы, но только декораторы \texttt{simple\_deco\_2} и \texttt{clock} выведут в терминал строки, потому как эти строки расположены на верхнем уровне декораторов (т.е. находятся не внутри вложенных функций). Декоратор \texttt{simple\_deco\_1} ничего не выводит, так как строка находится в области видимости вложенной функции.

Важно отметить следующее: после загрузки модуля, как уже говорилось выше, будут выведены в терминал строки, расположенные на верхнем уровне декораторов, но самое главное заключается в том, что после выполнения декоратора \texttt{clock} объект \texttt{snooze} уже будет ссылаться на внутреннюю функцию \texttt{clocked} декоратора \texttt{clock}, а после выполнения декоратора \texttt{simple\_deco\_1} объект \texttt{simple\_func\_1} будет ссылаться на внутреннюю функцию \texttt{inner}. Что же касается декоратора \texttt{simple\_deco\_2}, то объект \texttt{simple\_func\_2} будет ссылаться на \texttt{simple\_func\_2}.

По этой причине при вызове функции \texttt{simple\_func\_1()} печатается строка из внутренней функции \texttt{inner}, а при вызове функции \texttt{simple\_func\_2()} -- строка из этой же функции.

Еще один пример декоратора с замыканием

\begin{lstlisting}[
style = ironpython,
emph = {deco, inner, target},
numbers = none
]
def deco(f):
    def inner(*args, **kwargs):
        print(f'from `deco-inner`: args={args}, kwargs={kwargs}')
        return f  # f - свободная переменная
    return inner

@deco  # target = deco(f=target) -> inner :: target -> inner :: target=inner
def target(a, b=10):
    return (f'from `target`: a={a}, b={b}')

print(target(20, b=500)(250))  # сначала вызывается inner(20, b=500), а потом target(250)
\end{lstlisting}

Выведет

\begin{lstlisting}[
numbers = none
]
from `deco-inner`: args=(20,), kwargs={'b': 500}
from `target`: a=250, b=10
\end{lstlisting}

\subsection{Кэширование с помощью \texttt{functools.lru\_cache}}

Декоратор \texttt{functools.lru\_cache} очень полезен на практике. Он реализует запоминание: прием оптимизации, смысл которого заключается в сохранении результатов предыдущих дорогостоящих вызовов функции, что позволяет избежать повторного вычисления с теми же аргументами, что и раньше \cite[\strbook{230}]{ramalho:python-2016}.

Например

\begin{lstlisting}[
style = ironpython,
emph = {fibonacci},
numbers = none
]
import functools
from clockdeco import clock

@functools.lru_cache
@clock
def fibonacci(n):
    if n < 2:
        return n
    return fibonacci(n-2) + fibonacci(n-1)
    
if __name__ == '__main__':
    print(fibonacci(6))
\end{lstlisting}

\remark{
\texttt{lru\_cache} хранит результаты в словаре, ключи которого составлены из позиционных и именованных аргументов вызовов, а это значит, что все аргументы, принимаемые декорируемой функции должны быть \emph{хешируемыми}
}

\subsection{Одиночная диспетчеризация и обобщенные функции}

Декоратор \texttt{functools.singledispatch} позволяет каждому модулю вносить свой вклад в общее решение. Обычная функция, декорированная \texttt{@singledispatch} становится \emph{обобщенной функцией}: групповой функцией, выполняющей одну и ту же логическую операцию по-разному в зависимости от типа первого аргумента \cite[\strbook{234}]{ramalho:python-2016}. Именно это и называется \emph{одиночной диспетчеризацией}. Если бы для выбора конкретных функций использовалось больше аргументов, то мы имели бы дело с \emph{множественной диспетчеризацией}.

Например

\begin{lstlisting}[
style = ironpython,
emph = {htmlize},
numbers = none
]
from functools import singledispatch
from collections import abc
import numbers
import html

@singledispatch  # делает функцию обобщенной
def htmlize(obj):
    content = html.escape(repr(obj))
    return '<pre>{}</pre>'.format(content)
    
    
@htmlize.register(str)  # будет вызываться для объектов строкового типа данных
def _(text):
    content = html.escape(text).replace('\n', '<br>\n')
    return '<p>{}</p>'.format(content)
    
    
@htmlize.register(numbers.Integral)  # будет вызваться для объектов целочисленного типа данных
def _(n):
    return '<pre>{} (0x{:x})</pre>'.format(n)
    
    
@htmlize.register(tuple)
@htmlize.register(abc.MutableSequence)
def _(seq):
    inner = '</li>\n<li>'.join(htmlize(item) for item in seq)
    return '<ul>\n<li>' + inner + '</li>\n</ul>'
\end{lstlisting}

\remark{
По возможности следует стараться регистрировать специализированные функции для обработки абстрактных базовых классов, например, \texttt{numbers.Integral} или \texttt{abc.MutableSequence}, а не конкретные реализации типа \texttt{int} или \texttt{list}
}

Замечательное свойство механизма \texttt{singledispatch} состоит в том, что специализированные функции можно зарегистрировать в любом месте системы, в любом модуле \cite{ramalho:python-2016}.

\subsection{Композиции декораторов}

Когда два декоратора \texttt{@d1} и \texttt{@d2} применяются к одной и той же функции \texttt{f} в указанном порядке, получается то же самое, что в результате композиции \texttt{f = d1(d2(f))}.

Иными словами

\begin{lstlisting}[
style = ironpython,
emph = {f},
numbers = none
]
@d1
@d2
def f():
    print('f')
\end{lstlisting}
эквивалентен следующему
\begin{lstlisting}[
style = ironpython,
emph = {f},
numbers = none
]
def f():
    print('f')
    
f = d1(d2(f))
\end{lstlisting}

Рассмотрим еще один пример композиции декораторов

\begin{lstlisting}[
style = ironpython,
emph = {deco1, deco2, target, inner1, inner2},
numbers = none
]
def deco1(f):  # выполняется вторым
    print('deco-1')  # # будет выведена в терминал
    def inner1():
        print('string from `deco1-inner`')
    return inner1

def deco2(f):  # выполняется первым
    print('deco-2')  # будет выведена в терминал
    def inner2():
        print('string from `deco2-inner')
    return inner2

@deco1  # 2) inner2 = deco1(f=inner2) -> inner1 :: inner2 -> inner1 :: inner2 = inner1
@deco2  # 1) target = deco2(f=target) -> inner2 :: target -> inner2 :: target = inner2
def target():  # 3) target -> inner1
    print('string from `target`')


if __name__ == '__main__':
    target()  # выведет string from `deco1-inner`
\end{lstlisting}

Выведет

\begin{lstlisting}[
numbers = none
]
deco-2
deco-1
string from `deco1-inner`
\end{lstlisting}

\remark{
Первым выполняется тот декоратор, который ближе расположен к декорируемой функции
}


То есть при загрузке или импорте модуля будут выполнены декораторы \texttt{deco1} и \texttt{deco2}: сначала \texttt{deco2}, а затем \texttt{deco1}, потому как \texttt{deco2} ближе к декорируемой функции. Декоратор \texttt{deco1} применяется к той функции, которую возвращает \texttt{deco2}.

\subsection{Параметризованные декораторы}

Параметризованные декораторы часто называют \emph{фабриками декораторов}. Фабрики декораторов возвращают настоящие декораторы, которые применяются к декорируемой функции.

Пример

\begin{lstlisting}[
style = ironpython,
emph = {register, decorate, f1, f2, f3},
numbers = none
]
registry = set()

def register(activate=True):  # фабрика декораторов
    def decorate(func):  # декоратор
        print(f'running register(activate={activate})->decorate({func})')
        if activate:
            registry.add(func)
        else:
            registry.discard(func)
        return func
    return decorate

@register(activate=False)  # f1 = decorate(func=f1) -> f1 :: f1 -> f1
def f1():
    print('running f1()')

@register()  # f2 = decorate(func=f2) -> f2 :: f2 -> f2
def f2():
    print('running f2()')

def f3():
print('running f3()')
\end{lstlisting}

Идея в том, что функция \texttt{register()} возвращает декоратор \texttt{decorate}, который затем применяется к декорируемой функции \cite{ramalho:python-2016}. 

\remark{
\emph{Фабрика декораторов} возвращает \emph{декоратор}, который применяется к декорируемой функции
}

Чуть подробнее: сразу после загрузки или импорта модуля выполняется фабрика декораторов \texttt{register}, которая возвращает декоратор \texttt{decorate}, который и применяется к функциям. Можно представлять, что фабрика декораторов нужна только для того, чтобы собрать значения каких-то дополнительных переменных, которые потребуются позже. В данном примере можно представить, что строка \texttt{@register()} заменяется на строку \texttt{@decorate}. То есть декоратор применяется к функции, расположенной на следующей строке, и работает как обычно.

Как можно работать с этой фабрикой декораторов

\begin{lstlisting}[
style = ironpython,
numbers = none
]
register()(f3)  # добавить ссылку на функцию f3 во множенство registry
register(activate=False)(f2)  # удалить ссылку на функцию f2
\end{lstlisting}

Конструкция \texttt{register()} возвращает декоратор, который затем применяется к переменной (например, к \texttt{f3}), ассоциированной с декорируемой функцией, и работает так, как если бы изначально был только он (без фабрики декораторов) \cite{ramalho:python-2016}.

Если бы у декоратора был еще один уровень вложенности, т.е. было бы определено еще и замыкание, то это изменило бы только ссылку на функцию, которую возвращает замыкание

\begin{lstlisting}[
style = ironpython,
emph = {fabricdeco, deco, inner, target},
numbers = none
]
def fabricdeco():  # фабрика декораторов
    def deco(f):  # декоратор
        def inner():  # замыкание
            print(f'from inner: {f}')
        return inner
    return deco
    
@fabicdeco()  # target = deco(f=target) -> inner :: target -> inner :: target=inner
def target():
    print('from target')
    
target()  # на самом деле вызывается inner() -> from inner: <function target at 0x0...08B05318>
\end{lstlisting}

Рассмотрим еще один пример параметризованного декоратора

\begin{lstlisting}[
style = ironpython,
emph = {clock, clocked, decorate, snooze},
numbers = none
]
import time

DEFAULT_FMT = '[{elapsed}s] {name}({args}) -> {result}'

def clock(fmt=DEFAULT_FMT):  # фабрика декораторов
    def decorate(func):  # декоратор
        count = 0
        def clocked(*_args):  # замыкание
            nonlocal count  # делает переменную свободной
            count += 1
            print(f'args-{count}: {_args}')
            t0 = time.time()
            _result = func(*_args)
            elapsed = time.time() - t0
            name = func.__name__
            args = ', '.join(repr(arg) for arg in _args)
            result = repr(_result)
            print(fmt.format(**locals())) # использование **locals() позволяет ссылаться
                                          # на любую локальную переменную clocked
            return _result
        return clocked
    return decorate


if __name__ == '__main__':
    @clock()  # snooze = decorate(func=snooze) -> clocked :: snooze -> clocked
    def snooze(seconds):
        time.sleep(seconds)

    for i in range(3):
        snooze(0.123)
\end{lstlisting}

Теперь фабрику декораторов можно вызывать, например, так:
\begin{lstlisting}[
style = ironpython,
emph = {snooze},
numbers = none
]
@clock('log::{name}({args}), dt={elapsed:.5g}s')
def snooze(seconds):
    time.sleep(seconds)
\end{lstlisting}

Объяснение: сразу после загрузки модуля (когда модуль загружается как скрипт), интерпретатор наталкивается на строку \texttt{@clock()} после чего вызывает \emph{фабрику декораторов} \texttt{clock}, которая возвращает ссылку на \emph{декоратор} \texttt{decorate}, который в свою очередь начинает работать как и в описанных выше случаях, т.е. аргумент \texttt{func} декоратора получает ссылку на \texttt{snooze}, а сам декоратор возвращает ссылку на \emph{замыкание} \texttt{clocked}.

\remark{
Интерпретатор вызывает \emph{декоратор} или \emph{фабрику декораторов} из той строки, в которой находится конструкция \texttt{@deco}, поэтому если, как в данном примере, \texttt{@clock()} разместить в блоке проверки значения атрибута \texttt{\_\_name\_\_}, а сам модуль \emph{импортировать} (а не выполнять как сценарий), то фабрика декораторов не будет вызвана, потому что не будет выполнено условие \texttt{if \_\_name\_\_ == '\_\_main\_\_'} и фрагмент модуля со строкой \texttt{@clock()} останется скрытым от интерпретатора 
}

Однако здесь есть любопытный момент. Переменные \texttt{fmt}, \texttt{func} и \texttt{count} вообще говоря являются \emph{свободными переменными}, поэтому их значения можно читать из-под замыкания (находясь в области видимости замыкания) даже после того, как \emph{локальная область видимости объемлющей функции (декоратора) будет уничтожена}.

Но, присваивая значение переменной \texttt{count} на уровне замыкания \texttt{clocked}, мы делаем эту переменную локальной и привязываем к области видимости функции \texttt{clocked}. Таким образом, интерпретатор <<думает>>, что переменная \texttt{count} локальная для функции \texttt{clocked} и следовательно значение этой переменной должно быть в пределах функции \texttt{clocked}. При вызове функции \texttt{clocked} вычисления \texttt{count = count + 1} начинаются с правой части и когда интерпретатор не находит значения переменой \texttt{count} в области видимости функции \texttt{clocked} возникает ошибка \texttt{UnboundLocalError}.

\remark{%
Если переменная локальная, то интерпретатор в поисках значения этой переменной не может покинуть соответствующую локальную области видимости 
}

Еще раз. \emph{Свободные переменные} по умолчанию можно \underline{только читать} из-под замыкания. Когда мы присваиваем новое значение переменной \texttt{count} в теле замыкания, то мы делаем эту переменную \emph{локальной} для замыкания \texttt{clocked}, т.е. переменная \texttt{count} перестает быть свободной.

Чтобы объяснить интерпретатору, что переменная \texttt{count} должна рассматриваться как \emph{свободная} даже если ей присваивается значение в области видимости замыкания (что делает переменную локальной), следует использовать оператор \texttt{nonlocal}.

\remark{%
Можно сказать, что оператор \texttt{nonlocal} разрешает интерпретатору искать значение указанных переменных в области видимости \emph{объемлющей функции}, а оператор \texttt{global} -- в глобальной области видимости, т.е. на уровне модуля
}

Пример\vspace*{2mm}

\begin{minipage}{0.45\textwidth}
\begin{lstlisting}[
style = ironpython,
emph = {f, inner},
numbers = none
]
a = 10

def f():
    '''
    Разрешает искать в
    области видимости объемлющей функции
    '''
    a = 100
    def inner():
        nonlocal a  # <-- NB
        a += 1
        print(a)
    return inner
    
f()()  # 101
\end{lstlisting}
\end{minipage}\hspace*{5mm}
\begin{minipage}{0.45\textwidth}
\begin{lstlisting}[
style = ironpython,
emph = {f, inner},
numbers = none
]
a = 10
	
def f():
    '''
    Разрешает искать
    в глобальной области видимости
    '''
    a = 100
    def inner():
        global a  # <-- NB
        a += 1
        print(a)
    return inner
	
f()()  # 11
\end{lstlisting}
\end{minipage}


\subsection{Обобщение по механизму работы декораторов}

Если обобщить сказанное выше, то получается, что задекорированная функция ссылается на ту функцию, которую возвращает декоратор, аргумент которого получил ссылку на данную функцию. И происходит это \emph{сразу после} загрузки или импорта модуля. А затем остается только вызвать задекорированную функцию, которая вообще говоря уже ссылается на какую-то другую функцию, которую возвращает декоратор, т.е. если
\begin{lstlisting}[
style = ironpython,
emph = {deco, inner, target},
numbers = none
]
def deco(f):
    def inner():  # замыкание
        print('inner')
    return inner
    
@deco  # выполняется при загрузке/импорте модуля
def target():
    print('target')
\end{lstlisting}
то
\lstinline[style = ironpython]{target = deco(f=target) -> inner}\\и, следовательно, \lstinline{target -> inner} (можно считать, что \lstinline{target=inner});\\поэтому при вызове \lstinline{target()} на самом деле вызывается \lstinline{inner()} и будет выведена строка \lstinline[style=ironpython]{'inner'} (см.~\pic{fig:target_deco_formula}).

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.85]{figures/target_deco_formula.png}
	\caption{ К вопросу о механизме работы декоратора с вложенной функцией }\label{fig:target_deco_formula}
\end{figure}


\section{Замыкания/фабричные функции в \texttt{Python}}

Под термином \emph{замыкание} или \emph{фабричная функция} подразумевается объект функции, который сохраняет значения в \emph{объемлющих областях видимости}, даже когда эти области могут прекратить свое существование \cite[\strbook{488}]{lutz:learningpython-2011}.

В источнике \cite[\strbook{222}]{ramalho:python-2016} приводится несколько отличное определение\footnote{Определение содержит авторские правки}: \emph{замыкание} -- это вложенная функция с расширенной областью видимости, которая охватывает все \emph{неглобальные} переменные, объявленные в области видимости объемлющей функции, и способная работать с этими переменными даже после того как локальная область видимости объемлющей функции будет уничтожена.

Замыкания и вложенные функции особенно удобны, когда требуется реализовать концепцию отложенных вычислений \cite{beazley:python-2010}.

\remark{
Все же правильнее <<фабрикой функций>> называть всю конструкцию из объемлющей и вложенной функций, а <<замыканием>> -- только вложенную функцию
}

Рассмотрим в качестве примера следующую функцию

\begin{lstlisting}[
style = ironpython,
emph = {maker, action},
numbers = none
]
def maker(N):
    def action(X):
        return X**N  # функция action запоминает значение N в объемлющей области видимости
    return action
\end{lstlisting}

Здесь определяется внешняя функция, которая просто создает и возвращает вложенную функцию, не вызывая ее. Если вызвать внешнюю функцию

\begin{lstlisting}[
style = ironpython,
emph = {maker},
numbers = none
]
>>> f = maker(2)  # запишет 2 в N
>>> f  # <function action at 0x0147280>
\end{lstlisting}
она вернет ссылку на созданную ею вложенную функцию, созданную при выполнении вложенной инструкции \texttt{def}. Если теперь вызвать то, что было получено от внешней функции

\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> f(3)  # запишет 3 в X, в N по-прежнему хранится число 2
>>> f(4)  # 4**2
\end{lstlisting}
будет вызвана вложенная функция, с именем \texttt{action} внутри функции \texttt{maker}. Самое необычное здесь то, что вложенная функция продолжает хранить число 2, значение переменной \texttt{N} в функции \texttt{maker} даже при том, что к моменту вызова функции \texttt{action} функция \texttt{maker} уже \emph{завершила свою работу и вернула управление}.

Когда функция используется как вложенная, в замыкание включается все ее окружение, необходимое для работы внутренней функции \cite[\strbook{137}]{beazley:python-2010}.

\subsection{Области видимости и значения по умолчанию применительно к переменным цикла}

Существует одна известная особенность для функций или lambda-выражений: если lambda-выражение или инструкция \texttt{def} вложены в цикл внутри другой функции и вложенная функция ссылается на переменную из объемлющей области видимости, которая изменяется в цикле, все функции, созданные в этом цикле, будут иметь одно и то же значение -- значение, которое имела переменная на последней итерации \cite[\strbook{492}]{lutz:learningpython-2011}.

Например, ниже предпринята попытка создать список функций, каждая из которых запоминает текущее значение переменной \texttt{i} из объемлющей области видимости

\begin{lstlisting}[
title = {\sffamily Эта реализация работать НЕ будет},
style = ironpython,
emph = {makeActions},
numbers = none
]
def makeActions():
    acts = []
    for i in range(5):  # область видимости объемлющей функции
        acts.append(
            lambda x: i**x  # локальная область видимости вложенной анонимной функции
        ) 
    return acts
    
acts = makeActions()
print(acts[0](2))  # вернет 4**2, последнее значение i
print(acts[3](2))  # вернет 4**2, последнее значение i
\end{lstlisting}

Такой подход не дает желаемого результата, потому что поиск переменной в объемлющей области видимости производится позднее, \emph{при вызове вложенных функций}, в результате все они получат одно и то же значение (значение, которое имела переменная цикла на последней итерации).

Это один из случаев, когда необходимо явно сохранять значение из объемлющей области видимости в виде аргумента со значением по умолчанию вместо использования ссылки на переменную из объемлющей области видимости.

То есть, чтобы фрагмент заработал, необходимо передать текущее значение переменной из объемлющей области видимости в виде значения по умолчанию. Значения по умолчанию вычисляются в момент \emph{создания вложенной функции} (а не когда она \emph{вызывается}), поэтому каждая из них сохранит свое собственное значение \texttt{i}

\begin{lstlisting}[
title = {\sffamily Правильная реализация},
style = ironpython,
emph = {makeActions},
numbers = none
]
def makeActions():
    acts = []
    for i in range(5):
        acts.append(
            lambda x, i=i: i**x  # сохранить текущее значение i
        )
    return acts

acts = makeActions()
print(acts[0](2))  # вернет 0**2
print(acts[2](2))  # вернет 2**2
\end{lstlisting}

\quinta{%
Значения аргументов по умолчанию вложенных функций, динамически создаваемых в цикле на уровне области видимости объемлющей функции, вычисляются в момент \emph{создания} этих вложенных функций, а не в момент их вызова, поэтому \texttt{lambda x, i=i: ...} работает корректно
}

\section{Значения по умолчанию изменяемого типа данных в \texttt{Python}}

Если у функции есть аргумент, который получает ссылку на \emph{объект изменяемого типа данных} как на значение по умолчанию, то \emph{все вызовы функций} будут ссылаться на один и тот же изменяемый объект\footnote{По этой причине, как правило, только \emph{объекты неизменяемого типа данных} могут быть значениями по умолчанию. Если значение аргумента функции должно иметь возможность изменяться динамически, то этот аргумент функции инициализируют с помощью \texttt{None}, а затем передают ссылку на объект по условию} (идентификационный номер объекта не изменится).

Это удивляет. И когда говорят об аномальном поведении функции, аргумент которой ссылается на объект изменяемого типа данных, то обычно такое поведение объясняют следующим образом: значения аргументов по умолчанию вычисляются только один раз при загрузке модуля \cite[\strbook{77}]{slatkin:python-2016}. Однако такое объяснение не вскрывает механизм <<разделения>> ссылки между вызовами.

Лучше сказать так: если у функции есть аргумент, который ссылается на объект изменяемого типа данных, и в теле функции выполняется какая-то работа с этим изменяемым объектом (т.е. вносятся изменения в объект), то новые вызовы такой функции не сбрасывают значения по умолчанию до тех, которые были вычислены при загрузке модуля. Другими словами, если аргумент функции ссылается на объект изменяемого типа данных и над этим объектом выполняется какая-то работа в теле функции, то каждый новый вызов функции будет изменять этот изменяемый объект в \emph{определении} функции и потому каждый следующий вызов будет оперировать с уже измененным объектом изменяемого типа данных.

\remark{
Значения аргументов по умолчанию для избежания странного поведения функции должны ссылаться на \emph{объекты неизменяемого типа данных}
}

\section{Генераторы, сопрограммы в \texttt{Python}}

Цепочки вычислений лучше строить на базе генераторов или сопрограмм. Например простую цепочку преобразований можно построить и с помощью функции \texttt{reduce} (но лучше так не делать!)
\begin{lstlisting}[
style = ironpython,
title = {\sffamily Так лучше не делать!},
numbers = none	
]
from functools import reduce

test_str = 'python fortran matlab'
pipe_func = (
    str.split,  # разбивает переданную строку по пробелу и возвращает список
    lambda lst: (elem.upper() for elem in lst)  # каждую выделенную на предыдущем этапе
                                                # строку приводит к верхнему регистру
)

main_red = reduce(
    lambda args, f: f(args),
    pipe_func,
    test_str
)

for elem in main_red:
    print(elem)
# PYTHON
# FORTRAN
# MATLAB
\end{lstlisting}

Для данной задачи конвейер преобразований на базе генераторов может выглядеть так
\begin{lstlisting}[
style = ironpython,
title = {\sffamily Правильный вариант решения задачи о конвейерах преобразований},
numbers = none	
]
def gen_split(s: str):
    '''
    Функция-генератор
    '''
    for elem in s.split():
        yield elem
        
def gen_upper(sub_str: str):
    '''
    Функция-генератор
    '''
    for elem in sub_str:
        yield elem.upper()
        
main_gen = gen_upper(gen_split(test_str))  # объект-генератор
for elem in main_gen:
    print(elem)
# PYTHON
# FORTRAN
# MATLAB
\end{lstlisting}

\section{Библиотека \texttt{functools}}



\subsection{Каррированные функции с помощью \texttt{functools.partial}}

\begin{lstlisting}[
style = ironpython,
numbers = none	
]
from functools import partial

# инициализируем частичную функцию списком
part_f = partial(lambda lst, transform: [transform(elem) for elem in lst],
                 lst=['python', 'fortran'])
# передает преобразование
part_f(transform=str.upper)  # ['PYTHON', 'FORTRAN']
\end{lstlisting}

Иногда бывает удобно использовать метод \texttt{partialmethod}
\begin{lstlisting}[
style = ironpython,
emph = {__init__, set_live, get_live, __call__},
numbers = none	
]
from functools import partialmethod

class Live:
    def __init__(self):
        self._live = False
        
    def set_live(self, state: bool):
        self._live = state
        
    def get_live(self):
        return self._live
        
    def __call__(self):
        return self.get_live()
    
    # атрибуты класса    
    set_alive = partialmethod(set_live, True) # замораживает метод set_live() со значением True
    set_dead = partialmethod(set_live, False) # замораживает метод set_live() со значением False
    
live = Live()
live.set_alive()  # изменит значение атрибута self._live на True
                  # метод set_alive() вызовет метод set_live(True)
print(live())  # True: вызовет __call__, а тот в свою очередь get_live()
# разумеется можно вызвать метод set_live(False) и напрямую
\end{lstlisting}



\section{Калибровка классификаторов}

Подробности в статье А. Дьяконова \href{https://dyakonov.org/2020/03/27/%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D0%B0-%D0%BA%D0%B0%D0%BB%D0%B8%D0%B1%D1%80%D0%BE%D0%B2%D0%BA%D0%B8-%D1%83%D0%B2%D0%B5%D1%80%D0%B5%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8/}{<<Проблема калибровки уверенности>>}.
\vspace{2mm}

Ниже описываются способы оценить качество калибровки алгоритма. Надо сравнить \emph{уверенность} (confidence) и \emph{долю верных ответов} (accuracy) на тестовой выборке.

Если классификатор <<хорошо откалиброван>> и для большой группы объектов этот классификатор возвращает вероятность принадлежности к положительному классу 0.8, то среди этих объектов будет приблизительно 80\% объектов, которые в действительности принадлежат положительному классу. То есть, если для группы точек данных общим числом 100 классификатор возвращает вероятность положительного класса 0.8, то приблизительно 80 точек на самом деле будут принадлежать положительному классу и доля верных ответов тогда составит 0.8.

\subsection{Непараметрический метод гистограммной калибровки (Histogram Binning)}

Изначально в методе использовались бины одинаковой ширины, но можно использовать и равномощные бины.

Недостатки подхода:

\begin{itemize}
	\item число бинов задается наперед,
	
	\item функция деформации не непрерывна,
	
	\item в <<равноширинном варианте>> в некоторых бинах может содержаться недостаточное число точек.
\end{itemize}

Метод был предложен Zadrozny В. и Elkan C. \href{http://cseweb.ucsd.edu/~elkan/calibrated.pdf}{\ttfamily Obtaining  calibrated  probability  estimates  from  decision  trees  and naive bayesian classifiers}.

\subsection{Непараметрический метод изотонической регрессии (Isotonic Regression)}

Строится монотонно неубывающая функция деформации оценок алгоритма.

Метод был предложен Zadrozny B. и Elkan C. \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7457&rep=rep1&type=pdf}{\ttfamily Transforming classifier scores into accurate multiclass probability estimates}.

Функция деформации по-прежнему не является непрерывной.

\subsection{Параметрическая калибровка Платта (Platt calibration)}

Изначально этот метод калибровки разрабатывался только для метода опорных векторов, оценки которого лежат на вещественной оси (по сути, это расстояния до оптимальной разделяющей классы прямой, взятые с нужным знаком). Считается, что этот метод не очень подходит для других моделей.

Предложен Platt~J. \href{http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=EA4888FEE74FB677B492740F59CDFE1F?doi=10.1.1.41.1639&rep=rep1&type=pdf}{\ttfamily Probabilistic  outputs  for  support  vector machines and comparisons to regularized likelihood methods}.

\subsection{Логистическая регрессия в пространстве логитов}

\subsection{Деревья калибровки}

Стандартный алгоритм строит строит суперпозицию дерева решений на исходных признаках и логистических регрессий (каждая в своем листе) над оценками алгоритма:

\begin{itemize}
	\item Построить на исходных признаках решающее дерево (не очень глубокое),
	
	\item В каждом листе -- обучить логистическую регрессию на одном признаке,
	
	\item Подрезать дерево, минимизируя ошибку.
\end{itemize}

\subsection{Температурное шкалирование (Temperature Scaling)}

Этот метод относится к классу DL-методов калибровки, так как он был разработан именно для калибровки нейронных сетей. Метод представляет собой простое многомерное обобщение шкалирования Платта.

\section{Приемы работы с менеджером пакетов \texttt{conda}}

\subsection{Создание виртуального окружения}

Создать виртуальное окружение \texttt{dashenv}

\begin{lstlisting}[
numbers = none
]
conda create --name dashenv
\end{lstlisting}

Создать виртуальное окружение с указанием версии \texttt{Python}

\begin{lstlisting}[
numbers = none
]
conda create --name testenv python=3.6
\end{lstlisting}

Создать виртуальное окружение с указанием пакета

\begin{lstlisting}[
numbers = none
]
conda create --name testenv scipy
\end{lstlisting}

Создать виртуальное окружение с указанием версии \texttt{Python} и нескольких пакетов

\begin{lstlisting}[
numbers = none
]
conda create --name testenv python=3.6 scipy=0.15.0 astroid babel
\end{lstlisting}

\remark{%
Рекомендуется устанавливать сразу несколько пакетов, чтобы избежать конфликта зависимостей
}

Для того чтобы при создании нового виртуального окружения не требовалось каждый раз устанавливать базовые пакеты, которые обычно используются в работе, можно привести их список в конфигурационном файле \texttt{.condarc} в разделе \texttt{create\_default\_packages}

\begin{lstlisting}[
title = {\sffamily .condarc},
numbers = none
]
ssl_verify: true
channels:
 - conda-forge
 - defaults
report_errors: true
default_python:
create_default_packages:
 - matplotlib
 - numpy
 - scipy
 - pandas
 - seaborn
\end{lstlisting}

Если для текущего виртуального окружения не требуется устанавливать пакеты из набора по умолчанию, то при создании виртуального окружения следует указать специальный флаг
\lstinline{--no-default-packages}

\begin{lstlisting}[
numbers = none
]
conda create --no-default-packages --name testenv python
\end{lstlisting}

Создать виртуальное окружение можно и из файла \texttt{environment.yml} (первая строка этого файла станет именем виртуального окружения)

\begin{lstlisting}[
title = {\sffamily environment.yml},
numbers = none
]
name: stats2
channels:
 - conda-forge
 - defaults
dependencies:
 - python=3.6   # or 2.7
 - bokeh=0.9.2
 - numpy=1.9.*
 - nodejs=0.10.*
 - flask
 - pip:
  - Flask-Testing
\end{lstlisting}

\begin{lstlisting}[
numbers = none
]
conda env create -f environment.yml
\end{lstlisting}

При создании виртуального окружения можно указать путь до целевой директории, где будут размещаться файлы окружения. Следующая команда создаст виртуальное окружение в поддиректории текущей рабочей директории \texttt{envs}\footnote{В данном случае чтобы удалить виртуальную среду достачно просто удалить директорию \texttt{envs}}

\begin{lstlisting}[
numbers = none
]
conda create --prefix ./envs jupyterlab matplotlib
\end{lstlisting}

С помощью файла спецификации можно создать \emph{идентичное виртуальное окружение} (i) на той же платформе операционной системы, (ii) на той же машине, (iii) на какой-либо другой машине (перенести настройки окружения).

Для этого предварительно требуется создать собственно файл спецификации

\begin{lstlisting}[
numbers = none
]
conda list --explicit > spec-file.txt
\end{lstlisting}

Имя файла спецификации может быть любым. Файл спецификации обычно не является кросс-платформенным и поэтому имеет комментарий в верхней части файла (\lstinline{#platform: osx-64}), указывающий платформу, на которой он был создан.

Теперь для того чтобы \emph{создать} окружение достаточно воспользоваться командой

\begin{lstlisting}[
numbers = none
]
conda create --name myenv --file spec-file.txt
\end{lstlisting}

Файл спецификации можно использовать для установки пакетов в существующее окружение

\begin{lstlisting}[
numbers = none
]
conda install --name myenv --file spec-file.txt
\end{lstlisting}


\subsection{Активация/деактивация виртуального окружения}

Активировать виртуальное окружение \texttt{dashenv}

\begin{lstlisting}[
numbers = none
]
conda activate dashenv
\end{lstlisting}

Активировать виртуальное окружение в случае, когда оно создавалось с \lstinline{--prefix}, можно указав полный путь до окружения
\begin{lstlisting}[
numbers = none
]
conda activate E:\[WorkDirectory]\[Python_projects]\directory_for_experiments\envs
\end{lstlisting}

В этом случае в строке приглашения командной оболочки по умолчанию будет отображаться полный путь до окружения. Чтобы заменить длинный префикс в имени окружения на более удобный псевдоним достаточно использовать конструкцию

\begin{lstlisting}[
numbers = none
]
conda config --set env_prompt ({name})
\end{lstlisting}
которая добавит в конфигурационный файл \texttt{.condarc} следующую строку

\begin{lstlisting}[
title = {\sffamily .condarc},
numbers = none
]
...
env_prompt: ({name})
\end{lstlisting}
и теперь имя окружения будет \texttt{(envs)}.

Деактивировать виртуальное окружение

\begin{lstlisting}[
numbers = none
]
conda deactivate
\end{lstlisting}

\subsection{Обновление виртуального окружения}

Обновить виртуальное окружение может потребоваться в следующих случаях:

\begin{itemize}
	\item обновилась одна из ключевых зависимостей,
	
	\item требуется добавить пакет (добавление зависимости),
	
	\item требуется добавить один пакет и удалить другой.
\end{itemize}

В любом из этих случаев все что нужно для того чтобы обновить виртуальное окружение это просто обновить файл \texttt{environment.yml}\footnote{Этот файл должен находится в той же директории что и директория окружения \texttt{envs}}, а затем запустить команду

\begin{lstlisting}[
numbers = none
]
conda env update --prefix ./envs --file environment.yml --prune
\end{lstlisting}

Опция \lstinline{--prune} приводит к тому, что \texttt{conda} удаляет все зависимости, которые больше не нужны для окружения.

\subsection{Вывод информации о виртуальном окружении}

Вывести список доступных виртуальных окружений

\begin{lstlisting}[
numbers = none
]
conda env list
\end{lstlisting}

Вывести список пакетов, установленных в указанном окружении

\begin{lstlisting}[
numbers = none
]
conda list --name myenv
\end{lstlisting}

Вывести информацию по конкретному пакету указанного окружения

\begin{lstlisting}[
numbers = none
]
conda list --name dashenv matplotlib
\end{lstlisting}


\subsection{Удаление виртуального окружения}

Удалить виртуальное окружение \texttt{heroku\_env}

\begin{lstlisting}[
numbers = none
]
conda env remove --name heroku_env
\end{lstlisting}

\subsection{Экспорт виртуального окружения в \texttt{environment.yml}}

Экспортировать активное виртуальное окружение в yml-файл

\begin{lstlisting}[
numbers = none
]
conda env export > environment.yml
\end{lstlisting}


\section{Инструмент автоматического построения дерева проекта под задачи машинного обучения}

Для автоматизации построения типового (или кастомизированного) дерева проекта по машинному обучению и анализу данных удобно использовать \href{https://cookiecutter.readthedocs.io/en/1.7.2/README.html}{\texttt{cookicutter}}.

На операционную систему под управлением \texttt{Windows} \texttt{cookicutter} можно установить с помощью менеджера пакетов \texttt{pip}

\begin{lstlisting}[
numbers = none
]
pip install cookiecutter
\end{lstlisting} 
а на операционную систему под управлением \texttt{MacOS X} с помощью менеджера \texttt{brew}

\begin{lstlisting}[
numbers = none
]
brew install cookiecutter
\end{lstlisting}

В самом простом случае \texttt{cookicutter} можно использовать как утилиту командной строки. Например для того чтобы создать проект по шаблону для задач машинного обучения достаточно сделать следующее

\begin{lstlisting}[
style = ironpython,
numbers = none,
]
cookiecutter https://github.com/drivendata/cookiecutter-data-science
\end{lstlisting}

Утилита предложит ответить на несколько вопросов (название репозитория, имя автора и т.д.), а затем создаст дерево проекта.

\section{Управление локальными переменными окружения проекта}

Для того чтобы создать \emph{локальные переменные проекта}\footnote{То есть переменные, привязанные к текущему проекту} достаточно разместить пары вида <<ключ=значение>> в файле \texttt{.env}, а затем прочитать его с помощью специальной библиотеки \texttt{dotenv} \url{https://pypi.org/project/python-dotenv/}. Например

\begin{lstlisting}[
style = ironpython,
numbers = none
]
#.env в текущей директории проекта
EMAIL = leor.finkelberg@yandex.ru
POSTGRESQL_PASSWORD = Evdimonia
\end{lstlisting}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import os
from pathlib import Path
from dotenv import load_dotenv

dotenv_path = Path(__file__).resolve().parents[0].joinpath('.env')
print(f'[INFO] path: {dotenv_path}')  # [INFO] path: E:\[WorkDirectory]\[Python_projects]\directory_for_experiments\.env

load_dotenv(dotenv_path)  # загрузить .env

# извлекать значения локальных переменных окружения проекта можно с помощью `os.getenv(key)`
# или `os.environ.get(key)`
for key in (s.upper() for s in ('email', 'postgresql_password')):
    print('[INFO] from file `.env`({}) -> {}'.format(key, os.getenv(key)))
\end{lstlisting}

\section{Приемы работы с модулем \texttt{subprocess}}

Ниже приводится пример использования модуля \texttt{subprocess} для отыскания самого большого файла в \texttt{git}-репозитории
\begin{lstlisting}[
style = ironpython,
emph = {popen_2_str},
numbers = none
]
import os
import subprocess
import pathlib
from subprocess import Popen, PIPE, STDOUT

# --- объявление функций: begin
def popen_2_str(cmd: str, shell=True, universal_newlines=True, stdout=PIPE) -> str:
    return Popen(cmd, shell=shell,
                 universal_newlines=universal_newlines,
                 stdout=stdout).stdout.read().strip()

def stat(filename):
    res = popen_2_str(f"stat {filename}")
    print(f'>>> Statistic:\n{res}')

def summary(commits):
    print(f'### Summary ({__file__}) ###:\n>>> idx-file name: {idx_file}'
          f'\n>>> SHA blob: {shablob}\n>>> Commits:')
    print(commits)
# --- объявление функций: end

GIT_PATH = pathlib.Path('.git/objects/pack/')

# тоже самое что и `git gc &> /dev/null`
exit_code = subprocess.call("git gc", shell=True,
                            stdout=open(os.devnull, 'w'), stderr=STDOUT)

if not exit_code:
    # возвращает имя idx-файла
    idx_file = popen_2_str(f"ls -l {GIT_PATH} | grep -iE '*.idx' "
                           f"| awk -F ' ' '{{ print $9 }}'")
    # возвращает абсолютный путь до idx-файла
    abs_path_idx_file = pathlib.Path.joinpath(GIT_PATH, idx_file)
    if os.path.exists(abs_path_idx_file):
    # возвращает SHA <<большого>> файла
        shablob = popen_2_str(f"git verify-pack -v {abs_path_idx_file} | sort -k 3 -n "
                              f"| tail -n 1 | awk -F ' ' '{{ print $1 }}'")
        # возвращает имя файла по его SHA
        filename = popen_2_str(f"git rev-list --objects --all | grep {shablob} "
                               f"| awk -F ' ' '{{ print $2 }}'")
        # возвращает коммиты, связанные с данным файлом
        commits = popen_2_str(f"git log --oneline -- {filename}")
        summary(commits)
        stat(filename)
    else:
        print(f"File {abs_path_idx_file} not found...")
else:
    print('Something went wrong.')
\end{lstlisting}

\section{Решающие деревья и сопряженные вопросы}

\subsection{Коэффициент Джини}

\emph{Коэффициент Джини}\footnote{Еще говорят индекс Джини или загрязненность Джини} (Gini impurity) это просто вероятность неверной маркировки в узле случайно выбранного образца (для чистых листьев коэффициент Джини равен 0)
\begin{align}
	I_G(n) = 1 - \sum_{i=1}^{J}p_i^2,
\end{align}
где $ p_i $ -- частоты представителей разных классов в листе дерева.

К примеру, если решается задача бинарной классификации ($ J = 2 $) на выборке из 6 объектов и в данном расщеплении в один класс попали 2 объекта, а в другой 4, то индекс Джини будет равен
\begin{align}
	I_G(n)=1 - \left( \left( \frac{2}{6} \right)^2 + \left( \frac{4}{6} \right)^2 \right) = 0,444.
\end{align}

\subsection{Случайный лес}

Случайный лес -- это модель, представляющая ансамбль решающих деревьев, дополненная двумя концепциями:
\begin{itemize}
	\item концепцией бутрстапированных выборок,
	
	\item концепцией случайных подпространств.
\end{itemize}

Хотя каждое решающее дерево может иметь большой разброс по отношению к определенному набору тренировочных данных, обучение деревьев на разных наборах образцов позволяет снизить общий разброс леса.


\section{Анализ временных рядов}

\subsection{Признаки на временных рядах}

Можно выделить следующие несколько групп признаков, которые можно вычислить на временных рядах:
\begin{itemize}
	\item признаки на основе коэффициентов автокорреляции и частных коэффицентов автокорреляции,
	
	\item оптимальное значение параметра $ \lambda $ преобразования Бокса-Кокса,
	
	\item коэффициент Херста,
	
	\item количество раз, когда временной ряд пересекает свою собственную медиану,
	
	\item признаки, рассчитываемые на основе STL-компонент разложения временного ряда,
	
	\item дисперсия дисперсии, рассчитанных по наблюдениям из непересекающихся временных отрезков,
	
	\item дивергенция Кульбака-Лейблера в следующих друг за другом отрезках,
	
	\item спектральная энтропия ряда,
	
	\item дисперсия средних значений,
	
	\item минимальное число дифференциирований временного ряда, необходимое для достижения его стационарности.
\end{itemize}

\subsection{Прогнозирование временных рядов. Метод имитированных исторических прогнозов}

При разбиении данных на обучающую и проверочную выборки важно помнить о том, как модель в итоге будет использоваться на практике. Так, при выполнении предсказаний для той же генеральной совокупности, из которой получены исходные данные (\emph{интерполяция}), достаточным может оказаться простое случайное разбиение данных. В случаях же, когда модель предназначена для прогнозирования будущего (\emph{экстраполяция}), более точную оценку ее предсказательных свойств можно получить только если проверочная выборка содержит данные из будущего (например, если исходные данные охватывают период в два года, то модель можно было бы обучить на данных первого года, а затем проверить ее обобщающую способность на данных второго года).

Стандартным методом оценки качества нескольких альтернативных моделей является перекрестная проверка. Суть этого метода сводится к тому, что исходные обучающие данные случайным образом разбиваются на $ k $ блоков, после чего модель $ k $ раз обучается на $ k - 1 $ блоках, а оставшийся блок каждый раз используется для проверки качества предсказаний на основе той или иной подходящей случаю метрики. Полученная таким образом средняя метрика будет хорошей оценкой качества предсказаний модели на новых данных.

К сожалению, в случае с моделями временных рядов такой способ выполнения перекрестной проверки будет бессмысленным и не отвечающим стоящей задаче. Поскольку во временных рядах, как правило, имеет место тесная корреляция между близко расположенными наблюдениями, мы не можем просто разбить такой ряд случайным образом на $ k $ частей -- это приведет к потере указанной корреляции. Более того, в результате случайного разбиения данных на несколько блоков может получиться так, что в какой-то из итераций мы построим модель преимущественно по недавним наблюдениям, а затем оценим ее качество на блоке из давних наблюдений. Другими словами, мы построим модель, которая будет предсказывать прошлое, что не имеет никакого смысла -- ведь мы пытаемся решить задачу по предсказаним будущего.

Для решения описанной проблемы при работе с временными рядами применяют несколько модификаций перекрестной проверки. Например, в пакете Prophet, реализован так называемый метод <<имитированных исторических прогнозов>> (simulated historical forecast).

Метод имитированных исторических прогнозов \url{https://r-analytics.blogspot.com/2019/10/prophet-shf.html}. Для создания модели временного ряда мы используем данные за определенный исторический отрезок времени. Далее по полученной модели рассчитываются прогнозные значения для некоторого интересующего нас промежутка времени (горизонта прогноза) в будущем. Такая процедура повторяется каждый раз, когда необходимо сделать новый прогноз.

В пределах отрезка с исходными обучающими данными выбирают $ k $ точек отсчета (в терминологии Prophet), на основе которых формируются блоки данных для выполнения перекрестной проверки: все исторические наблюдения, предшествующие $ k $-ой точке отсчета (а также сама эта точка), образуют обучающие данные для подгонки соответствующей модели, а $ H $ исторических наблюдений, следующих за точкой отсчета, образуют \emph{прогнозный горизонт}. Расстояние между точками отсчета называется периодом и по умолчанию составляет $ H/2 $. Обучающие наблюдения в первом из $ k $ блоков образуют так называемый начальный отрезок. В Prophet длина этого отрезка по умолчанию составляет $ 3\,H $, однако этот параметр можно изменить.

Каждый раз после подгонки модели на обучающих данных из $ k $-ого блока рассчитываются предсказания для прогнозного горизонта того же блока, что позволяет оценить качество прогноза с помощью подходящей метрики. Значения этой метрики, усредненные по каждой дате прогнозных горизонтов каждого блока, в итоге дают оценку качества предсказаний, которую можно ожидать от модели, построенной \emph{по всем исходным обучающим данным}.

\subsection{Обнаружение аномалий во временных рядах}

Обнаружение аномалий относится к поиску непредвиденных значений (паттернов) в потоках данных. Аномалия (выброс, ошибка, отклонение или исключение) -- это отклонение поведение системы от стандартного (ожидаемого).

Аномалии могут возникать в данных самой различной природы и структуры в результате технических сбоев, аварий, преднамеренных взломов и т.д.

Аномалии в данных могут быть отнесены к одному из трех основных типов \cite{chandola:2009}:
\begin{itemize}
	\item \emph{Точечные аномалии}: возникают в ситуации, когда отдельный экземпляр данных может рассматриваться как аномальный по отношению к остальным данным; большинство существующих методов создано для распознавания точечных аномалий,
	
	\item \emph{Контекстуальные аномалии}: наблюдаются, если экземпляр данных является аномальным лишь в определенном контексте (данный вид аномалий также называется условным)
	
	\begin{itemize}
		\item контекстуальные атрибуты используются для определения контекста (или окружения) для каждого экземпляра; во временных рядах контекстуальным атрибутом является время, которое определяет положение экземпляра в целой последовательности; контекстуальным атрибутом также может быть положение в пространстве или более сложные комбинации свойств,
		
		\item поведенческие атрибуты определяют не контекстуальные характеристики, относящиеся к конкретному экземпляру данных,
	\end{itemize}
	
	\item \emph{Коллективные аномалии}: возникают, когда последовательность связанных экземпляров данных (например, фрагмент временного ряда) является аномальной по отношению к целому набору данных. Отдельный экземпляр данных в такой последовательности может не являться отклонением, однако совместное появление таких экземляров является коллективной аномалией; кроме того, если точечные или контекстуальные аномалии могут наблюдаться в любом наборе данных, то коллективные наблюдаются только в тех, где данные связанны между собой.
\end{itemize}

Часто для решения задачи поиска аномалий требуется набор данных, описывающих систему. Каждый экземпляр в нем описывается меткой, указывающей, является ли он нормальным или аномальным. Таким образом, множество экземпляров с одинаковой меткой формируют соответствующий класс. 

Создание подобной промаркированной выборки обычно проводится вручную и является трудоемким и дорогостоящим процессом. В некоторых случаях получить экземпляры аномального класса невозможно в силу отсутствия данных и возможных отклонениях в системе, в других могут отсутствовать метки обоих классов. В зависимости от того, какие классы данных используются для реализации алгоритма, методы поиска аномалий могут выполняться в одном из трех перечисленных режимов:
\begin{itemize}
	\item \textbf{Режим распознавания с учителем}. Данная методика требует наличия обучающей выборки, полноценно представляющей систему и включающей экземпляры данных нормального и аномального классов. Работа алгоритма происходит в два этапа: обучение и распознование. На первом этапе строится модель, с которой в последствии сравниваются экземпляры, не имеющие метки. В большинстве случаев предполагается, что \emph{данные} \emph{не меняют свои статистические характеристики}, иначе возникает необходимость изменять классификатор. Основной сложностью алгоритмов, работающих в режиме распознования с учителем, является формирование данных для обучения. Часто аномальный класс представлен значительно меньшим числом экземпляров, чем нормальный, что может приводить к неточностям в полученной модели. В таких случаях применяется \emph{искусственная генерация аномалий}.
	
	\textbf{Режим распознования частично с учителем}. Исходные данные при этом подходе представляют только нормальный класс. Обучившись на одном классе, система может определять принадлежность новых данных к нему, таким образом, определяя противоположенный. Алгоритмы, работающие в режиме распознования частично с учителем, не требуют информации об аномальном классе экземпляров, вследствие чего они шире применимы и позволяют распознавать отклонения в отсутствие заранее определенной информации о них.
	
	\textbf{Режим распознавания без учителя}. Применяется при отсутствии априорной информации о данных. Алгоритмы распознавания в режиме без учителя базируются на предположении о том, что аномальные экземпляры встречаются гораздо реже нормальных. Данные обрабатываются, наиболее отдаленные определяются как аномалии. Для применения этой методики должен быть доступен весь набор данных, т.е. она не может применяться в режиме реального времени.
\end{itemize}

Метод опорных векторов\footnote{В \texttt{sklearn} есть реализация одноклассового метда опорных векторов \href{https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html}{\ttfamily OneClassSVM} (позволяет задать долю аномальных объектов в выборке с помощью параметра \texttt{nu})} применяется для поиска аномалий в системах, где нормальное поведение представляется только одним классом. Данный метод определяет границу региона, в котором находятся экземпляры нормальных данных. Для каждого исследуемого экземпляра определяется, находится ли он в определенном регионе. Если экземпляр оказывается вне региона, он определяется как аномальный.

Пример использования одноклассового метода опорных векторов
\begin{lstlisting}[
style = ironpython,
emph = {transform_to_zero_minus_one},
numbers = none
]
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import OneClassSVM


def transform_to_zero_minus_one(arr):
    return np.where(arr < 0, arr, 0)

N = 250  # длина временного ряда
scale = 50  # масштаб меток для графика аномалий

# подготавливаем тренировочный и тестовый набор данных
data_train = np.random.RandomState(42).randn(N)
data_test = np.random.RandomState(2).randn(int(0.1*N))
data_train[[40, 50, 80]] *= 100
data_test[[2, 5]] *= 50

# обучаем классификатор и готовим предсказания
clf = OneClassSVM(nu=0.03).fit(data_train.reshape(-1, 1))
predicted_anomalies = clf.predict(data_test.reshape(-1, 1))

plt.plot(data_test,
    marker = '.',
    markersize = 12,
    markerfacecolor = 'w',
    color = 'k',
    label='тестовый набор данных')

plt.bar(np.arange(0, data_test.shape[0]),
    transform_to_zero_minus_one(predicted_anomalies)*scale,
    alpha = 0.5,
    color = 'b',
    label='аномалии')
plt.legend()
\end{lstlisting}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{figures/oneclasssvm_test.pdf}
	\caption{ Пример детектирования аномалий на тестовой наборе данных }\label{fig:oneclasssvm}
\end{figure}

Кластеризация. Данная методика предполагает группировку похожих экземпляров в кластеры и не требует знаний о свойствах возможных отклонений: нормальные данные образуют большие плотные кластеры, а аномальные -- маленькие и разрозненные. Одной из простейших реализацией подхода на основе кластеризации является алгоритм метода $ k $-средних.

При использовании методов статистического анализа исследуется процесс, строится его профиль (статистическая модель), которые затем сравнивается с реальным поведением. Если разница в реальном и предполагаемом поведении системы, определяется заданной функцией аномальности, выше установленного порога, делается вывод о наличии отклонений. Применяется предположении о том, что нормальное поведение системы будет находиться в зоне высокой вероятности, в то время как выбросы -- в зоне низкой.

Данный класс методов удобен тем, что не требует заранее определенных знаний о виде аномалии. Однако сложности могут возникать в определении точного статистического распределения и порога.

Методы статистического анализа подразделяются на две группы:
\begin{itemize}
	\item \emph{Параметрические методы}. Предполагают, что нормальные данные генерируются параметрическим распределением с параметрами $ \theta $ и функцией плотности вероятности $ \mathbb{P}(x, \theta) $, где $ x $ -- наблюдение. Аномалия является обратной функцией распределения. Эти методы часто основываются на Гауссовской или регрессионной модели, а также их комбинации.
	
	\item Непараметрические методы. Предполагается, что структура модели не определена априорно, вместо этого она определяется из предоставленных данных. Включает методы на основе гистограмм или функции ядра.
\end{itemize}

Базовый алгоритм поиска аномалий с применением гистограмм включает два этапа. На первом этапе происходит построение гистограммы на основе различных значений выбранной характеристики для экземпляров тренировочных данных. На втором этапе для каждого из исследуемых экземпляров определяется принадлежность к одному из столбцов гистограммы. Не принадлежащие ни к одному из столбцов экземпляры помечаются как аномальные.

Алгоритм ближайшего соседа. Для использования данной методики необходимо определить понятие расстояния (меры похожести) между объектами. Примером может быть евклидово расстояние.

Два основных подхода основаваются на следующих предположениях:
\begin{itemize}
	\item Расстояние до $ k $-ого ближайшего соседа. Для реализации этого подхода расстояние до ближайшего объекта определяется для каждого тестируемого экземпляра класса. Экземпляр, являющийся выбросом, наиболее отдален от ближайшего соседа.
	
	\item Использование относительной плотности основано на оценке плотности окрестности каждого экземпляра данных. Экземпляр, который находится в окрестности с низкой плотностью, оценивается как аномальный, в то время как экземпляр в окрестности с высокой плотностью оценивается как нормальный. Для данного экземпляра данных расстояние до его $ k $-ого ближайшего соседа эквивалентно радиусу гиперсферы с центром в данном экземпляре и содержащей $ k $ остальных экземпляров.
\end{itemize}

Выявление аномалий в режиме реального времени может потребовать дополнительной модификации методов. Наиболее простым в реализации является \emph{алгоритм скользящего окна}.

Данная методика используется для временных рядов, которые разбиваются на некоторое число последовательностей -- окон. Необходимо выбрать окно фиксированной длины, меньшей чем длина самого временного ряда, чтобы захватить аномалию в процессе скольжения. Поиск аномальной последовательности осуществляется при помощи скольжения окна по всему ряду с шагом, меньшим длины окна.




\subsection{Приемы работы с библиотекой \texttt{Prophet}}

Установить библиотеку можно с помощью менеджера пакетов \texttt{conda}
\begin{lstlisting}[
style = bash,
numbers = none
]
conda install -c conda-forge fbprophet
\end{lstlisting}

\texttt{Prophet} была разработана для прогнозирования большого числа различных бизнес-показателей и строит неплохие baseline-прогнозы.

По сути Prophet-модель представляет собой аддитивную регрессионную модель
\begin{align*}
	y(t) = g(t) + s(t) + h(t) + \varepsilon_t,
\end{align*}
где $ g(t) $ -- тренд (может быть представлен \emph{кусочно-линейной} или \emph{логистической функцией}\footnote{Логистическая функция удобна для моделирования роста с насыщением, когда при увеличении показателя снижается темп его роста}); $ s(t) $ -- сезонная компонента, отвечающая за периодические/квазипериодические изменения, связанные с \emph{недельной} и \emph{годовой сезонностью}\footnote{Моделируется с помощью рядов Фурье}; $ h(t) $ -- отвечает за аномальные дни (праздники, Black Fridays и т.д.); $ \varepsilon $ -- содержит информацию, которая не учтена моделью.

Подробнее о математической стороне вопроса рассказывается в статье \url{https://peerj.com/preprints/3190/}. К слову, в этой статье качество моделей оценивается с помощью MAPE и MAE. MAPE (mean absolute percentage error) -- это средняя абсолютная ошибка нашего прогноза. Пусть $ y_i $ -- значение целевого вектора, а $ \hat{y}_i $ -- это соответствующий этой величине прогноз модели. Тогда $ \varepsilon_i = y_i - \hat{y}_i $ -- это ошибка прогноза, а $ p_i = \frac{\varepsilon_i}{y_i} $ -- относительная ошибка прогноза.

Таким образом средняя абсолютная ошибка выражается следующей формулой
$$
MAPE = \frac{1}{N}\sum_{i = 1}^{N} |p_i|.
$$

MAPE часто используется для оценки качества, поскольку эта величина относительная и по ней можно сравнивать качество даже на различных наборах данных.

Библиотека \texttt{Prophet} имеет интерфейс, похожий на интерфейс \texttt{sklearn}: сначала мы создаем модель, затем вызываем у нее метод \texttt{fit} и затем получаем прогноз. На вход метод \texttt{fit} получает объект DataFrame с двумя столбцами: \texttt{ds} -- временная метка (поле должно иметь тип \texttt{date} или \texttt{timestamp}), и целевой показатель \texttt{y}.

Разработчики рекомендуют делать предсказания по нескольким месяцам данных (в идеале год и более).

Пример
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import fbprophet
from fbprophet.plot import add_changepoints_to_plot
import pandas as pd
import matplotlib.pyplot as plt


data_all = pd.read_csv('AirPassengers.csv')
# в наборе данных, на котором обучается модель обязательно должны быть столбцы `ds` и `y`
data_all = data_all.rename(columns={ 'Month' : 'ds', 'Passengers' : 'y' })
data_all['ds'] = pd.to_datetime(data_all['ds'])
M = 100
data_train = data_all[:M]  # обучающий набор данных
data_test = data_all[M:]   # тестовый набор данных

model = fbprophet.Prophet(
    changepoint_prior_scale=0.035,
    weekly_seasonality=True,
    yearly_seasonality=True,
    seasonality_mode='multiplicative'
)
model.fit(data_train)  # обучение модели

future_points = data_test.shape[0]  # число точек прогнозного горизонта
# преобразование в точек в метки, имеющие смысл времени
time_points_for_predict = model.make_future_dataframe(future_points, freq='M')
forecast = model.predict(time_points_for_predict)  # прогноз

fig, ax = plt.subplots(figsize=(8, 4))
plt.plot(data_train['ds'], data_train['y'], marker='.', label='Train data')
plt.plot(data_test['ds'], data_test['y'], marker='.', color='k', label='Test data')
plt.plot(forecast['ds'][M:], forecast['yhat'][M:], marker='.', color='r', label='Predict')
plt.axvspan(forecast['ds'][M],forecast['ds'][M+43], facecolor='grey', alpha=0.25)
plt.legend()
# добавить точки перегиба
a = add_changepoints_to_plot(fig.gca(), model, forecast)
\end{lstlisting}

С помощью конструкции \lstinline{model.plot_components(forecast);} можно посмотреть компоненты временного ряда (тренд, недельную и годовую сезонность).

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.65]{figures/prophet_test.pdf}
	\caption{Пример использования библиотеки \texttt{fbprophet}}\label{fig:prophet_test}
\end{figure}

С помощью библиотеки \texttt{Prophet} можно учитывать эффекты <<праздников>>. Под термином <<праздник>> здесь понимается как <<настоящие>> официальные праздничные и выходные дни (например, Новый Год, Рождество и пр.), так и другие события, во время которых свойства моделируемой зависимой переменной существенно изменяются (спортивные или культурные мероприятия, природные явления и пр.).

Для добавления эффектов <<праздников>> в Prophet-модель необходимо сначала создать отдельную таблицу, содержащую как минимум два обязательных столбца: \texttt{holiday} и \texttt{ds}. Важно, чтобы эта таблица охватывала как исторический период, на основе которого происходит обучение модели, так и период в будущем, для которого необходимо сделать прогноз. Например, если какое-то важное событие встречается в обучающих данных, то его следует указать и для прогнозного периода (при условии, конечно, что мы ожидаем повторение этого события в будущем, и что дата этого события входит в прогнозный период).

Параметры класса \texttt{Prophet}:
\begin{itemize}
	\item \texttt{growth}: тип тренда. Принимает два возможных значения: linear и logistic,
	
	\item \texttt{changepoints}: список временных меток, соответствующих точкам излома тренда (т.е. датам, когда, как предполагается, произошли существенные изменения в тренде временного ряда). Если этот список не задан, то такие точки излома будут вычисляться автоматически,
	
	\item \texttt{n\_changepoints}: предполагаемое количество, точек излома (по умолчанию 25). Если параметр \texttt{changepoints} задан, то параметр \texttt{n\_changepoints} будет проигнорирован. Если же \texttt{changepoints} не задан, то \texttt{n\_changepoints} потенциальных точек излома будут распределены равномерно в предалах исторического отрезка, заданного параметром \texttt{changepoint\_range},
	
	\item \texttt{changepoint\_range}: доля исторических данных (начиная с самого первого наблюдения), в пределах которых будут оценены точки излома. По умолчанию составляет 0.8 (т.е. 80\% наблюдений),
	
	\item \texttt{yearly\_seasonality}: параметр настройки годовой сезонности (т.е. закономерных колебаний в пределах года). Принимает следующие возможные значения: auto, True, False или количество членов ряда Фурье, с помощью которого аппроксимируются компоненты годовой сезонности,
	
	\item \texttt{weekly\_seasonality}: параметр настройки недельной сезонности (т.е. закономерных колебаний в пределах недели). Возможные значения те же, что и у \texttt{yearly\_seasonality},
	
	\item \texttt{daily\_seasonality}: параметр настройки дневной сезонности (т.е. закономерных колебаний в пределах дня). Возможные значения те же, что и у \texttt{yearly\_seasonality},
	
	\item \texttt{holidays}: объект-DataFrame со столбцами \texttt{holiday} и \texttt{ds}. По желанию можно добавить еще два столбца -- \texttt{lower\_window} и \texttt{upper\_window}, которые задают отрезок времени вокруг соответствующего события,
	
	\item \texttt{seasonality\_mode}: режим моделирования сезонных компонент. Принимает два возможных значения: additive и multiplicative,
	
	\item \texttt{seasonality\_prior\_scale}: параметр, задающий <<силу>> сезонных компонентов модели (10 по умолчанию). Более высокие значения приведут к более <<гибкой>> модели, а низкие -- к модели со слабо выраженными сезонными эффектами,
	
	\item \texttt{holidays\_prior\_scale}: параметр, задающий выраженность эффектов <<праздников>> и других важных событий (по умолчанию 10). Если объект-DataFrame, передаваемый в параметр \texttt{holidays}, имеет столбец \texttt{prior\_scale}, то параметр \texttt{holidays\_prior\_scale} будет проигнорирован,
	
	\item \texttt{changepoint\_prior\_scale}: параметр, задающий <<гибкость>> автоматического механизма обнаружения <<точек излома>> (по умолчанию 0.05). Более высокие значения позволят иметь больше таких точек излома,
	
	\item \texttt{mcmc\_samples}: целое число (по умолчанию 0). Если $ > 0 $, то параметры модели будут оценены путем \emph{полного байесовского анализа} с использованием указанного числа итераций алгоритма MCMC. Если 0, тогда используется \emph{оценка апостериорного максимума} (MAP),
	
	\item \texttt{interval\_width}: число, определяющее ширину доверительного интервала для предсказанных моделью значений (по умолчанию 0.8, что соответствует 80\%-ному интервалу),
	
	\item \texttt{uncertainty\_samples}: количество итераций для оценивания доверительных интервалов (по умолчанию 1000).
\end{itemize}

\emph{Оценка максимума апостериорной вероятности} (maximum aposteriori probability, MAP) тесно связана с \emph{методом наибольшего правдоподобия} (ML), но дополнительно при оптимизации использует априорное распределение величины, которую оценивает.

Можно записать
\begin{align*}
	\hat{\theta}_{\rm MAP}(x) = \underset{\theta}{\operatorname{arg\,max}}\, f(x | \theta)g(\theta),
\end{align*}
где $ f(x | \theta) $ -- функция правдоподобия, $ g(\theta) $ -- априорная плотность распределения оцениваемого параметра $ \theta $.

Пример. Предположим, что у нас есть последовательность $ (x_1, \dots, x_n) $ i.i.d (независимых и одинаково распределенных) $ N(\mu, \sigma_v^2) $ случайных величин и априорное распределение $ \mu $ задано $ N(0, \sigma_m^2) $. Требуется найти MAP-оценку $ \mu $.

Функция, которую нужно максимизировать задана
\begin{align*}
	\pi(\mu)L(\mu) = \dfrac{1}{\sqrt{2\pi \sigma_m}} \exp \Bigg( - \dfrac{1}{2}\Big(\dfrac{\mu}{\sigma_m}\Big)^2 \Bigg) \prod_{j=1}^{n} \dfrac{1}{\sqrt{2\pi \sigma_v}} \exp \Bigg( -\dfrac{1}{2}\Big( \dfrac{x_j - \mu}{\sigma_v} \Big)^2 \Bigg).
\end{align*}

Теперь остается записать логарифм этой функции, затем найти производную по оцениваемому параметру, приравнять полученную производную нулю и, наконец, выразить искомый параметр. Что в итоге даст
\begin{align*}
    \hat{\mu}_{\rm MAP} = \dfrac{\sigma_m^2}{n\sigma_m^2 + \sigma_v^2} \sum_{j=1}^{n}x_j.
\end{align*}





\subsection{Преобразование нестационарного временного ряда в стационарный}

Чтобы превратить нестационарный ряд в стационарный можно использовать следующие общие приемы:
\begin{itemize}
	\item выделить в структуре временного ряда тренд и сезонную компоненту, затем удалить их исходного временного ряда; построить прогноз на временном ряду, приведенном к стационарному, а после вернуть эти компоненты в прогноз,
	
	\item провести сглаживание (за несколько часов, за неделю и т.п.); в простейших случаях, когда период временного четко определен, можно пользоваться обычным скользящим средним, но в более сложных случаях, когда период сложно подсчитать, следует пользоваться \emph{экспоненциально-взвешенным скользящим средним}
	\lstinline{time_series.ewm(halflife=12).mean()}.
\end{itemize}


\subsection{Стабилизация дисперсии}

Для временных рядов с \emph{монотонно} меняющейся дисперсией можно использовать стабилизирующие преобразования. Например, \emph{логарифмирование} \lstinline{np.log(ts)}.

Если исходный временной ряд не проходит тест на \emph{гауссовость}, то можно либо воспользоваться непараметрическими методами, либо обратиться к специальным приемам, позволяющим преобразовать исходную ненормальную статистику в нормальную.

Среди множества таких методов преобразований одним из лучших (при неизвестном типе распределения) считается \href{http://www.machinelearning.ru/wiki/index.php?title=Метод_Бокса-Кокса}{\itshape преобразование Бокса-Кокса}\footnote{Степенные преобразования -- это семейство параметрических, монотонных преобразований, целью которых является отображение данных из произвольного распределения в близкое к гауссовскому распределению таким образом, чтобы \emph{стабилизировать дисперсию} и \emph{минимизировать ассиметрию}}, то есть это преобразование \emph{нормализует} данные (делает их более гауссовскими)
\begin{align*}
	\hat{y}_i =
	\begin{cases}
		\log y_i, &\lambda = 0,\\
		(y_i^\lambda - 1)/\lambda, &\lambda \neq 0
	\end{cases}
\end{align*}
для исходной последовательности $ y = \{y_1, \dots, y_n\}, \,y_i > 0,\, i = (1, \dots, n) $.

Пример использования преобразования Бокса-Кокса приведен на \pic{fig:boxcox}. Такого рода преобразования полезны в ситуациях, связанных с проблемой \emph{гетероскедостичности} (непостоянная дисперсия), или в ситуациях, где требуется \emph{гауссовость} данных.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.70]{figures/boxcox.pdf}
	\caption{Влияние преобразования Бокса-Кокса на временной ряд\\с изменяющейся во времени дисперсией}\label{fig:boxcox}
\end{figure}

Параметр $ \lambda $ можно подбирать так, чтобы дисперсия была как можно более стабильной во времени. Прямое и обратное преобразования Бокса-Кокса реализованы в библиотеках \texttt{scipy} и \texttt{statsmodels}
\begin{lstlisting}[
style = ironpython,
emph = {invboxcox},
numbers = none
]
from scipy.stats import boxcox
from statsmodels.tsa.hotwinters import (
                                        #boxcox,
                                        inv_boxcox
                                        )

# пользуемся готовым решением для обратного преобразования Бокса-Кокса
lmbda = 0.25
arr = np.array([3, 5, 10])
# можно задать значение ламбда самому или позволить вычислить его
arr_transformed = boxcox(arr, lmbda) # array([1.26429605, 1.98139512, 3.11311764])
arr_transformed, lmbda_compute = boxcox(arr)  # здесь lmbda вычисляется
                                              # с помощью максимизации логарифма правдоподобия
inv_boxcox(arr_transformed, lmbda)  # array([ 3.,  5., 10.])

# пишем свою реализацию обратного преобразования Бокса-Кокса
def invboxcox(arr: np.array, lmbda: np.float) -> np.array:
    if lmbda == 0:
        return (np.exp(arr))
    else:
        return (np.exp(np.log(lmbda*arr + 1)/lmbda))
\end{lstlisting}

Так как классическое преобразование Бокса-Кокса предполагает работу только с положительными величинами, то было предложено несколько модификаций, учитывающих нулевые и отрицательные значения. Самым очевидным вариантом является сдвиг всех значений на некоторую константу $ \alpha $ так, чтобы выполнялось условие $ (y_i + \alpha) > 0, \, i = 1,\dots, n $
\begin{align*}
	\hat{y}_i =
	\begin{cases}
		\log (y_i + \alpha), &\lambda = 0,\\
		\dfrac{(y_i + \alpha)^\lambda - 1}{\lambda}, &\lambda \neq 0.
	\end{cases}
\end{align*}

Также для того чтобы сделать данные <<более гауссовскими>> можно воспользоваться \emph{преобразованием Йео-Джонсона} (Yeo-Johnson)
\begin{align*}
	\hat{y}_i = 
	\begin{cases}
		\dfrac{(y_i + 1)^\lambda - 1}{\lambda}, &\lambda \neq 0, y_i \geqslant 0,\\
		\ln(y_i + 1), &\lambda = 0, y_i \geqslant 0,\\
		- \dfrac{(-y_i + 1)^{2 - \lambda} - 1}{2 - \lambda}, &\lambda \neq 2, y_i < 0,\\
		-\ln(-y_i + 1), &\lambda = 2, y_i < 0.
	\end{cases}
\end{align*}

Преобразование Йео-Джонсона (как впрочем и преобразование Бокса-Кокса) реализовано в библиотеке \texttt{sklearn} (см.~раздел документации \href{https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-transformer}{Non-linear transformation})
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import numpy as np
from sklearn.preprocessing import PowerTransformer
yj = PowerTransformer(method='yeo-johnson')
bc = PowerTransformer(method='box-cox', standardize=False)

data_log = np.random.RandomState(616).lognormal(size=(3,3))
yj.fit_transform(data_log)  # вернет новое представление данных
\end{lstlisting}

\remark{%
Преобразование Бокса-Кокса требует, чтобы значения набора данных были строго положительными, в то время как преобразование Йео-Джонсона может работать как с положительными, так и с отрицательными значениями
}


\section{Кодирование признаков}

Полезная статья про кодировщики и различные стратегии валидации \url{https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8}.

Можно использовать готовые алгоритмы кодирования, реализованные в библиотеке\\ \texttt{category\_encoders} \url{https://pypi.org/project/category-encoders/}\footnote{Установить можно как обычно: \texttt{pip install category-encoders}}

Категориальные признаки можно разделить на \emph{порядковые признаки} (например, <<медлено>>, <<быстро>>, <<быстрее>> и т.д.) и \emph{номинальные признаки} (<<кошки>>, <<собаки>> и т.д.).

Основные приемы кодирования категориальных признаков:
\begin{itemize}
	\item для порядковых признаков
	\begin{itemize}
		\item \emph{пользовательское отображение}, которое учитывает определенный порядок (соотношение) категорий, например, \verb|{"cold" : 0, "warm" : 1, "hot" : 2}|: класс \texttt{LabelEncoder} следует применять только (!) для \emph{целевого вектора}, для которого порядок меток не имеет никакого значения; если выполнить кодировку порядкового признака с помощью \texttt{LabelEncoder}, то алгоритм будет предполагать, что между категориями существует порядковая зависимость, то есть их можно как-то отсортировать, а это неверно! \cite[151]{raschka:2019}
	\end{itemize}

    \item для именных признаков
    \begin{itemize}
    	\item \emph{частотное кодирование}: например, с помощью \texttt{category\_encoder.CountEncoder} (затем, разумеется, нужно число элементов в категории разделить на число строк в наборе данных); можно реализовать свой собственный кодировщик 
\begin{lstlisting}[
style = ironpython,
numbers = none
]
train_nominal_freq_enc = train_nominal.copy()
fq = train_nominal.groupby("col_nominal_name").size()/train_nominal.shape[0]
train_nominal_freq_enc["col_nominal_name"] =
    train_nominal_freq_enc["col_nominal_name"].map(fq)
\end{lstlisting}

    \item \emph{кодирование с одним активным состоянием} (еще говорят унитарное кодирование): легко реализовать с помощью \\ \texttt{sklearn.preprocessing.OneHotEncoder} или \\ с помощью \texttt{pd.get\_dummies(df, drop\_first=True)} (параметр \texttt{drop\_first=True} нужен для того, чтобы сократить взаимосвязь между признаками, полученными с помощью техники кодирования с одним активным состоянием, так как унитарное кодирование привносит мультиколлиниарность, что может приводить к появлению неустойчивых оценок \cite[153]{raschka:2019}),
    
    \item \emph{M-вероятностная оценка правдоподобия} (другие варианты: аддитивное сглаживание; кодирование сглаженным средним): можно использовать \\ класс \texttt{category\_encoders.m\_estimate.MEstimateEncoder}. Каждая категория категориального признака кодируются по следующей формуле
    \begin{align*}
        e_k = \dfrac{ s_k^t + m\, \dfrac{s^t}{n} }{c_k + m},\ (k = 1,\ldots, K),
    \end{align*}
    где $ s_k^t $ -- сумма значений целевого вектора для $ k $-ой категории; $ m $ -- степень регуляризации; $ n $ -- длина целевого вектора (число строк в наборе данных); $ c_k $ -- размер $ k $-ой категории (число экземпляров $ k $-ой категории); $ K $ -- число категорий (уникальных значений) категориального признака.
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
from category_encoders.m_estimate import MEstimateEncoder

# здесь m -- это степень регуляризации
me_enc = MEstimateEncoder(m=50)  # рекомендуемые значения для m = [1, 100]

data_mEst_enc = data.copy()
data_mEst_enc["col3"] = me_enc.fit_transform(
    data_mEst_enc.loc[:, "col3"],
    data_mEst_enc.loc[:, "target"]
)
\end{lstlisting}

    \item \emph{взвешенное суждение} (Weight of Evidence \url{https://contrib.scikit-learn.org/category_encoders/woe.html}): чаще всего применяется в кридитном скоринге.
    
    \item \emph{CatBoost-кодировщик}: можно использовать готовый класс \texttt{CatBoostEncoder} \url{https://contrib.scikit-learn.org/category_encoders/catboost.html}\\ из библиотеки \texttt{category\_encoders}; этот кодировщик предназначен для преодоления проблем утечки данных, присущих кодировщику \texttt{Leave-one-out Encoder} (LOO); чтобы предотвратить переобучение, процесс кодирования повторяется несколько раз для перетасованных версий набора данных, а результаты усредняются.
    \end{itemize}
\end{itemize}

Удобно использовать трансформеры столбцов
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import make_pipeline
from category_encoders.cat_boost import CatBoostEncoder

col_trans = make_column_transformer(
    (OneHotEncoder(sparse=False), ["col2", "col10"]),
    (CatBoostEncoder(), ["col20"])
)

gbm = GradientBoostingRegressor()
gbm_pipeline = make_pipeline(col_trans, gbm)

gbm_pipeline.fit(X_train, y_train)
gbm_pred = gbm_pipeline.predict(X_test)
\end{lstlisting}

Что касается стратегий валидации, то можно выделить 3 основные:
\begin{itemize}
	\item None Validation: очень грубо,
	
	\item Single Validation: что-то среднее между None Validation и Double Validation,
	
	\item Double Validation: очень медленно.
\end{itemize}

\vspace*{2mm}
Стратегия для одиночной валидации:
\begin{enumerate}
	\item Разбиваем имеющийся набор данных на обучение и тест,
	
	\item Обучающий набор данных разбиваем на несколько фолдов,
	
	\item Каждый фолд кодируем своим кодировщиком (т.е. кодировщиков будет столько, сколько было фолдов),
	
	\item На каждом закодированном фолде обучаем свою модель (моделей столько, сколько было фолдов),
	
	\item Кодируем тестовый набор данных теми же кодировщиками, которые мы использовали на этапе обучения (т.е. несколько представлений тестового набора данных),
	
	\item на каждом закодированном тестовом представлении делаем прогноз с помощью моделей, обученных на соответствующих закодированных данных обучающего набора,
	
	\item для получения финального прогноза усредняем прогнозы моделей.
\end{enumerate}




\section{Машинное обучение с AutoML}

На данный момент AutoML представленная следующими направлениями:
\begin{itemize}
	\item AutoML для автоматизации подбора гиперпараметров модели,
	
	\item AutoML для неглубокого обучения,
	
	\item AutoML для глубокого обучения.
\end{itemize}

Список наиболее полезных библиотек:
\begin{itemize}
	\item \texttt{featuretools} \url{https://featuretools.alteryx.com/en/stable/},
	
	\item \texttt{MLBox} \url{https://mlbox.readthedocs.io/en/latest/index.html},
	
	\item \texttt{TransmogrifAI} (Scala API) \url{https://github.com/salesforce/TransmogrifAI}.
\end{itemize}



\section{Хранилища данных. DWH}

Хранилище данных (Data WareHouse, DWH) -- предметно-ориентированная информационная база данных, специально разработанная и предназначенная для подготовки отчетов и безнес-анализа с целью поддержки принятия решений в организации. Строится на основе систем управления базами данных и систем поддержки принятия решений. Данные, поступающие в хранилище данных, как правило, доступны только для чтения.

\remark{%
DWH необходимо для проведения эффективного бизнес-анализа и построения выжных для бизнеса выводов
}

Данные из OLTP-систем копируются в хранилище данных таким образом, чтобы при построении отчетов и OLAP-анализе не использовать ресурсы транзакционной системы и не нарушалась ее стабильность.

В чем разница между обычными базами данных и хранилищем данных:
\begin{itemize}
	\item Обычные СУБД хранят данные строго для определенных подсистем (другими словами базы данных привязаны к своим приложениям). Например, база данных кадровиков хранит данные по персоналу, но не товары или сделки. DWH, как правило, \emph{хранит информацию разных подразделений} -- там найдутся данные и по товарам, и по персоналу, и по сделкам,
	
	\item Обычная база данных, которая ведется в рамках стандартной деятельности компании , содержит только актуальную информацию, нужную в данный момент времени для функционирования определенной системы. В DWH пишутся не столько копии актуальных состояний, сколько \emph{исторические данные} и \emph{агрегированные значения}. Например, состояние запасов разных категорий товаров на конец смены за последние пять лет. Иногда в DWH пишутся и более крупные пачки данных, если они имеют критическое значение для бизнеса -- например, полные данные по продажам и сделкам, то есть, по сути, это копия базы данных отдела продаж,
	
	\item Информация обычно сразу попадает в рабочие базы данных, а уже оттуда некоторые записи переползают в DWH. Склад данных, по сути, отражает состояние других баз данных и процессов в компании уже после того, как вносятся изменения в рабочих базах.
\end{itemize}

Короче говоря, DWH -- это система данных, отдельная от оперативной системы обработки данных. В корпоративных хранилищах в удобном для анализа виде хранятся архивные данные из разных, иногда очень разнородных источников. Эти данные предварительно обрабатываются и загружаются в хранилище в ходе процессов извлечения, преобразования и загрузки, называемых ETL.

Хранилище данных, кроме всего прочего, упрощает процедуру сбора данных из корпоративных СУБД:
\begin{itemize}
	\item Доступ к нужным данным. Если компания большая, на получение данных из разных источников нужно собирать разрешения и доступы. У каждого подразделения в такой ситуации, как правило, свои базы данных со своими паролями, которые надо будет запрашивать отдельно. В DWH все нужное будет под рукой в готовом виде. Можно просто сконстуировать запрос и вытащить нужную информацию,
	
	\item Сохранность нужных данных. Данные в DWH не теряются и хранятся в виде, удобном для принятия решений: есть исторические записи, есть агрегированные значения. В операционной базые данных такой информации может и не быть. Например, администраторы точно не будут хранить на складском сервере архив запасов за последние 10 лет -- БД склада была бы в таком случае слишком тяжелой. А вот хранить агрегированные запасы со склада в DWH -- это нормально,
	
	\item Устойчивость работы бизнес-систем. DWH оптимизируется для работы аналитиков, которые могут использовать сложные, тяжелые запросы к базе данных, способные повесить сервер с боевой базой данных, и вызвать проблемы в сопряженных системах.
\end{itemize}


Для задач, связанных с промышленным интернетом вещей (IIoT), данные с датчиков можно собирать в <<\emph{озеро данных}\footnote{Озеро данных -- хранилище, в котором собрана неструктурированная информация любых форматов из разных источников данных. Озера данных дешевле обычных баз данных, они более гибкие и легче масштабируются. Данные можно извлекать из озера по определенным признакам или анализировать прямо внутри озера, используя системы аналитики, но важно контролировать данные, поступающие в озеро данных}>> без фильтрации, а когда данных накопиться достаточно, можно будет их проанализировать и понять из-за чего случаются поломки. Озера данных нужны для гибкого анализа данных и построения гипотез. Они позволяют собирать как можно больше данных, чтобы потом с помощью инструментов машинного обучения и аналитики извлекать полезную для бизнеса информацию.



\section{Приемы работы с ETL-инструментом \texttt{Apache NiFi}}

<<Одиночный>> экземпляр \texttt{Apache NiFi} \url{https://nifi.apache.org/} можно создать с использованием Docker
\begin{lstlisting}[
style = bash,
numbers = none
]
docker run -d --name nifi -p 8080:8080 apache/nifi:latest
\end{lstlisting}

Экземпляр будет доступен через web-браузер по \verb|http://localhost:8080/nifi|.



\section{Приемы работы с библиотекой \texttt{Vowpal Wabbit}}

\texttt{Vowpal Wabbit} -- библиотека с открытым исходным кодом, ориентированная на крупно-масштабные онлайн\footnote{В компьютерных науках онлайновое машинное обучение (online machine learning \url{https://en.wikipedia.org/wiki/Online_machine_learning}) -- это метод машинного обучения, при котором данные становятся доступными не сразу, а постепенно в определенном порядке и используются для обновления наилучшего предиктора для будущих данных на каждом шаге. В то время как пакетное машинное обучение, возвращает прогноз на основе всего набора имеющихся данных. Онлайновое машинное обучение это распространенный способ, используемый там, где обучение по всему набору данных неосуществимо с точки зрения объема вычислений. Эта техника также используется в ситуациях, когда требуется, чтобы алгоритм динамически адаптировался к новым паттернам в данных}-задачи машинного обучения, в основе которых лежат так называемые алгоритмы, работающие во внешней памяти (их еще называют \emph{внеядерными} (out-of-core) алгоритмами). 

\texttt{Vowpal Wabbit}:
\begin{itemize}
	\item очень качественная реализация стохастического градиентного спуска для линейных моделей,
	
	\item считывает данные с диска по одному прецеденту и делает шаг только по нему, нет необходимости хранить выборку в памяти,
	
	\item может быть запущен на кластере,
	
	\item нормализация признаков, взвешивание объектов, адаптивный градиентный шаг,
	
	\item матричное разложение, тематическое моделирование, активное обучение, обучение с подкреплением,
	
	\item разнообразие методов оптимизации: сопряженные градиенты, квазиньютоновские методы (L-DBGS).
\end{itemize}


Внеядерные алгоритмы машинного обучения не требуют загрузки всех данных в память.

Пример. Пусть выборка записана в файле \texttt{train.txt}. Тогда
\begin{lstlisting}[
style = bash,
numbers = none
]
# обучение
vw -d train.txt --passes 10 -c -f model.vw
\end{lstlisting}
где

\texttt{-d filename} -- имя входного файла,

\verb|--passes n| -- количество проходов по выборке,

\verb|-c| -- включает кэширование, позволяет ускорить все проходы после первого,

\verb|-f filename| -- имя файла, в который сохраняется модель (здесь \texttt{f} от final).

\begin{lstlisting}[
style = bash,
numbers = none
]
# прогноз
vw -d test.txt -i model.vw -t -p predictions.txt
\end{lstlisting}
где

\texttt{-d filename} -- имя входного файла,

\texttt{-i filename} -- имя файла с моделью,

\texttt{-t} -- режим применения существующей модели (только тестирование),

\texttt{-p filename} -- имя файла с прогнозами.

Можно работать из-под Python
\begin{lstlisting}[
style = ironpython,
numbers = none
]
from vowpalwabbit import pyvw

model = pyvw.vw()

train_examples = [
    "0 | price:.23 sqft:.25 age:.05 2006",
    "1 | price:.18 sqft:.15 age:.35 1976",
    "0 | price:.53 sqft:.32 age:.87 1924",
]

for example in train_examples:
    model.learn(example)
    
test_example = "| price:.46 sqft:.4 age:.10 1924"

prediction = model.predict(test_example); prediction # 0.0
\end{lstlisting}


\section{Приемы работы с \texttt{Microsoft Machine Learning for Apache Spark}}

\href{https://github.com/Azure/mmlspark}{Microsoft Machine Learning for Apache Spark} (MMLSpark) -- это экосистема инструментов, расширяющих возможности вычислительной платформы Apache Spark в нескольких новых направлениях.

Установить MMLSpark можно разными способами (см.~\url{https://github.com/Azure/mmlspark}), например, так расширяется библиотека \texttt{pyspark} из-под Python
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import pyspark
spark = (pyspark.sql.SparkSession.builder.appName('test spark').
             config('spark.jars.packages', 'com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc2').
             config('spark.jars.repositories', 'https://mmlspark.azureedge.net/maven').
             getOrCreate()
import mmlspark
\end{lstlisting}

\href{https://github.com/Azure/mmlspark/blob/master/notebooks/samples/HyperParameterTuning%20-%20Fighting%20Breast%20Cancer.ipynb}{Пример} использования библиотеки MMLSpark для решения задачи гиперпараметрической оптимизации
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import pandas as pd
data = spark.read.parquet("wasbs://publicwasb@mmlspark.blob.core.windows.net/BreastCancer.parquet")
tune, test = data.randomSplit([0.80, 0.20])
tune.limit(10).toPandas()


from mmlspark.automl import TuneHyperparameters
from mmlspark.train import TrainClassifier
from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier
logReg = LogisticRegression()
randForest = RandomForestClassifier()
gbt = GBTClassifier()
smlmodels = [logReg, randForest, gbt]
mmlmodels = [TrainClassifier(model=model, labelCol="Label") for model in smlmodels]

from mmlspark.automl import *

paramBuilder = \
    HyperparamBuilder() \
        .addHyperparam(logReg, logReg.regParam, RangeHyperParam(0.1, 0.3)) \
        .addHyperparam(randForest, randForest.numTrees, DiscreteHyperParam([5,10])) \
        .addHyperparam(randForest, randForest.maxDepth, DiscreteHyperParam([3,5])) \
        .addHyperparam(gbt, gbt.maxBins, RangeHyperParam(8,16)) \
        .addHyperparam(gbt, gbt.maxDepth, DiscreteHyperParam([3,5]))
searchSpace = paramBuilder.build()
# The search space is a list of params to tuples of estimator and hyperparam
print(searchSpace)
randomSpace = RandomSpace(searchSpace)


bestModel = TuneHyperparameters(
                evaluationMetric="accuracy", models=mmlmodels, numFolds=2,
                numRuns=len(mmlmodels) * 2, parallelism=1,
                paramSpace=randomSpace.space(), seed=0).fit(tune)

print(bestModel.getBestModelInfo())
print(bestModel.getBestModel())

from mmlspark.train import ComputeModelStatistics
prediction = bestModel.transform(test)
metrics = ComputeModelStatistics().transform(prediction)
metrics.limit(10).toPandas()
\end{lstlisting}


\section{Приемы работы с библиотекой \texttt{BeautifulSoup}}

\subsection{Пример использования \texttt{BeautifulSoup} для скрапинга сайта}

В качестве простого примера извлечем имена руководителей компаний из группы компаний оборонного комплекса. Имена нужных тегов удобно искать с помощью специальных инструментов разработчика, доступных в веб-браузере. Например, в Yandex-браузере получить доступ к панели разработчика можно так \menu{Настройки > Дополнительно > Дополнительные инструменты > Инструменты разработчика}.
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import requests
import pandas as pd
import psycopg2
from pprint import pprint
from bs4 import BeautifulSoup
from pandas import DataFrame, Series

main_url = 'http://ros-oborona.ru/koncerny.html'
res = requests.get(main_url)
soup = BeautifulSoup(res.text, features='lxml')

company_list = soup.find('div',
                         {'class' : 'elementor-text-editor elementor-clearfix'})
profile_list = company_list.find_all('td')

href_list = []
for elem in profile_list:
    try:
        href_list.append(elem.find('a').get('href'))
    except AttributeError:
        continue

heads_of_company_list = []
for company_url in href_list:
    res_elem = requests.get(company_url)
    soup_elem = BeautifulSoup(res_elem.text, features='lxml')
    head_of_company = soup_elem.find('span',
                                     {'class' : 'company-info__text'}).text
    if len(head_of_company.split()) == 3:
        heads_of_company_list.append(head_of_company.split())

heads_of_company_df = DataFrame(heads_of_company_list,
                                columns=['lastname', 'firstname', 'middlename'])
heads_of_company_df.index.name = 'id'
heads_of_company_df.to_csv('heads_of_company.csv', index=True)

# -- PostgreSQL
conn = psycopg2.connect('dbname=postgres user=postgres password=evdimonia')
cursor = conn.cursor()

heads_df = pd.read_csv('heads_of_company.csv')
heads_records = heads_df.to_dict('records')

cursor.execute(
            '''CREATE TABLE IF NOT EXISTS heads_of_company(
                   id integer primary key,
                   lastname text not null,
                   firstname text not null,
                   middlename text not null)'''
)
cursor.executemany(
            '''INSERT INTO heads_of_company(id, lastname, firstname, middlename)
                   VALUES (%(id)s, %(lastname)s, %(firstname)s, %(middlename)s)
                   ON CONFLICT DO NOTHING''', heads_records
)
conn.commit()

cursor.execute('SELECT * FROM heads_of_company')
fetchall = cursor.fetchall()
pprint(fetchall)
# выведет
# [(0, 'Мясников', 'Александр', 'Алексеевич'),
# (1, 'Медовщук', 'Ирина', 'Сергеевна'),
# (2, 'Матыцын', 'Александр', 'Петрович'),
# (3, 'Смирнова', 'Оксана', 'Константиновна'),
# ...]
\end{lstlisting}

\section{Приемы работы с библиотекой \texttt{pandas}}

\subsection{Определить число уникальных значений в каждом категориальном признаке}

Для того чтобы выяснить число уникальных значений в каждом категориальном признаке заданного набора данных следует воспользоваться конструкцией
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
df = ...
df.select_dtypes("object").nunique()
\end{lstlisting}

\subsection{Срезы в мультииндексах}

Для того чтобы выбрать определенные элементы индекса определенного уровня, нужно воспользоваться констукцией
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
df = ...
df.xs("XOne", level="Platform")
\end{lstlisting}



\subsection{Число уникальных значений категориальных признаков в объекте \texttt{DataFrame}}

Для того чтобы вывести информацию по числу уникальных значений в каждом категориальном признаке некоторого объекта \texttt{pandas.DataFrame} можно воспользоваться конструкцией

\begin{lstlisting}[
style = ironpython,
numbers = none
]
X.select_dtypes('object').apply(lambda col: col.unique().shape[0])
X.select_dtypes('object').apply(lambda col: col.unique().size)
X.select_dtypes('object').nunique().values[0]
\end{lstlisting}

\subsection{Прочитать файл, распарсить временную метку, назначить временную метку индексом}

Иногда случается, что столбец в обрабатываемом файле, имеющий смысл временной метки, не приведен к нужному формату и поэтому простое чтение файла средствами \texttt{pandas} не помогает. Чтобы правильно распарсить столбец с временной меткой следует сделать так
\begin{lstlisting}[
style = ironpython,
numbers = none
]
#! cat test_file.csv
# date, stress
# 2020/08/18, 100
# 2020/08/19, 200

>>> import pandas as pd
>>> data = pd.read_csv('test_file.csv', index_col='date', parse_date=True)
>>> type(data.index[0]) # pandas._libs.tslibs.timestamps.Timestamp
\end{lstlisting}

\subsection{Число пропущенных значений в объекте \texttt{DataFrame}}

Информацию по числу пропущенных значений в каждом столбце можно вывести следующим образом

\begin{lstlisting}[
style = ironpython,
numbers = none
]
X.isna().any(axis=0)
\end{lstlisting}

\subsection{Управление стилями объекта \texttt{DataFrame}}

У объектов \texttt{DataFrame} есть стили и ими можно управлять, выделяя максимальные/минимальные значения в таблицы, значения, которые удовлетворяют какому-то специфическому условию и пр. Однако, эти приемы работают только в notebook'ах

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import pandas as pd
import numpy as np
from pandas import DataFrame, Series

# определяем объект-DataFrame
m, n = 10, 4
df = DataFrame(np.random.randn(m, n),
               columns=[f'col{i}' for i in range(1, n+1)])
df.loc[[4, 6, 9], ['col1', 'col4']] = np.nan
\end{lstlisting}

\begin{lstlisting}[
style = ironpython,
emph = {threshold_color, background_color_max, backgournd_color_min},
numbers = none
]
from typing import List, TypeVar

# это способ обойти ограничения аннотаций для объектов pandas
ElemOfDataframe = TypeVar('DataFrame.iloc[int, int]')

# определяем функции для управления стилями объекта-DataFrame
def threshold_color(val: ElemOfDataframe) -> str:
    '''
    Значения большие 0.5, но меньшие 1.0 выделяет красным;
    Отрицательные значения выделяет синим;
    Все прочие значения печатаются черным
    '''
    return 'color : {}'.format('red' if ((val > 0.5) and (val < 1.0)) else
                               'blue' if val < 0. else 'black')

def background_color_max(col: Series) -> List[str]:
    '''
    Фон максимальных значений в столбце выделяется желтым.
    '''
    mask = col == col.max()  # булева маска
    return ['background-color : yellow' if bool_elem else '' for bool_elem in mask]

def background_color_min(col: Series) -> List[str]:
    '''
    Фон максимальных значений в столбце выделяется светло-зеленым.
    '''
    mask = col == col.min()  # булева маска
    return ['background-color : lightgreen' if bool_elem else '' for bool_elem in mask]
\end{lstlisting}

Работа со стилями объекта-DataFrame в ячейке выглядит следующим образом

\begin{lstlisting}[
style = ironpython,
numbers = none
]
(  # скобки здесь нужны для переноса строки без символа `\`
    df.style.
        applymap(threshold_color).
        apply(background_color_max).
        apply(background_color_min).
        format(
            {  # можно применять разные спецификаторы формата к разным столбцам
                'col2' : '{:.5e}',
                'col4' : '{:.3G}'
            }
        )
)
\end{lstlisting}

Результат будет выглядеть как на \pic{fig:format-df}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{figures/format_df.png}
	\caption{ Отформатированный вывод \texttt{DataFrame} }\label{fig:format-df}
\end{figure}

Еще одно очень полезное применение этого приема: можно раскрашивать наиболее частые значения категориального признака

\begin{lstlisting}[
style = ironpython,
emph = {color_code_freq_cat},
numbers = none
]
from typing import List

def color_code_freq_cat(col: Series) -> List[str]:
    '''
    Раскрашивает самые частые значения категориальных столбцов
    '''
    # принимает столбец-Series `col`
    freq_cat = col.value_counts().index[0]  # самое частое значение категории
    return ['color : {}'.format('red' if elem == freq_cat else 'black') for elem in col]

df = DataFrame({'col1' : list('abbbabbaaab'),
                'col2' : list('cdcccddcscd'),
                'col3' : np.random.randn(11)})

# apply работает со столбцами или строками
df_test.iloc[:5].select_dtypes('object').style.apply(color_code_freq_cat)
\end{lstlisting}

Результат приведен на \pic{fig:color-code-freq-cat}. Вывести самое частое значение в каждом столбце можно с помощью конструкции

\begin{lstlisting}[
style = ironpython,
numbers = none
]
# apply работает со столбцами или строками
df.apply(lambda col: col.value_counts().index[0])
\end{lstlisting}

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{figures/color_code_freq_cat.png}
	\caption{ Результат применения функции \texttt{color\_code\_freq\_cat} }\label{fig:color-code-freq-cat}
\end{figure}


\subsection{Заполнить пропущенные значения средними по группе}

Часто возникает задача заполнить отсутствующие значения групповыми средними. Сделать это можно с помощью метода \texttt{transform}
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
import pandas as pd
from pandas import DataFrame, Series

seed = 123
df = DataFrame({
	'package_name' : np.array(['Ansys',
	'Nastran',
	'Comsol',
	'Abaqus'])[np.random.RandomState(seed).randint(4, size=5)],
	'comp_effect' : np.abs(100*np.random.RandomState(seed).randn(5))
})
data.iloc[[1, 3], 1] = np.nan
# df =
#  package_name  comp_effect
#0       Comsol   108.563060
#1      Nastran          NaN
#2       Comsol    28.297850
#3       Comsol          NaN
#4        Ansys    57.860025

# следует указывать столбец, по которому будет выполняться группировка
# и столбец, для которого будут вычисляться средние по группе
data['comp_effect'] = (
    data[['package_name', 'comp_effect']].groupby('package_name').
        transform(lambda g: g.fillna(g.mean()))
)
\end{lstlisting}

Для сравнения эту же задачу можно было бы решить с помощью более общего метода \texttt{apply}, но код будет значительно сложнее
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
...
# подавить создание иерархического индекса можно с помощью `group_keys`
data['comp_effect'] = (
    data[['package_name', 'comp_effect']].groupby('key2', group_keys=False).
        apply(lambda g: g.fillna(g.mean())).
            drop(['key2'], axis=1).  # удалить столбец группировки
                sort_index()         # отсортировать
)
\end{lstlisting}
или так
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
...
# в apply можно указать интересующий столбец 
data['comp_effect'] = (
    data.groupby('key2', group_keys=False).
        apply(lambda g: g['key1'].fillna(g['key1'].mean())).
            sort_index()
)
\end{lstlisting}

\section{Приемы работы с библиотекой \texttt{Plotly}}

Рассмотрим простой пример работы с библиотекой \texttt{plotly} в блокноте
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import numpy as np
import chart_studio.plotly as py
import plotly.graph_objs as go
from plotly.offline import (
    download_plotlyjs,
    init_notebook_mode,
    plot,
    iplot
)
init_notebook_mode(connected=True)

# в текущей директории будет создан html-файл, а график откроется в браузере
plot(go.Figure(data=[
    go.Scatter(y=np.random.randn(100).cumsum()),
    go.Scatter(y=np.random.randn(100).cumsum())
]), filename='file_name.html')
\end{lstlisting}

\section{Максимальный информационный коэффициент}

Как известно, коэффициент корреляции Пирсона оценивает тесноту \emph{только} линейной связи. Таким образом, даже если коэффициент Пирсона близок к нулю, что говорит об отсутствии линейной связи между переменными, эти переменные все равно могут быть связаны, например, \emph{нелинейной} зависимостью.

По этой причине был предложен \emph{максимальный информационный коэффициент} (MIC). Формально MIC эквивалентен коэффициенту детерминации $ R^2 $ и принимает значения от 0 (статистическая независимость) до 1 (бесшумная функциональная связь)
\begin{align*}
	MIC(x, y) = \max\limits_{x,y < B}\ \dfrac{I(x, y)}{\log_2 \min (x, y)}.
\end{align*}

В числителе стоит \emph{взаимная информация} между переменными $ x $ и $ y $
$$
    I(x, y) = \sum_{x,y}p(x,y) \log_2 \dfrac{p(x,y)}{p(x)p(y)},
$$
где $ p(x,y) $ -- доля данных, попавших в ячейку $ x, y $, т.е. это совместное распределение $ x $ и $ y $.

Взаимная информация делится на логарифм от наименьшего числа ячеек $ x $ или $ y $, а $ B $ это в некотором смысле произвольное число ячеек.

MIC можно интерпретировать как процент одной переменной, который может быть объяснен с помощью другой переменной. MIC присваивает одну и ту же оценку одинаково шумным связям, не зависимо от типа связи.

Достоинства MIC:
\begin{itemize}
	\item умеет выявлять широкий спектр линейных и нелинейных зависимостей (кубические, экспоненциальные, синусоидальные, суперпозиции и пр.),
	
	\item симметричный, потому что основан на взаимной информации,
	
	\item Не требует предположений относительно распределения переменных,
	
	\item Устойчив к выбросам.
\end{itemize}

Есть реализация этого коэффициента в библиотеке \texttt{mictools} (\verb|pip install mictools|).




\section{Интерпретация моделей и оценка важности признаков с библиотекой \texttt{SHAP}}

\subsection{Общие сведения о значениях Шепли}

В библиотеке \texttt{SHAP} \url{https://github.com/slundberg/shap} для оценки \emph{важности признаков} используются \emph{значения Шепли}\footnote{Термин пришел из теории кооперативных игр} (Shapley value) \url{https://en.wikipedia.org/wiki/Shapley_value}.

Или несколько точнее: при построении \emph{локальной} интерпретации (то есть интерпретации на конкретной точке данных) значения Шепли, строго говоря, оценивают \emph{силу влияния}\footnote{Еще эту оценку можно интерпретировать как \emph{вклад}} $ i $-ого признака $ f_i $ на значения целевого вектора $ y $, а вот \emph{важность признака} в контексте модели можно оценить при построении \emph{глобальной} интерпретации с помощью значений Шепли, взятых по абсолютной величине и усредненных по имеющемуся набору данных. 

\remark{
Значения Шепли объясняют как <<справедливо>> оценить вклад каждого признака в прогноз модели
}

Значения Шепли $i$-ого признака на \emph{конкретном объекте} (на текущей точке данных) вычисляются следующим образом (здесь сумма распространяется на все подмножества признаков $ S $ из множества признаков $ N $, не содержащие $ i $-ого признака)
\begin{align*}
\phi_i(v) = \sum_{S \subseteq N\setminus\{i\}} \dfrac{|S|!(n - |S| - 1)!}{n!}\Bigl( \underset{f_i-contribution }{\underbrace{ v(S \cup \{i\}) - v(S)}} \Bigr),
\end{align*}
где $ n $ -- общее число признаков; $ v(S \cup \{i\}) $ -- прогноз модели с учетом $ i $-ого признака; $ v(S) $ -- прогноз модели без $ i $-ого признака.

Выражение $ v(S \cup \{i\}) - v(S) $ -- это вклад $ i $-ого признака. Если теперь вычислить среднее вкладов по всем возможным перестановкам, то получится <<честная>> оценка вклада $ i $-ого признака.

Значение Шепли для $ i $-ого признака вычисляется для каждой точки данных (например, для каждого клиента в выборке) на всех возможных комбинациях признаков (в том числе и для пустых подмножеств $ S $).

\remark{
Метод анализа важности признаков, реализованный в библиотеке \texttt{SHAP}, является и \emph{согласованным}, и \emph{точным} (см. \href{https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27}{Interpretable Machine Learning with XGBoost})
}



\subsection{ Пример построения локальной и глобальной интерпретаций }

Примеры использования библиотеки \texttt{SHAP} не только для tree-base моделей можно найти по адресу \url{https://github.com/slundberg/shap/tree/master/notebooks/tree_explainer}.

Решается задача регрессии для классического набора данных \texttt{boston}. Требуется предсказать стоимость квартиры. 

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import shap
import os
import pandas as pd
import numpy as np
from pandas import DataFrame, Series
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston
#%matplotlib inline  # если код оформляется в JupyterLab
#shap.initjs()  # если код оформляется в JupyterLab

boston = load_boston()
X, y = boston['data'], boston['target']  # numpy-массивы

# объекты pandas
X_full = DataFrame(X, columns=boston['feature_names'])
y_full = Series(y, name = 'PRICE')

X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, random_state=42)

rf = RandomForestRegressor(n_estimators=500).fit(X_train, y_train)

explainer = shap.TreeExplainer(rf)  # <- NB
shap_values_train = explainer.shap_values(X_train)  # <- NB
\end{lstlisting}

\subsubsection{Локальная интерпретация отдельной точки данных обучающего набора}

Теперь можно построить локальную интерпретацию для одной точки данных из обучающего набора (см. \pic{fig:shap_force_plt_train})

\begin{lstlisting}[
style = ironpython,
title = {\sffamily К вопросу о локальной интерпретации отдельной точки данных обучающего набора},
numbers = none
]
row = 1
shap.force_plot(
    explainer.expected_value,  # ожидаемое значение
    shap_values_train[row, :],  # 2-ая строка в матрице значений Шепли
    X_train.iloc[row, :]  # 2-ая строка в обучающем наборе данных
)
\end{lstlisting}

Можно считать, что \texttt{explainer.expected\_value} это значение, полученное усреднением целевого вектора по точкам обучающего набора данных, т.е. \texttt{y\_train.mean()}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.80]{figures/shap_force_plt_train.png}
	\caption{ Локальная интерпретация для одной точки данных обучающего набора }\label{fig:shap_force_plt_train}
\end{figure}

Еще можно построить график частичной зависимости (\pic{fig:shap_dependence_plt_train})

\begin{lstlisting}[
style = ironpython,
numbers = none
]
shap.dependence_plot('LSTAT', shap_values, X_train)
\end{lstlisting}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.85]{figures/shap_dependence_plt_train.png}
	\caption{ График частичной зависимости признака \texttt{LSTAT} от значений Шепли\\с учетом влияния признака \texttt{CRIM} }\label{fig:shap_dependence_plt_train}
\end{figure}

\subsubsection{Локальная интерпретация отдельной точки данных тестового набора}

Прежде чем приступить к вычислению значений Шепли, следует создать поверхностную копию тестового набора данных

\begin{lstlisting}[
style = ironpython,
numbers = none
]
X_test_for_pred = X_test.copy()
X_test_for_pred['predict'] = np.round(rf.predict(X_test), 2)

explainer = shap.TreeExplainer(rf)
# вычисляем значения Шепли для тестового набора данных со столбцом 'predict'
shap_values_test = explainer.shap_values(X_test_for_pred)
\end{lstlisting}

Теперь можно построить локальную интерпретацию для отдельной точки данных тестового набора (\pic{fig:shap_dependence_plt_test}).

\begin{figure}
	\centering
	\includegraphics[scale=0.7]{figures/shap_dependence_plt_test.png}
	\caption{ Локальная интерпретация для одной точки данных тестового набора }\label{fig:shap_dependence_plt_test}
\end{figure}

Из \pic{fig:shap_dependence_plt_test} видно, что признаки с различной <<силой>>\footnote{Ширина полосы}, которая определяется значениями Шепли, смещают предсказание модели на данной точке. Например, признак \texttt{LSTAT} (процент населения с низким социальным статусом) в значительной степени \emph{повышает}\footnote{Потому что значение этого признака невелико; чем меньше процент населения с низким социальным статусом проживает в округе, тем выше стоимость квартиры} стоимость квартиры на \underline{данной точке} по отношению к базовому значению \texttt{base\_value}, а признак \texttt{RM} (среднее число комнат в жилом помещении) в значительной степени снижает.

\begin{lstlisting}[
style = ironpython,
title = {\sffamily К вопросу о локальной интерпретации отдельной точки данных тестового набора},
numbers = none
]
row = 3
shap.force_plot(
    explainer.expected_value,  # 22.879814248021106
    #y_train.mean()  # 22.907915567282323
    shap_values_test[row, :],
    X_test_for_pred.iloc[row, :]
)
\end{lstlisting}

\subsubsection{Глобальная интерпретация модели на тестовом наборе данных}

Удобно работать с диаграммой рассеяния \texttt{shap.summary\_plot} (\pic{fig:summary_plt_test}), на которой изображаются признаки в порядке убывания их важности, с одновременным указанием того, насколько сильно каждый из признаков влияет на целевую переменную.

\begin{lstlisting}[
style = ironpython,
numbers = none
]
shap.summary_plot(shap_values_test, X_test_for_pred)
\end{lstlisting}

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.85]{figures/summary_plt_test.png}
	\caption{ Диаграмма рассеяния для точек тестового набора данных }\label{fig:summary_plt_test}
\end{figure}

Какие выводы можно сделать из \pic{fig:summary_plt_test}:

\begin{itemize}
	\item Признаки \texttt{LSTAT}, \texttt{RM} и \texttt{CRIM} имеют высокую важность для модели в целом,
	
	\item Для признака \texttt{LSTAT} наблюдается отрицательная статистическая зависимость от целевой переменной, т.е. низкие значения этого признака отвечают высоким значениям целевой переменной (стоимости на квартиру),
	
	\item Для признака \texttt{RM} наблюдается положительная статистическая зависимость от целевой переменной: чем больше комнат в жилом помещении, тем выше стоимость квартиры.
\end{itemize}

Затем можно детальнее изучить графики частичной зависимости, построенные на тестовом наборе данных. Рассмотрим зависимость признака \texttt{CRIM} (уровень преступности в городе на душу населения) от значений Шепли, вычисленных для этого признака (\pic{fig:dependeces_plt_test_CRIM}).

\begin{lstlisting}[
style = ironpython,
numbers = none
]
shap.dependence_plot('CRIM', shap_values_test[:, :-1], X_test_pred.iloc[:, :-1])
\end{lstlisting}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.85]{figures/dependeces_plt_test_CRIM.png}
	\caption{ График частичной зависимости признака \texttt{CRIM} от значений Шепли\\с учетом влияния \texttt{LSTAT} }\label{fig:dependeces_plt_test_CRIM}
\end{figure}

Какие выводы можно сделать из \pic{fig:dependeces_plt_test_CRIM}:

\begin{itemize}
	\item Чем выше уровень преступности в городе, тем в большей степени снижается стоимость квартиры,
	
	\item Не везде, где проживает высокий процент населения с низким социальным статусом наблюдается высокий уровень преступности, однако в тех местах, где регистрируется высокий уровень преступности одновременно регистрируется и высокий процент населения с низким социальным статусом.
\end{itemize}


\section{Перестановочная важность признаков в библиотеке \texttt{eli5}}


Еще важность признаков можно оценивать с помощью так называемой \emph{перестановочной важности} (permutation importances) \url{https://www.kaggle.com/dansbecker/permutation-importance}.

Идея проста: нужно в заранее отведенном для исследования важности признаков наборе данных (валидационном наборе) перетасовать значения признака, влияние которого изучается на данной итерации, оставив остальные признаки (столбцы) и целевой вектор без изменения.

Признак считается <<важным>>, если метрики качества модели падают, и соответственно -- <<неважным>>, если перестановка не влияет на значения метрик. Перестановочная важность вычисляется после того как модель будет обучена.

\remark{
Перестановочная важность обладает свойством \emph{согласованности}, но не обладает свойством \emph{точности} \href{https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27}{Interpretable Machine Learning with XGBoost}
}

Рассмотрим задачу построения регрессионной модели на наборе данных \texttt{load\_boston}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import eli5
import pandas as pd
from eli5.sklearn import PermutationImportance
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston
from pandas import DataFrame, Series

boston = load_boston()

X_train, X_test, y_train, y_test = train_test_split(boston['data'],
                                                    boston['target'], 
                                                    random_state=2)

X_train_sub, X_valid, y_train_sub, y_valid = train_test_split(X_train,
                                                              y_train,
                                                              random_state=0)

# модель случайного леса, как обычно, обучается на обучающей выборке
rf = RandomForestRegressor(n_estimators=500).fit(X_train_sub, y_train_sub)

# модель перестановочной важности обучается на валидационном наборе данных
perm = PermutationImportance(rf, random_state=42).fit(X_valid, y_valid)

eli5.show_weights(perm, feature_names = boston['feature_names'])  # визуализирует перестановочные важности признаков
\end{lstlisting}

\section{Регулярные выражения в \texttt{Python}}

В языке \texttt{Python} есть несколько тонких особенностей, связанных с регулярными выражениями, а имеено с поведением жадных и нежадных квантификаторов. Рассмотрим пример с \emph{жадным} квантификатором
\begin{lstlisting}[
style = ironpython,
numbers = none
]
# python
import re
re.compile('y*(\d{1,3})').search('xy1234z').groups()[0]  # '123'
\end{lstlisting}

Аналогичный результат получается и в \texttt{PostgreSQL}
\begin{lstlisting}[
style = sql,
numbers = none
]
-- postgresql
select substring('xy1234z', 'y*(\d{1,3})'); -- '123'
\end{lstlisting}

Но если используется \emph{нежадный} квантификатор, то результаты будут различаться
\begin{lstlisting}[
style = ironpython,
numbers = none
]
# python
import re
re.compile('y*?(\d{1,3})').search('xy1234z').groups()[0]  # '123'
\end{lstlisting}

А вот в \texttt{PostgreSQL}
\begin{lstlisting}[
style = sql,
numbers = none
]
-- postgresql
select substring('xy1234z', 'y*?(\d{1,3})'); -- '1'
\end{lstlisting}

Совпадать результаты будут только в том случае, если в регулярном выражении \texttt{Python} специально указать, что \verb|{m,n}| должен быть нежадным, т.е. \verb|{m,n}?|
\begin{lstlisting}[
style = ironpython,
numbers = none
]
# python
import re
re.compile('y*?(\d{1,3}?)').search('xy1234z').groups()[0]  # '1'
\end{lstlisting}

\section{Неравенство Маркова}

\emph{Неравенство Маркова} в теории вероятностей дает оценку вероятности, что неотрицательная случайная величина превзойдет по модулю фиксированную положительную константу, в терминах ее математического ожидания. Хотя получаемая оценка обычно груба, она позволяет получить определенное представление о распределении, когда последнее не известно явным образом.

Пусть неотрицательная случайная величина $ X: \Omega \to \mathbb{R}^+ $ определена на вероятностном пространстве $ (\Omega, F, \mathbb{P}) $, и ее математическое ожидание $ \mathbb{E}X $ конечно. Тогда
$$
\mathbf{P}(X \geqslant a) \leqslant \dfrac{\mathbb{E}X}{a}, \, a > 0.
$$

Пример. Пусть в среднем ученики опаздывают на 3 минуты (оценка математического ожидания), и нас интересует какова вероятность того, что ученик опоздает на 15 и более минут. Чтобы получить грубую оценку сверху
$$
\mathbf{P}(|X| \geqslant 15) \leqslant \dfrac{3}{15} = 0,2.
$$


\section{Асинхронное программирование в \texttt{Python}}

\subsection{Библиотека \texttt{aiomisc}}

Кроме библиотек \texttt{asyncio}, \texttt{asyncpg} и пр. есть еще одна очень полезная библиотека \texttt{aiomisc} \url{https://github.com/aiokitchen/aiomisc}. В числе прочего там есть такой полезный сервис как \texttt{MemoryTracer}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import asyncio
import os
from aiomisc import entrypoint
from aiomisc.service import MemoryTracer

async def main():
    leaking = []
    
    while True:
        leaking.append(os.urandom(128))
        await asyncio.sleep(0)
        
with entrypoint(MemoryTracer(interval=1, top_results=5)) as loop:
    loop.run_until_complete(main())
\end{lstlisting}

Еще один готовый сервис помогает профилировать приложение

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import asyncio
import os
import time
from aiomisc import entrypoint
from aiomisc.service import Profiler


async def main():
    for i in range(100):
        time.sleep(0.01) # синхронная функция в асинхронном коде! Плохо


with entrypoint(Profiler(interval=0.1, top_results=5)) as loop:
    loop.run_until_complete(main())
\end{lstlisting}

Есть некоторые вещи, которые нельзя реализовать в асинхронном коде, поэтому приходится пользоваться \emph{потоками}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> import asyncio, time, aiomisc

>>> @aiomisc.threaded
    def blocking_function():
        ''' Блокирующая функция. '''
        time.sleep(1)

# НО производительность решения будет зависеть от пула потоков.
# Если в пуле, скажем, 8 потоков, то значит одновременно смогут
# выполниться только 8 запросов, а остальные запросы будут
# вставать в очередь
>>> async def main():
        ''' Функции будут выполняться парарллельно. '''
        await asyncio.gather( # в двух потоках
            blocking_function(),
            blocking_function()
        )
    
    %%timeit # 1.02 s +/- 3.82 ms per loop (mean +/- std. dev. of 7 runs, 1 loop each)
    with aiomisc.entrypoint() as loop:
        loop.run_until_complete(main())
\end{lstlisting}

Еще удобно работать с генераторами
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import aiomisc
import asyncio


@aiomisc.threaded_iterable(max_size=100)
def blocking_reader():
    ''' Синхронный генератор. '''
    with open('/dev/urandom', 'r+') as fp:
        mdt_hash = hashlib.md5()
        while True:
            md5_hash.update(fp.read(32))
            yield md5_hash.hexdigest().encode()


async def main():
    reader, writer = await asyncio.open_connection('127.0.0.1', 2233)
    async with blocking_reader() as gen: # асинхронный менеджер контекста нужен обязательно!!!
        async for line, digest in gen:
            writer.write(digest)
            writer.write(b'\t')
            writer.write(line)


with aiomisc.entrypoint() as loop:
    loop.run_until_complete(main())
\end{lstlisting}

Можно создать \emph{отдельный} поток, который <<умирает>> вместе с декорируемой функцией (например, можно писать логи в отдельном потоке)
\begin{lstlisting}[
style = ironpython,
numbers = none
]
queue = Queue(max_size=100)

@aiomisc.threaded_separate
def blocking_reader(fname):
    with open('/dev/urandom', 'r+') as fp:
        while True:
            mdt_hash.update(fp.read(32))
            queue.put(md5_hash.hexdigest().encode())
\end{lstlisting}

Декораторы \verb|@threaded| делают из обычной функции \emph{awaitable-объект}. Нельзя выполнить функцию, обернутую \verb|@threaded| из функции, обернутой \verb|@threaded|.

\section{Приемы работы с DVC}

С основами использования инструмента DVC можно познакомиться по статье \url{https://proglib.io/p/git-dlya-data-science-kontrol-versiy-modeley-i-datasetov-s-pomoshchyu-dvc-2020-12-02}.

\section{Работа с базами данных в \texttt{Python}}

Для работы с \texttt{PostgreSQL} из-под \texttt{Python}, как правило, используется драйвер \texttt{psycopg2}. Можно использовать еще и \texttt{sqlalchemy}. Согласно спецификации DB-API 2.0, после создания объекта соединения необходимо создать объект-курсор. Все дальнейшие запросы должны производиться через этот объект.

Пример
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import psycopg2
import sqlalchemy

# PostgreSQL
conn_pg = psycopg2.connect('postgesql://postgres@localhost:5432/demo')
cur_pg = conn_pg.cursor()
# возвращает название источника данных в формате строки
conn_pg.dsn # 'postgesql://postgres@localhost:5432/demo'
conn_pg.get_dsn_parameters()
#{'user': 'postgres',
# 'passfile': 'C:\\Users\\ADM\\AppData\\Roaming/postgresql/pgpass.conf',
# 'dbname': 'demo',
# 'host': 'localhost',
# 'port': '5432',
# 'tty': '',
# 'options': '',
# 'sslmode': 'prefer',
# 'sslcompression': '0',
# 'krbsrvname': 'postgres',
# 'target_session_attrs': 'any'}

# вывести элементы из столбца `kv` из таблицы `test_hstore`, хранящей пары <<ключ-значение>>,
# и выбрать те строки, в которых содержится ключ `solver type`
cur_pg.execute('''
    SELECT kv->'solver type' FROM test_hstore WHERE kv ? 'solver type'
''')
cur_pg.fetchall()  # [('direct',), ('iterative',)]

# SQLAlchemy
engine_sql = sqlalchemy.create_engine('postgresql://postgres@localhost:5432')
conn_sql = engine_sql.connect()
conn_sql.execute('''
    SELECT kv->'solver type' FROM test_hstore WHERE kv ? 'solver type'
''').fetchall()  # [('direct',), ('iterative',)]
\end{lstlisting}

Еще чтобы не беспокоиться на счет статуса объекта-курсора и соединения можно пользоваться менеджером контекста
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import psycopg2

with psycopg2.connect('postgresql://postgres@localhost:5432/demo') as conn: # соединение
    with conn.cursor() as cur: # объект-курсора
        cur.execute('select * from tickets limit %(lmt)s;', {'lmt' : 5})  # даже если передается объект целочисленного типа следует использовать %()s!!!
        res = cur.fetchall()
        
        for row in res:
            print(row)
\end{lstlisting}

Библиотека \texttt{asyncpg} \url{https://github.com/MagicStack/asyncpg} используется когда требуется реализовать \emph{асинхронную} работу с базой данной PostgreSQL. Устанавливается библиотека как обычно c помощью менеджера пакетов \texttt{pip}: \texttt{pip install asyncpg}.

Библиотека \texttt{asyncpg} не реализует Python DB-API, так как DB-API это синхронный интерфейс программного приложения, а \texttt{asyncpg} построена вокруг асинхронной I/O-модели.

В библиотеке \texttt{psycopg2} метод \texttt{cursor.execute()} \emph{блокирует} программу на все время выполнения запроса. Если запрос сложный, то программа будет заблокирована надолго, что не всегда желательно. Это означает, что пока запрос выполняется, программа может заниматься другими делами.

Библиотека \texttt{asyncpg} предоставляет асинхронный API, предназначенный для работы совместно с \texttt{asyncio} -- библиотекой, используемой для написания конкурентного кода на Python.

Замечания: ключевое слово \texttt{async} означает, что определенная далее функция является сопрограммой, т.е. асинхронна и должна выполняться особым образом, а ключевое слово \texttt{await}\footnote{Запускает сопрограмму из асинхронного кода с явным переключением контекста} служит для синхронного выполнения сопрограмм.

По рекомендациям разработчиков \texttt{asyncio}
\begin{itemize}
	\item следует использовать \texttt{asyncio.run()}, а цикл не нужен!,
	
	\item должна быть одно точка входа,
	
	\item следует использовать \texttt{async/await} везде,
	
	\item никогда не следует передавать ссылку на цикл
\end{itemize}

По рекомендациям разработчиков \texttt{asyncio} НЕ нужно использовать
\begin{itemize}
	\item декораторы \texttt{@coroutine},
	
	\item низкоуровневый API (\texttt{asyncio.Future}, \texttt{call\_soon()}, \texttt{call\_later()}, \texttt{event loop} etc)
\end{itemize}


Простой пример и сравнение
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import asyncio

# ------- old style
def get_text_oldschool():
    '''
    Вызов функции возвращает объект-генератор
    '''
    yield 'test string (old school)'
    
gto = get_text_oldschool() # переменная связывается с объектом-генератором
gto.__next__() # 'test string (old school)'

# ------- new style
async def get_text_coro():
    '''
    Вызов функции возвращает объект-сопрограмму
    '''
    return 'test string (new style)'
# запустить сопрограмму можно с помощью ключевого слова await
await get_text_coro() # 'test string (new style)'
\end{lstlisting}

Еще один вариант запуска сопрограмм с помощью \texttt{asyncio.run()} (запускает цикл событий)
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import asyncio

async def get_text(delay, text):
    await asyncio.sleep(delay)
    return text
    
async def say_text():
    ''' Здесь задачи будут выполняться парраллельно'''
    # создаем задачи и ставим их в очередь; они еще не выполняются
    task1 = asyncio.create_task(get_text(1, 'hello'))
    task2 = asyncio.create_task(get_text(1, 'world'))
    # явно переключаем контекст и выполняем сопрограммы
    await task1
    await task2
    return ', '.join([task1.result(), task2.result()])
    
result = asyncio.run(say_text()); result # 'hello, world'
\end{lstlisting}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
# задачи выполнятюсят парраллельно!!!
async def output(t):
    return await asyncio.sleep(t, 'test message')
    
async def main():
    tasks = [
        asyncio.create_task(output(n))
        for n in (2, 1, 3, 1)
    ]
    for task in asyncio.as_completed(tasks):
         print(await task)
\end{lstlisting}

Ограничить время ожидания awaitable-объекта можно так
\begin{lstlisting}[
style = ironpython,
numbers = none
]
async def eternity():  # сопрограмма
    try:
        await asyncio.sleep(3600)
    except asyncio.CancelledError:
        print('I was cancelled')
        raise # возбуждается исключение
    print('Finished')  # не печатается
    
async def main():  # сопрограмма
    try:
        await asyncio.wait_for(eternity(), timeout=1.0)
    except asyncio.TimeoutError:
        print('Timeout')
        
asyncio.run(main())
\end{lstlisting}

Подождать выполнения awaitable-объектов можно так
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import asyncio

async def delayed_res(delay):
    return await asyncio.sleep(delay, f'test string: {delay}')
    
async def main():
    tasks = [ # создаем задачи
        delayed_res(i)
        for i in range(1,10+1)
    ]
    for earliest in asyncio.as_completed(tasks):
        res = await earliest # выполняем сопрограмму
        print(res)
        
asyncio.run(main())
\end{lstlisting}

Можно запустить синхронный код в процессе/потоке
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import asyncio
import concurrent

async def main():
    loop = asyncio.get_running_loop()
    with concurrent.futures.ProcessPoolExecutor() as pool:
        res = await loop.run_in_executor(pool, cpu_bound)
        print('custom process pool', res)
        
asyncio.run(main())
\end{lstlisting}

Несколько задач сразу можно запустить так
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import asyncio

async def main():
    coros = (
        some_async_coro(i)
        for i in range(10)
    )
    results = await asyncio.gather(*coros) # << NB
    
asyncio.run(main())  # у приложения должна быть одна точка входа
\end{lstlisting}

В асинхронном программировании поддерживаются еще и \emph{асинхронные генераторы}, т.е. асинхронные функции, использующие ключевое слово \texttt{yield}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
async def ticker(delay, to):
    ''' Асинхронный генератор'''
    for i in range(to):
        yield i
        await asyncio.sleep(delay)
        
async def main():
    async for i in ticker(1,10):
        print(i)
        
asyncio.run(main())
\end{lstlisting}

В python 3.6+ поддерживаются все comprehensions
\begin{lstlisting}[
style = ironpython,
numbers = none
]
# их всех можно сочетать с for и if
{ i async for i in agen() } # множество
[ i async for i in agen() ] # список
{ i : i**2 async for i in agen() } # словарь
( i**2 async for i in agen()) # генераторное выражение
\end{lstlisting}

Пример использования асинхронного генератора
\begin{lstlisting}[
style = ironpython,
numbers = none
]
async def ticker(delay, to):
    ''' Асинхронный генератор'''
    for i in range(to):
        yield i # <-
        await asyncio.sleep(delay)
        
async def main():
    results = [ # асинхронный генератор списка с двумя циклами
        (i,j)
        async for i in ticker(0.1, 5)
            async for j in ticker(0.1, 5)
        if not i % 2 and j % 2
    ]
    print(results)
\end{lstlisting}

Пример использования
\begin{lstlisting}[
style = ironpython,
numbers = none
]
#import asyncio
>>> import asyncpg

>>> conn = await asyncpg.connect('postgresql://postgres@localhost:5432/demo')
>>> values = await conn.fetch('''
        SELECT passenger_name, count(*)
        FROM tickets
        GROUP BY 1
        ORDER BY 2 DESC
        LIMIT 5;
    ''')
>>> type(values[0]) # asyncpg.Record
>>> values
#[<Record passenger_name='ALEKSANDR IVANOV' cnt=842>,
# <Record passenger_name='ALEKSANDR KUZNECOV' cnt=755>,
# <Record passenger_name='SERGEY IVANOV' cnt=634>,
# <Record passenger_name='SERGEY KUZNECOV' cnt=569>,
# <Record passenger_name='VLADIMIR IVANOV' cnt=551>]

>>> res = await conn.fetch('''
              SELECT passenger_name, contact_data #>> '{phone}'::text[] AS phone
              FROM tickets
              LIMIT 3;
          ''')
>>> res
# [<Record passenger_name='VALERIY TIKHONOV' phone='+70127117011'>,
#  <Record passenger_name='EVGENIYA ALEKSEEVA' phone='+70378089255'>,
#  <Record passenger_name='ARTUR GERASIMOV' phone='+70760429203'>]
>>> res[0].get('phone') # '+70127117011'
>>> for k in res[1].keys():
        print(k)
# passenger_name
# phone
>>> for v in res[2].values():
        print(v)
# ARTUR GERASIMOV
# +70760429203
>>> for i, row in enumerate(res, 1): # обход строк выдачи
        print(f'{i}: ' +
               ','.join([f'/{k}/->{v}' for k,v in row.items()])
        )
>>> await conn.close()
\end{lstlisting}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import asyncio
import asyncpg
import datetime

async def main():
    # Establish a connection to an existing database named "test"
    # as a "postgres" user.
    conn = await asyncpg.connect('postgresql://postgres@localhost/test')
    # Execute a statement to create a new table.
     # `execute` если не нужно ничего возвращать
    await conn.execute('''
            CREATE TABLE users(
              id serial PRIMARY KEY,
              name text,
              dob date
            )
   ''')

    # Insert a record into the created table.
    await conn.execute('''
        INSERT INTO users(name, dob) VALUES($1, $2)
    ''', 'Bob', datetime.date(1984, 3, 1))

    # Select a row from the table.
    row = await conn.fetchrow(
        'SELECT * FROM users WHERE name = $1', 'Bob')
    # *row* now contains
    # asyncpg.Record(id=1, name='Bob', dob=datetime.date(1984, 3, 1))

    # Close the connection.
    await conn.close()

asyncio.get_event_loop().run_until_complete(main())
\end{lstlisting}

Иногда бывает удобно использовать предварительно подготовленные параметризованные SQL-запросы
\begin{lstlisting}[
style = ironpython,
numbers = none
]
# подготовленный параметризованный SQL-запрос
>>> cmpt_stmt = await conn.prepare('select 2^$1')
>>> cmpt_stmt # <asyncpg.prepared_stmt.PreparedStatement at 0xfc4fd68>
>>> res = await cmpt_stmt.fetchval(2); res # 4.0
>>> res await cmpt_stmt.fetchval(5); res # 32.0
\end{lstlisting}

Можно вывести план выполнения запроса
\begin{lstlisting}[
style = ironpython,
numbers = none
]
p = await cmpt_stmt.explain(5); p
# [{'Plan': {'Node Type': 'Result',
# 'Parallel Aware': False,
# 'Startup Cost': 0.0,
# 'Total Cost': 0.01,
# 'Plan Rows': 1,
# 'Plan Width': 8,
# 'Output': ["'32'::double precision"]}}]
p = await cmpt_stmt.explain(5, analyze=True); p
# [{'Plan': {'Node Type': 'Result',
# 'Parallel Aware': False,
# 'Startup Cost': 0.0,
# 'Total Cost': 0.01,
# 'Plan Rows': 1,
# 'Plan Width': 8,
# 'Actual Startup Time': 0.001,
# 'Actual Total Time': 0.001,
# 'Actual Rows': 1,
# 'Actual Loops': 1,
# 'Output': ["'1.0715086071862673e+301'::double precision"]},
# 'Planning Time': 0.065,
# 'Triggers': [],
# 'Execution Time': 0.026}]
\end{lstlisting}

Можно использовать \emph{транзакции}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> conn = await asyncpg.connect('...')
>>> async with conn.transaction():
        res = await conn.fetch('INSERT INTO tab VALUES (1, 2, 3)')
>>> res
\end{lstlisting}

Еще пример на транзакции
\begin{lstlisting}[
style = ironpython,
numbers = none
]
async with conn.transaction():
    res = await conn.fetch('''
       SELECT passenger_name, contact_data ->> 'phone' AS phone
       FROM tickets
       LIMIT $1 
    ''', 3)
print(res)
\end{lstlisting}

Библиотека \texttt{asyncpg} поддерживает асинхронное итерирование с помощью \texttt{async for}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
async def iterate(conn: Connection):
    async with conn.transaction():
        async for record in conn.cursor('SELECT generate_series(0, 100)'):
            print(record)
\end{lstlisting}



В случае когда используется связка \texttt{SQLAlchemy} и \texttt{asyncpg}, можно воспользоваться специальной библиотекой \texttt{asyncpgsa} \url{https://asyncpgsa.readthedocs.io/en/latest/}.

Для работы с аналитической СУБД \texttt{Vertica} есть своя библиотека \texttt{vertica\_python}\footnote{Устанавливается как обычно с помощью менеджера пакетов \texttt{pip}: \texttt{pip install vertica-python}} \url{https://github.com/vertica/vertica-python}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
imoprt vertica_python

conn_info = {
    'host' : '127.0.0.1',
    'port' : 5433,
    'user' : 'some_user',
    'password' : 'some_password',
    'database' : 'a_database',
    'kerberos_service_name' : 'vertica_krb',
    'kerberos_host_name' : 'vlcuster.example.com'
}

with vertica_python.conn(**conn_info) as conn:
    # do things
\end{lstlisting}

Вариант с баллансировкой нагрузки
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import vertica_python

conn_info = {
    'host' : '127.0.0.1',
    'port' : 5433,
    'user' : 'some_user',
    'password' : 'some_password',
    'database' : 'vdb',
    'connection_laod_balance' : True
}

# Server enables load balancing
with vertica_python.connect(**conn_info) as conn:
    cur = conn.cursor()
    cur.execute('SELECT NODE_NAME FROM V_MONITOR.CURRENT_SESSION')
    print('Client connects to primary node:', cur.fetchone()[0])
    cur.execute("SELECT SET_LOAD_BALANCE_POLICY('ROUNDROBIN')")
    
with vertica_python.connect(**conn_info) as conn:
    cur = conn.cursor()
    cur.execute('SELECT NODE_NAME FROM V_MONITOR.CURRENT_SESSION')
    print('Client redirects to node:', cur.fetchone()[0])
\end{lstlisting}

Доступ к колоночной аналитической СУБД \texttt{ClickHouse}, позволяющей выполнять аналитические запросы в режиме реального времени на структурированных больших данных, можно получить с помощью Python-библиотеки \texttt{clickhouse\_driver}\footnote{Устанавливается с помощью менеджера пакетов \texttt{pip}: \texttt{pip install clickhouse-driver}}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
# DP API example
from clickhouse_driver import connect

conn = connect('clickhouse://localhost')
cursor = conn.cursor()

cursor.execute('CREATE TABLE test(x Int32) ENGINE=Memory')
cursor.executemany(
    'INSERT INTO test(x) VALUES',
    [{'x' : 100}]
)
cursor.execute(
    'INSERT INTO test(x) '
    'SELECT * FROM system.numbers LIMIT %(limit)s',
    {'limit' : 3}
)
cursor.execute('SELECT sum(x) FROM test')
cursor.fetchall()  # [(303,)]
\end{lstlisting}

Также есть возможность управлять работой \emph{графовых} баз данных, например, Neo4j\footnote{Существует соответствующая python-библиотека \texttt{neo4j} \url{https://neo4j.com/developer/python/}. Используется собственные язык запросов Cypher, но поддерживается и Gremlin} \url{https://neo4j.com} с помощью, например, специального языка обхода графов \texttt{Gremlin} (есть альтернативы). Есть реализация Gremlin-Python \url{https://tinkerpop.apache.org/docs/current/reference/#gremlin-python} и соответствующая библиотека \verb|gremlin_python|\footnote{Устанавливается как обычно с помощью менеджера пакетов \texttt{pip}: \texttt{pip install gremlinpython}}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
from gremlin_python.process.anonymous_traversal_source import traversal

g = traversal().withRemote(
        DriverRemoteConnection('ws://localhost:8182/gremlin','g', headers={'Header' : 'Value'}))
\end{lstlisting}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
# классы, функции и токены, которые обычно используются с Gremlin
from gremlin_python import statics
from gremlin_python.process.anonymous_traversal import traversal
from gremlin_python.process.graph_traversal import __
from gremlin_python.process.strategies import *
from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection
from gremlin_python.process.traversal import T
from gremlin_python.process.traversal import Order
from gremlin_python.process.traversal import Cardinality
from gremlin_python.process.traversal import Column
from gremlin_python.process.traversal import Direction
from gremlin_python.process.traversal import Operator
from gremlin_python.process.traversal import P
from gremlin_python.process.traversal import Pop
from gremlin_python.process.traversal import Scope
from gremlin_python.process.traversal import Barrier
from gremlin_python.process.traversal import Bindings
from gremlin_python.process.traversal import WithOptions

...
\end{lstlisting}

Затем в консоли можно выполнить запрос
\begin{lstlisting}[
style = ironpython,
numbers = none
]
>>> g.V().hasLabel('person').has('age',P.gt(30)).order().by('age',Order.desc).toList()  # [v[6], v[4]]
\end{lstlisting}

Приемы базовой работы с Gremlin можно изучить в разделе документации \url{https://tinkerpop.apache.org/docs/current/reference/#basic-gremlin}
\begin{lstlisting}[
style = ironpython,
numbers = none
]
v1 = g.addV('person').property('name','marko').next()
v2 = g.addV('person').property('name','stephen').next()
g.V(Bindings.of('id',v1)).addE('knows').to(v2).property('weight',0.75).iterate()
\end{lstlisting}

Простыми словами, обход графа -- это переход от одной его вершины к другой в поисках свойств связей этих вершин. Связи (линии, соединяющие вершины) называются направлениями, путями, гранями или \emph{ребрами} графа. Вершины графа также называются \emph{узлами}.

Основными алгоритмами обхода графа являются:
\begin{itemize}
	\item поиск в глубину (depth-first search, DFS),
	
	\item поиск в ширину (breadth-first search, BFS).
\end{itemize}



\section{Особенности использования менеджера пакетов \texttt{pip}}

Чтобы установить пакет в редактируемом режиме в текущую директорию следует использовать флаг \verb*|-e|
\begin{lstlisting}[
style = bash,
numbers = none
]
pip install -e .
\end{lstlisting}

Это нужно, если требуется править исходники пакета, который устанавливается.


\section{Приемы работы с \texttt{flake8}}

\texttt{flake8} -- инструмент, позволяющий выявить стилистические ошибки в коде.

Установаить \texttt{flake8} можно как обычно
\begin{lstlisting}[
style = bash,
numbers = none	
]
pip install flake8
\end{lstlisting} 

Можно передать путь до рабочей директории или имя конкретного файла
\begin{lstlisting}[
style = bash,
numbers = none	
]
flake8 work_dir
flake8 file_name.py
\end{lstlisting}

Еще Flake8 поддерживает pre-commit хуки для Git и Mercurial. Эти хуки позволяют, например, не допускать создание коммита при нарушении каких-либо правил оформления. 

Установить хук для Git 
\begin{lstlisting}[
style = bash,
numbers = none	
]
flake8 --install-hook git
\end{lstlisting}

Настроить сам Git, чтобы он учитывал правила Flake8
\begin{lstlisting}[
style = bash,
numbers = none	
]
git config --bool flake8.strict true
\end{lstlisting}

Управлять поведением Flake8 можно с помощью конфигурационных файлов (\texttt{setup.cfg}, \texttt{tox.ini}, \texttt{.flake8}). Например
\begin{lstlisting}[
title = {\sffamily .flake8},
style = bash,
numbers = none	
]
[flake8]
  ignore =
    # F812: list comprehension redefines ...
    F812,
    D203
    # H101: Use TODO(NAME)
    H101,
    H301
  exclude = .git, __pycache__, build, old, dist, docs/conf.py
\end{lstlisting}


\section{Особенности работы с форматером \texttt{black}}

Black \url{https://pypi.org/project/black/} -- инструмент анализа и автоматического форматирования кода. Следит за файлами и форматирует только те файлы, которые были изменены. 

Можно запретить форматирование отдельных блоков кода с помощью \verb|# fmt: on| и \verb|fmt: off|, обзначающие начало и конец блока.

Установить Black можно так
\begin{lstlisting}[
style = bash,
numbers = none	
]
pip install black
\end{lstlisting}

Поведением Black удобно управлять с помощью конфигурационного toml-файла\footnote{TOML -- формат конфигурационных файлов}
\begin{lstlisting}[
title = {\sffamily pyproject.toml},
style = bash,
numbers = none	
]
[tool.black]
line-length = 80
target-version = ['py37']
include = '\.pyi?$'
exclude = '''
(
    \(
          \.eggs
        | \.git 
        | \.hg
        | \.mypy_cache
        | \.tox
        | \.venv
        | build
        | dist
    \)
    | foo.py
)
'''
\end{lstlisting}

Black будет искать файл \texttt{pyproject.toml}, начиная с текущего рабочего каталога и заканчивая корнем файловой системы, если придется.

Если вызвать \texttt{black} с флагом \verb|-v| (или \verb|--verbose|), то, при условии, что файл \texttt{pyproject.toml} найдется, в терминал будет выведен путь до конфигурационного файла.

Для работы с различными хуками удобно пользоваться специальным менеджером хуков \texttt{pre-commit} \url{https://pre-commit.com}. Нужно просто указать список хуков, а \texttt{pre-commit} будет запускать хук перед каждым коммитом.

Остается только добавить конфигурационный файл \texttt{.pre-commit-config.yaml} в рабочий локальный git-репозиторий
\begin{lstlisting}[
title  = {\sffamily .pre-commit-config.yaml},
style = bash,
numbers = none	
]
repos:
  - repo: https://github.com/psf/black
     rev: 19.10b0 # Replace by any tag/version: https://github.com/psf/black/tags
     hooks:
       - id: black
          language_version: python3 # Should be a command that runs python3.6+
\end{lstlisting}

А затем запустить установку хуков
\begin{lstlisting}[
style = bash,
numbers = none	
]
pre-commit install
\end{lstlisting}

Теперь \texttt{pre-commit} будет автоматически запускаться каждый раз при создании коммита.

Перед фиксацией изменений можно на всякий случай вызвать
\begin{lstlisting}[
style = bash,
numbers = none	
]
pre-commit run --all-files
# вывод
[INFO] Initializing environment for https://github.com/psf/black.
[INFO] Installing environment for https://github.com/psf/black.
[INFO] Once installed this environment will be reused.
[INFO] This may take a few minutes...
black....................................................................Failed
- hook id: black
- files were modified by this hook

reformatted /Users/leor.finkelberg/Python_projects/termostablizator/css.py
reformatted /Users/leor.finkelberg/Python_projects/termostablizator/sou.py
All done!
2 files reformatted.
\end{lstlisting}

Здесь говориться, что отработал хук \texttt{black} и отформатировал 2 файла.



\listoffigures\addcontentsline{toc}{section}{Список иллюстраций}

% Источники в "Газовой промышленности" нумеруются по мере упоминания 
\begin{thebibliography}{99}\addcontentsline{toc}{section}{Список литературы}
	\bibitem{lutz:learningpython-2011}{\emph{Лутц М.} Изучаем Python, 4-е издание. -- Пер. с англ. -- СПб.: Символ-Плюс, 2011. -- 1280~с. }
		
	\bibitem{beazley:python-2010}{\emph{Бизли Д.} Python. Подробный справочник. -- Пер. с англ. -- СПб.: Символ-Плюс, 2010. -- 864~с. }
	
	\bibitem{chacon:2020}{ \emph{Чакон С.}, \emph{Штрауб Б.} Git для профессионального программиста. -- СПб.: Питер, 2020. -- 496~с. }
	
	\bibitem{ramalho:python-2016}{\emph{Рамальо Л.}{ Python. К вершинам мастерства. -- М.: ДМК Пресс, 2016. -- 768 с.}}
	
	\bibitem{slatkin:python-2016}{ \emph{Слаткин  Б.} Секреты Python: 59 рекомендаций по написанию эффективного кода. -- М.: ООО~<<И.Д. Вильямс>>, 2016. -- 272~с.}
	
	\bibitem{prohorenok:python-2016}{ \emph{Прохоренок Н.А.}, \emph{Дронов В.А.} Python 3 и PyQt 5. Разработка приложений. -- СПб.: БХВ-Петербург, 2016. -- 832~с.}
	
	\bibitem{chandola:2009}{\emph{Chandola V.}, \emph{Banerjee A.} etc. } Anomaly detection: A survey, ACM Computing Surveys, vol. 41(3), 2009, pp. 1--58.
	
	\bibitem{albon:2019}{\emph{Элбон К.} Машинное обучение с использованием Python. Сборник рецептов. -- СПб.: БХВ-Петербург, 2019. -- 384 с.}
	
	\bibitem{karau:2018}{\emph{Карау Х.}, \emph{Уоррен Р.} Эффективный Spark. Масштабирование и оптимизация. -- СПб. Питер, 2018. -- 352 с.}
	
	\bibitem{raschka:2019}{\emph{Рашка С.}, \emph{Мирджалили В.} Python и машинное обучение: машинное и глубокое обучение с использованием Python, scikit-learn и TensorFlow. -- СПб.: ООО <<Диалектика>>, 2019. -- 656 с.}
	
	\bibitem{chun:2015}{\emph{Чан У.} Python: создание приложений. Библиотека профессионала, 3-е изд. -- М.: ООО <<И.Д. Вильямс>>, 2015. -- 816 с.}
\end{thebibliography}

\end{document}